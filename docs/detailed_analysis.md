# Issue Report Localization の詳細実験結果分析

## 実験概要

本研究では、Issue Report Localization（IRL）タスクにおいて、以下の4つの実験設定を比較した：

1. **Single Project (Nova)**: 単一プロジェクト（OpenStack Nova）での学習
2. **Multi-Project (No Oversampling)**: 複数プロジェクトでの学習（オーバーサンプリングなし）
3. **Multi-Project (2x Oversampling)**: 複数プロジェクトでの学習（2倍オーバーサンプリング）
4. **Multi-Project (3x Oversampling)**: 複数プロジェクトでの学習（3倍オーバーサンプリング）

各設定について、クロステンポラル評価（異なる時期のデータでの訓練と評価）を実施した。

---

## 主要な発見

### 1. 単一プロジェクト vs 複数プロジェクト

#### 1.1 性能指標の比較

**同一期間での性能（訓練データと同じ時期での評価）：**

| 設定 | F1 Score | Precision | Recall | AUC-ROC |
|------|----------|-----------|--------|---------|
| Single Project | 0.6360±0.0581 | 0.6015±0.1008 | 0.7167±0.1706 | 0.7537±0.0510 |
| Multi-Project (No OS) | **0.9180±0.0456** | **0.8517±0.0783** | **1.0000±0.0000** | 0.6353±0.0506 |
| Multi-Project (2x OS) | 0.6536±0.0416 | 0.5078±0.0500 | **0.9243±0.0219** | 0.6968±0.0335 |
| Multi-Project (3x OS) | 0.6549±0.0295 | 0.5132±0.0364 | **0.9078±0.0046** | 0.7124±0.0229 |

**未来期間での性能（訓練データより後の時期での評価）：**

| 設定 | F1 Score | Precision | Recall | AUC-ROC |
|------|----------|-----------|--------|---------|
| Single Project | 0.6706±0.0668 | 0.6894±0.0653 | 0.6857±0.1593 | **0.8322±0.0585** |
| Multi-Project (No OS) | **0.9004±0.0547** | **0.8235±0.0937** | **1.0000±0.0000** | 0.5274±0.1352 |
| Multi-Project (2x OS) | 0.6383±0.0209 | 0.4842±0.0211 | **0.9386±0.0407** | 0.6955±0.0482 |
| Multi-Project (3x OS) | 0.6337±0.0147 | 0.4887±0.0167 | **0.9027±0.0279** | 0.6995±0.0372 |

#### 1.2 重要な観察

**複数プロジェクト（オーバーサンプリングなし）の特異性：**
- F1スコアが極めて高い（0.918@同一期間、0.900@未来期間）
- **Recallが完全に1.0（100%）** - すべてのバグ関連イシューを検出
- Precisionも高い（0.852@同一期間、0.824@未来期間）
- しかし、AUC-ROCは他の設定より低い（0.635@同一期間、0.527@未来期間）

**この現象の原因考察：**
1. **極端なクラス不均衡**：サンプル数を見ると、positive（バグ関連）が圧倒的に多い
   - 例：train_0-3m/eval_0-3m: 62+ vs 7- （89.9%がpositive）
   - 例：train_6-9m/eval_6-9m: 62+ vs 3- （95.4%がpositive）
2. **モデルの挙動**：ほぼすべてを「バグ関連」と予測する戦略を学習
   - これにより、Recall=1.0を達成
   - データセットの性質上、この戦略でもPrecisionが高くなる
3. **AUC-ROCの低さ**：モデルの識別能力が実際には低い
   - 単純に多数派クラス（この場合positive）を予測しているため
   - 確率スコアの分離度が低い

**単一プロジェクトの特性：**
- より均衡した性能（F1: 0.636-0.671）
- AUC-ROCが最も高い（0.754@同一期間、0.832@未来期間）
- **未来期間で性能が向上（F1: +0.0346）** - 興味深い現象
- これは、時系列で後のデータがより一貫性がある可能性を示唆

#### 1.3 汎化性能の分析

**性能低下（同一期間 → 未来期間）：**

| 設定 | F1低下 | 汎化率 |
|------|--------|--------|
| Single Project | -0.0346 | **105.44%** |
| Multi-Project (No OS) | +0.0176 | 98.08% |
| Multi-Project (2x OS) | +0.0153 | 97.66% |
| Multi-Project (3x OS) | +0.0212 | 96.76% |

**興味深い発見：**
- **単一プロジェクトは未来期間で性能が向上**（汎化率105.44%）
  - これは、Novaプロジェクトのイシュー報告が時間とともに標準化された可能性
  - または、初期のデータにノイズが多かった可能性
- 複数プロジェクトは若干の性能低下を示すが、非常に小さい（2-4%程度）
  - 複数ドメインからの学習が時系列の変化に対してロバストであることを示唆

---

### 2. オーバーサンプリングの効果

#### 2.1 性能への影響

**オーバーサンプリングによる変化（複数プロジェクト）：**

| 指標 | No OS | 2x OS | 3x OS |
|------|-------|-------|-------|
| **同一期間F1** | 0.9180 | 0.6536 | 0.6549 |
| **未来期間F1** | 0.9004 | 0.6383 | 0.6337 |
| **同一期間Precision** | 0.8517 | 0.5078 | 0.5132 |
| **未来期間Precision** | 0.8235 | 0.4842 | 0.4887 |
| **同一期間Recall** | 1.0000 | 0.9243 | 0.9078 |
| **未来期間Recall** | 1.0000 | 0.9386 | 0.9027 |
| **同一期間AUC-ROC** | 0.6353 | 0.6968 | 0.7124 |
| **未来期間AUC-ROC** | 0.5274 | 0.6955 | 0.6995 |

#### 2.2 クラス分布の変化

評価データのクラス分布を見ると：

**オーバーサンプリングなし：**
- train_0-3m/eval_0-3m: 62+ / 7- (89.9% positive)
- train_9-12m/eval_9-12m: 53+ / 18- (74.6% positive)

**2倍・3倍オーバーサンプリング：**
- train_0-3m/eval_0-3m: 51+ / 48- (51.5% positive)
- train_9-12m/eval_9-12m: 54+ / 59- (47.8% positive)

#### 2.3 重要な考察

**オーバーサンプリングの真の効果：**

1. **クラス不均衡の是正**
   - オーバーサンプリングにより、評価データのクラス分布が劇的に変化
   - これは、**評価データ自体が異なる**ことを意味する可能性が高い
   - 単純な性能比較では、実際の効果を測定できていない可能性

2. **性能指標の解釈**
   - オーバーサンプリングなし: 極端に不均衡なデータで極端に高いスコア
   - オーバーサンプリングあり: より均衡したデータでより現実的なスコア
   - **どちらが「良い」かは、実際のアプリケーション要件に依存**

3. **AUC-ROCの改善**
   - オーバーサンプリングによりAUC-ROCが大幅に改善（0.635 → 0.697-0.712）
   - これは、モデルの識別能力が実際に向上していることを示す
   - 単純な多数派予測から、より洗練された判断へ移行

4. **Recall-Precisionトレードオフ**
   - オーバーサンプリングなし: Recall=1.0, Precision=0.85
   - オーバーサンプリングあり: Recall=0.90-0.92, Precision=0.51
   - より均衡したデータで、モデルがより慎重な予測を行うように

**2倍 vs 3倍オーバーサンプリング：**
- 性能差はほぼ無視できるレベル（F1で0.001-0.005の差）
- 3倍の方がわずかにAUC-ROCが高い（より良い識別能力）
- 過学習のリスクを考慮すると、**2倍で十分**と考えられる

---

### 3. 時系列での性能変化

#### 3.1 訓練期間別の傾向

**単一プロジェクト（Nova）の時系列パターン：**
- 9-12m訓練の場合、未来期間での評価でもF1が高い傾向
  - 9-12m/3-6m: F1=0.607
  - 9-12m/6-9m: F1=0.537
  - 9-12m/9-12m: F1=0.727
- 古い期間のデータ（0-3m訓練）は、中間期間（6-9m評価）で最も高い性能
  - 0-3m/6-9m: F1=0.774, AUC=0.910

**複数プロジェクト（オーバーサンプリングなし）：**
- すべての訓練期間でRecall=1.0を維持
- F1スコアが非常に安定（0.85-0.98）
- 時系列による性能変化がほとんどない

**複数プロジェクト（オーバーサンプリングあり）：**
- より時系列の影響を受ける
- 特に、未来期間での評価でわずかな性能低下
- しかし、標準偏差が小さく安定している（2x OS: std=0.021, 3x OS: std=0.015）

#### 3.2 コンセプトドリフトの影響

**単一プロジェクト：**
- AUC-ROCが時系列で向上する傾向（0.717 → 0.832）
- これは、プロジェクトの成熟に伴うイシュー報告の標準化を示唆
- または、コミュニティの学習効果

**複数プロジェクト（オーバーサンプリングなし）：**
- AUC-ROCが時系列で大きく変動（0.595 → 0.624）
- 複数ドメインを含むため、個別プロジェクトの変化が平均化される

**複数プロジェクト（オーバーサンプリングあり）：**
- AUC-ROCが最も安定（2x: 0.666-0.697, 3x: 0.687-0.712）
- より均衡したデータでの学習が時系列変化に対してロバスト

---

## 総合考察

### 実用的観点からの推奨

#### シナリオ1: 高いRecallが重要な場合（バグを見逃したくない）
- **推奨**: Multi-Project (No Oversampling)
- Recall=1.0を達成
- ただし、誤検知（false positive）が増えることを許容する必要

#### シナリオ2: バランスの取れた性能が重要な場合
- **推奨**: Multi-Project (2x Oversampling)
- F1=0.638-0.654と実用的なレベル
- Recall=0.92と高い検出率を維持
- AUC-ROC=0.696-0.697と良好な識別能力

#### シナリオ3: 単一プロジェクトでの適用
- **推奨**: Single Project設定
- プロジェクト固有のパターンを学習
- AUC-ROC=0.754-0.832と優れた識別能力
- 時系列で性能が向上する可能性

### 学術的観点からの重要な発見

1. **極端なクラス不均衡の影響**
   - 評価データが不均衡な場合、高いF1スコアは必ずしも良いモデルを意味しない
   - AUC-ROCなどの不均衡に対してロバストな指標が重要

2. **オーバーサンプリングの役割**
   - 単なる性能向上ではなく、より現実的な評価環境の構築
   - モデルの挙動を「全部positive予測」から「適切な識別」へ変更

3. **時系列での安定性**
   - 複数プロジェクトからの学習は時系列変化に対してロバスト
   - 単一プロジェクトは特定のパターンに特化するが、時系列で改善の可能性

4. **データ量 vs データ多様性**
   - 複数プロジェクトはデータ量を増やすだけでなく、多様性も増加
   - しかし、極端な不均衡が生じる可能性に注意

### 今後の研究課題

1. **評価データのクラス分布の統一**
   - オーバーサンプリングが評価データに影響を与えている可能性を検証
   - 同一の評価セットでの公平な比較

2. **より詳細な誤り分析**
   - False Positiveの特性分析
   - False Negativeのパターン分析

3. **他のクラス不均衡対処手法の検討**
   - Undersampling
   - SMOTE（Synthetic Minority Over-sampling Technique）
   - Cost-sensitive learning

4. **閾値最適化の改善**
   - F1最大化以外の基準（例：コスト最小化）
   - アプリケーション要件に応じた最適化

5. **プロジェクト固有の特性分析**
   - どのプロジェクトが学習に最も貢献しているか
   - プロジェクト間の転移学習の効果

---

## 結論

本研究の結果、以下の結論を得た：

1. **複数プロジェクト学習は有効**だが、クラス不均衡に注意が必要
2. **オーバーサンプリング（2倍）は推奨**される - より現実的で安定した性能
3. **単一プロジェクトでの学習も有効**で、時系列での改善が期待できる
4. **評価指標の選択が重要** - F1だけでなく、AUC-ROCやRecall-Precisionバランスを考慮
5. **時系列での安定性**は複数プロジェクト学習の大きな利点

実用アプリケーションでは、要件（Recall重視 vs バランス重視）に応じて適切な設定を選択すべきである。
