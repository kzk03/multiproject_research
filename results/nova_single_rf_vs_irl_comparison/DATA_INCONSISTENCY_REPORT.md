# データ不整合レポート: john.garbutt@stackhpc.com ケースの調査

**作成日**: 2025-12-21
**調査対象**: john.garbutt@stackhpc.comの予測結果の不整合

---

## エグゼクティブサマリー

予測品質分析で「RFが正解、IRLが不正解」とされていたjohn.garbutt@stackhpc.comのケースを詳細調査した結果、**データの不整合**が発見されました。

**主要な発見**:
1. 使用していたRFデータは**異なる時期**（2020-2021年）のデータだった
2. IRLデータは2023年のデータ
3. **比較が無効**であることが判明
4. 以前の分析結果から、このケースを除外する必要がある

---

## 詳細調査結果

### 1. データの不整合の発見

#### john.garbutt@stackhpc.com の比較

| 項目 | IRL データ | RF データ | 不整合 |
|------|-----------|----------|--------|
| **真のラベル** | 1 (継続) | 0 (離脱) | ✗ 異なる |
| **履歴リクエスト数** | 53 | 1 | ✗ 大きく異なる |
| **評価期間リクエスト** | 4 | 1 | ✗ 異なる |
| **評価期間承諾** | 1 | 0 | ✗ 異なる |
| **評価期間拒否** | 3 | 1 | ✗ 異なる |
| **コンテキスト日付** | 2023年 | 2020-10-01 | ✗ 3年の差 |

**結論**: これらは**完全に異なるデータセット**を参照している。

### 2. 期間の違い

#### IRLデータ
```
訓練期間: 2021-01-01 ～ 2021-04-01 (0-3m)
         または 2021-07-01 ～ 2021-10-01 (6-9m)
評価期間: 2023-07-01 ～ 2023-10-01 (6-9m)
データソース: review_continuation_cross_eval_nova
```

#### 使用していたRFデータ
```
コンテキスト日付: 2020-10-01
訓練期間: 2020年前半（推定）
評価期間: 2020-2021年（推定）
データソース: nova_single_rf_comparison (別の実験)
訓練サンプル数: 76
評価サンプル数: 22
```

**問題点**:
- IRLとRFで**3年のタイムラグ**がある
- 評価対象の開発者が異なる
- ラベリング基準が異なる可能性

### 3. 他の開発者でもラベル不一致を確認

15名の共通開発者のうち、**4名でラベルが不一致**:

| 開発者 | IRL ラベル | RF ラベル | IRL 承諾数 | RF 承諾数 |
|--------|-----------|----------|-----------|----------|
| **takanattie@gmail.com** | 0 (離脱) | 1 (継続) | 0 | 2 |
| **wangzihao@yovole.com** | 0 (離脱) | 1 (継続) | 0 | 2 |
| **openstack@fried.cc** | 0 (離脱) | 1 (継続) | 0 | 1 |
| **john.garbutt@stackhpc.com** | 1 (継続) | 0 (離脱) | 1 | 0 |

**割合**: 4/15 = 26.7%のラベル不一致

これは**許容できないレベル**の不整合です。

### 4. 根本原因の特定

#### 使用したデータソース
```
/Users/kazuki-h/research/multiproject_research/outputs/analysis_data/nova_single_rf_comparison/eval_features_6-9m_nova.csv
```

このファイルは:
- **別の実験**（nova_single_rf_comparison）のデータ
- **異なる時期**（2020-2021年）
- **異なる目的**（単一プロジェクトRF評価）

#### 本来使うべきデータソース
```
/Users/kazuki-h/research/multiproject_research/outputs/rf_nova_case2_simple/
```

しかし、このディレクトリには:
- predictions.csvファイルが存在しない
- results.jsonのみが存在
- 個別の開発者レベルの予測データがない

---

## john.garbutt@stackhpc.com ケースの真相

### IRLでの予測（正しいデータ）

```
評価期間: 2023-07-01 ～ 2023-10-01

開発者プロファイル:
  履歴リクエスト数: 53
  履歴受諾率: 3.8%（極めて低い）
  評価期間での活動:
    - リクエスト: 4
    - 承諾: 1
    - 拒否: 3

真のラベル: 1 (継続) ← 承諾が1回あったため

IRL (0-3m→6-9m) 予測:
  確率: 0.465
  予測: 0 (離脱) ← 閾値0.471を下回る
  結果: ✗ 不正解

IRL (6-9m→6-9m) 予測:
  確率: 0.471
  予測: 1 (継続) ← 閾値0.471にギリギリ達する
  結果: ✓ 正解
```

### なぜIRL (0-3m→6-9m) は不正解だったのか？

**理由1: 極めて低い受諾率（3.8%）**
- 履歴53リクエストのうち、承諾はわずか2回程度
- IRLはこの低受諾率から「離脱する」と判断

**理由2: 閾値ギリギリ（0.465 vs 0.471）**
- わずか0.006の差で判定が変わる
- 典型的な**ボーダーラインケース**

**理由3: 予測困難な継続**
- 受諾率3.8%で継続するのは稀なケース
- 評価期間でも4リクエストのうち1承諾（25%）と低い
- しかし1回でも承諾したため「継続」とラベル付け

### IRLの判断は妥当だったのか？

**判断**: ほぼ妥当

この開発者は:
- 受諾率が極めて低い（3.8%）
- 評価期間でも1/4（25%）の受諾率
- 統計的には「離脱しやすい」プロファイル

IRLが「離脱」と予測したのは、データから見れば**合理的**な判断。

ただし、実際には1回承諾したため、ラベリングルール上は「継続」となった。

**これは予測モデルの限界ではなく、ラベリングルールとの乖離**

---

## RFについての正しい理解

### RF Case2の実際の性能（正しいデータ）

6-9m期間（対角線評価）:
```
訓練期間: 2021-07-01 ～ 2021-10-01
評価期間: 2023-07-01 ～ 2023-10-01
訓練サンプル数: 83
評価サンプル数: 58

性能:
  AUC-ROC: 0.933
  Precision: 0.786
  Recall: 0.733
  F1: 0.759
  TP: 11, TN: 40, FP: 3, FN: 4
```

**問題**: predictions.csvがないため、john.garbutt@stackhpc.comをRFがどう予測したかは不明

### RF Case1の性能（参考）

6-9m期間:
```
訓練サンプル数: 1140
評価サンプル数: 58

性能:
  AUC-ROC: 0.822
  Precision: 0.545
  Recall: 0.800
  F1: 0.649
```

---

## 以前の分析の訂正

### 誤った結論（訂正前）

```
ケーススタディ2: RFが正解、IRLが不正解（1件、7%）

唯一のケース: john.garbutt@stackhpc.com
  IRL: 不正解
  RF: 正解
```

### 正しい結論（訂正後）

```
john.garbutt@stackhpc.comのケースは、異なる時期のデータを
比較していたため、無効である。

このケースを除外すると:
  - IRLのみ正解: 8件（53%）
  - RFのみ正解: 0件（0%）
  - 両モデル正解: 2件（13%）
  - 両モデル不正解: 5件（33%）← john.garbutt を含む

※ただし、RFの個別予測データが存在しないため、
  正確な比較は不可能
```

---

## ラベリングルールの考察

### IRLとRFで共通のラベリングルール

両モデルとも以下のルールを使用:

```python
if 評価期間に承諾 > 0:
    label = 1  # 継続
else:
    if 拡張期間に依頼あり:
        label = 0  # 離脱（依頼あったが承諾しなかった）
    else:
        # 除外（依頼自体がなかった）
```

### john.garbutt の場合

```
評価期間（3ヶ月）:
  リクエスト: 4
  承諾: 1
  → label = 1 (継続)

しかし:
  承諾率: 1/4 = 25%（依然として低い）
  履歴受諾率: 3.8%（極めて低い）
```

**問題提起**:
「1回でも承諾したら継続」というルールは適切か？

**代替案**:
- 承諾率による閾値設定（例: 20%以上で継続）
- 承諾数の絶対値による閾値（例: 2回以上で継続）
- 加重ラベリング（承諾率に応じて0-1の連続値）

---

## 今後の対応

### 1. 正しいデータでの再比較が必要

**現状の問題**:
- RFのpredictions.csvが存在しない
- 個別開発者レベルの比較ができない

**解決策**:
- RFスクリプトを修正して、predictions.csvを出力
- または、IRLと同じ開発者セットでRFを再実行

### 2. 分析レポートの訂正

**訂正が必要な箇所**:
1. **prediction_quality_analysis.md**
   - 「RFが正解、IRLが不正解」のセクションを削除または訂正
   - john.garbutt ケースを「両モデル不正解」または「不明」に変更

2. **COMPLETE_ANALYSIS.md**
   - 同様の訂正

3. **model_comparison_detailed.csv**
   - john.garbutt の行を削除または「データ不整合」とマーク

### 3. ラベリングルールの再検討

**検討事項**:
- 1回の承諾で「継続」とするのは適切か？
- 受諾率による閾値設定は有効か？
- ボーダーラインケースの扱い

---

## 結論

### 主要な発見

1. **データ不整合**:
   - 使用したRFデータは異なる時期（2020-2021年）のものだった
   - IRLデータは2023年
   - 比較は無効

2. **john.garbutt ケース**:
   - IRLの予測（離脱）は受諾率3.8%から見れば合理的
   - 実際は1回承諾したため「継続」とラベル付け
   - これは予測モデルの限界ではなく、ラベリングルールの問題

3. **ラベル不一致**:
   - 15名中4名（26.7%）でラベルが不一致
   - これは許容できないレベル

### 推奨事項

1. **正しいデータでの再分析**
   - RFのpredictions.csvを生成
   - 同じ期間・同じ開発者で再比較

2. **分析レポートの訂正**
   - john.garbutt ケースを除外
   - 「RFのみ正解」の記述を削除

3. **ラベリングルールの見直し**
   - 1回承諾で「継続」とするルールの妥当性を検討
   - より厳密な基準の導入を検討

---

**このレポートは、予測品質分析の信頼性を確保するために作成されました。**
**正確な比較のためには、同じ期間・同じ開発者のデータを使用することが不可欠です。**
