# RF vs IRL 特徴量重要度比較：完全版分析

**作成日**: 2025-12-25
**対象**: OpenStack Novaプロジェクトにおけるレビュア継続予測
**データソース**: RF（10パターン平均）vs IRL（4訓練期間平均）

---

## エグゼクティブサマリー

Random Forest（RF）とInverse Reinforcement Learning（IRL）の特徴量重要度を定量的に比較した結果、**両モデルが全く異なる特徴を重視している**ことが判明しました。特に以下の劇的な逆転が確認されました：

- **協力度**: RF 14位（0.00%） → IRL 2位（絶対値0.0131）【順位差：-12】
- **応答速度**: RF 1位（26.61%） → IRL 11位（絶対値0.0027）【順位差：+10】
- **最近の受諾率**: RF 6位（4.20%） → IRL 14位（絶対値0.0001）【順位差：+8】

この逆転は、**RFが「過去の静的な統計パターン」を学習する一方、IRLが「行動の因果的な影響」を学習している**ことを示唆しています。

---

## 1. モデル別特徴量重要度ランキング

### 1.1 Random Forest Top 10（Gini係数ベース）

| 順位 | 特徴量 | 重要度 | 割合 | IRL順位 | 順位差 |
|------|--------|--------|------|---------|--------|
| 1位 | 応答速度 | 0.2661 | 26.61% | 11位 | **+10** |
| 2位 | 総レビュー数 | 0.2545 | 25.45% | 1位 | -1 |
| 3位 | 最近の活動頻度 | 0.1621 | 16.21% | 7位 | +4 |
| 4位 | 経験日数 | 0.0928 | 9.28% | 12位 | **+8** |
| 5位 | 平均活動間隔 | 0.0660 | 6.60% | 3位 | -2 |
| 6位 | 最近の受諾率 | 0.0420 | 4.20% | 14位 | **+8** |
| 7位 | 強度（ファイル数） | 0.0412 | 4.12% | 4位 | -3 |
| 8位 | レビュー負荷 | 0.0371 | 3.71% | 13位 | +5 |
| 9位 | レビュー規模（行数） | 0.0352 | 3.52% | 10位 | +1 |
| 10位 | 活動トレンド | 0.0032 | 0.32% | 8位 | -2 |

**特徴**:
- 上位2特徴（応答速度、総レビュー数）で **52.06%** の重要度を占有
- 上位5特徴で **84.14%** を占有（極端な集中）
- 協力度、コード品質スコア、総コミット数などは **0.00%**（Giniで分岐に使われず）

---

### 1.2 IRL (LSTM) Top 10（勾配ベース、絶対値）

| 順位 | 特徴量 | 勾配値 | 絶対値 | RF順位 | 順位差 |
|------|--------|--------|--------|---------|--------|
| 1位 | 総レビュー数 | +0.0165 | 0.0165 | 2位 | -1 |
| 2位 | 協力度 | +0.0131 | 0.0131 | 14位 | **-12** |
| 3位 | 平均活動間隔 | -0.0107 | 0.0107 | 5位 | -2 |
| 4位 | 強度（ファイル数） | +0.0083 | 0.0083 | 7位 | -3 |
| 5位 | 総コミット数 | +0.0080 | 0.0080 | 11位 | **-6** |
| 6位 | コード品質スコア | -0.0078 | 0.0078 | 13位 | **-7** |
| 7位 | 最近の活動頻度 | +0.0076 | 0.0076 | 3位 | +4 |
| 8位 | 活動トレンド | -0.0064 | 0.0064 | 10位 | -2 |
| 9位 | 協力スコア | +0.0041 | 0.0041 | 12位 | -3 |
| 10位 | レビュー規模（行数） | -0.0034 | 0.0034 | 9位 | +1 |

**特徴**:
- 勾配値は **正負両方**（正: レビュー継続を促進、負: 抑制）
- 上位5特徴の絶対値合計は **0.0565**（RFほど集中していない）
- RFで無視された「協力度」「総コミット数」「コード品質スコア」が上位に

---

## 2. 劇的な順位逆転の詳細分析

### 2.1 RF最重要 → IRL低重要

| 特徴量 | RF順位 | RF重要度 | IRL順位 | IRL絶対値 | 順位差 | 解釈 |
|--------|--------|----------|---------|-----------|--------|------|
| **応答速度** | 1位 | 26.61% | 11位 | 0.0027 | **+10** | RFは応答速度の「統計的相関」を最重視。IRLは因果的影響が小さいと判断 |
| **経験日数** | 4位 | 9.28% | 12位 | 0.0026 | **+8** | RFは経験の長さを重視。IRLは負の勾配（-0.0026）＝経験が長いと継続しにくい |
| **最近の受諾率** | 6位 | 4.20% | 14位 | 0.0001 | **+8** | RFは過去の受諾率パターンを学習。IRLでは因果的影響がほぼゼロ |

**考察**:
- **応答速度の逆転**: RFは「応答が速い人は継続しやすい」という**静的な相関**を学習。一方IRLは、応答速度の変化が報酬に与える**因果的影響**は限定的と判断。
- **経験日数の負勾配**: IRLは経験が長いほど**継続しにくい**（バーンアウトや役割変化）ことを学習。RFはこの非線形関係を捉えられない。

---

### 2.2 IRL最重要 → RF低重要

| 特徴量 | IRL順位 | IRL勾配 | RF順位 | RF重要度 | 順位差 | 解釈 |
|--------|---------|---------|--------|----------|--------|------|
| **協力度** | 2位 | +0.0131 | 14位 | 0.00% | **-12** | IRLは行動の協力性が報酬に直結すると学習。RFはGini分岐で無視 |
| **コード品質スコア** | 6位 | -0.0078 | 13位 | 0.00% | **-7** | IRLは品質低下が継続を阻害すると学習。RFは分岐に使わず |
| **総コミット数** | 5位 | +0.0080 | 11位 | 0.00% | **-6** | IRLは活動量の因果的影響を重視。RFは無視 |

**考察**:
- **協力度の劇的逆転**: IRLは「協力的な行動」が将来報酬を最大化する**最適方策**の一部と学習。RFは過去データで協力度が決定境界にならなかったため無視。
- **行動特徴の重視**: IRLは action_importance（協力度、強度、応答速度、レビュー規模）を重視。RFは state_importance（総レビュー数、活動頻度など）を重視。

---

## 3. 両モデルで一致した重要特徴

| 特徴量 | RF順位 | IRL順位 | 順位差 | 両モデルの評価 |
|--------|--------|---------|--------|---------------|
| **総レビュー数** | 2位 | 1位 | -1 | **一致**: 両モデルとも最重要級 |
| **平均活動間隔** | 5位 | 3位 | -2 | **一致**: 高重要度 |
| **強度（ファイル数）** | 7位 | 4位 | -3 | **一致**: 中程度重要 |

**考察**:
- **総レビュー数**: RF（25.45%）とIRL（+0.0165）の両方で最重要級。過去の実績と行動選択の両方で重要。
- **平均活動間隔**: 活動の安定性を示す重要な指標として両モデルで認識。

---

## 4. 統計サマリー

### 4.1 重要度の集中度

| モデル | 上位5特徴の合計 | 上位10特徴の合計 | 集中度 |
|--------|----------------|------------------|--------|
| **RF** | 84.14% | 100.00% | 極めて高い |
| **IRL** | 0.0565 | 0.0860 | 分散的 |

**解釈**:
- RFは少数の特徴（応答速度、総レビュー数）に **極端に依存**
- IRLは多様な特徴を **バランスよく考慮**

---

### 4.2 正負の勾配分布（IRL）

| 符号 | 特徴量例 | 意味 |
|------|---------|------|
| **正 (+)** | 総レビュー数、協力度、総コミット数、強度 | これらが増加すると継続確率が **上昇** |
| **負 (-)** | 平均活動間隔、コード品質スコア、活動トレンド、経験日数 | これらが増加すると継続確率が **低下** |

**特に重要な負勾配**:
- **コード品質スコア (-0.0078)**: 品質低下は継続を阻害
- **平均活動間隔 (-0.0107)**: 活動間隔が開くと離脱しやすい

---

## 5. モデル間の本質的な違い

### 5.1 学習メカニズムの違い

| 観点 | Random Forest | IRL (LSTM) |
|------|--------------|------------|
| **学習対象** | 過去データの**静的パターン** | 行動系列の**動的な因果関係** |
| **特徴選択** | Gini係数（情報利得）による分岐 | 勾配ベース（報酬への影響度） |
| **時系列考慮** | なし（各時点独立） | あり（LSTMで系列学習） |
| **解釈** | 相関関係 | 因果的影響 |

---

### 5.2 なぜ逆転が起きるのか

#### ケース1: 応答速度（RF 1位 → IRL 11位）

- **RFの視点**: 過去データで「応答が速い人は継続率が高い」という**統計的相関**を発見 → Gini分岐で最優先
- **IRLの視点**: 応答速度の**変化**が将来報酬に与える**因果的影響**は限定的 → 勾配値0.0027（小さい）

#### ケース2: 協力度（RF 14位 → IRL 2位）

- **RFの視点**: 訓練データで協力度が決定境界にならない → Gini分岐に使われず重要度0%
- **IRLの視点**: 協力的な行動を取ることが**最適方策**の一部（報酬最大化に直結） → 勾配値+0.0131（大きい）

---

## 6. 実務的含意

### 6.1 レビュア継続を促進するには

**RFベースの推奨（相関ベース）**:
- 応答速度が速い人を優先的に選ぶ
- 総レビュー数が多い人を継続候補とする

**IRLベースの推奨（因果ベース）**:
- **協力度を高める**: 積極的な協力行動を促す施策（ペアレビュー、メンター制度）
- **総コミット数を増やす**: コード貢献の機会を提供
- **活動間隔を短く保つ**: 定期的な関与を促す仕組み
- **コード品質の維持**: 品質低下を防ぐサポート

**IRLの方が実務的に有用な理由**:
- 「協力度を上げれば継続しやすくなる」という**介入可能な因果関係**を提示
- RFの「応答速度」は個人特性で変えにくい

---

### 6.2 モデル選択のガイドライン

| 目的 | 推奨モデル | 理由 |
|------|-----------|------|
| **予測精度** | 両方試す | タスクにより異なる |
| **因果推論** | **IRL** | 行動の影響を学習 |
| **介入施策設計** | **IRL** | 変更可能な特徴を重視 |
| **高速な推論** | **RF** | シンプルで高速 |
| **解釈性（相関）** | **RF** | 決定木で可視化容易 |
| **解釈性（因果）** | **IRL** | 勾配が因果的影響を示す |

---

## 7. 結論

### 7.1 主要な発見

1. **劇的な順位逆転**: 協力度（RF 14位 → IRL 2位）、応答速度（RF 1位 → IRL 11位）など、モデル間で最大12ランクの差
2. **学習対象の違い**: RFは**静的な相関**、IRLは**動的な因果関係**を学習
3. **実務的価値**: IRLは**介入可能な特徴**（協力度、コミット数）を重視 → 施策設計に有用

---

### 7.2 推奨事項

**研究者向け**:
- 特徴量重要度を解釈する際、モデルの学習メカニズムを考慮すべき
- 因果推論にはIRLのような系列モデルが有効

**実務者向け**:
- レビュア継続を促進するには、**協力的な行動を促す施策**が効果的
- 単なる「過去の実績」ではなく「行動の変化」に着目すべき

---

## 付録: データソース

- **RF特徴量重要度**: [rf_nova_single_feature_importance.csv](../../outputs/analysis_data/nova_single_rf_comparison/rf_results/rf_nova_single_feature_importance.csv)
- **IRL特徴量重要度**: [gradient_importance_average.json](../../results/review_continuation_cross_eval_nova/average_feature_importance/gradient_importance_average.json)
- **比較結果CSV**: [rf_vs_irl_feature_importance.csv](../../outputs/analysis_data/feature_importance_comparison/rf_vs_irl_feature_importance.csv)
- **実験設定**: RF Case1 Sliding Window 10パターン vs IRL 4訓練期間平均
