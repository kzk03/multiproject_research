\documentclass[11pt]{jreport}
\usepackage{wuse_thesis}
\usepackage{indentfirst}
\usepackage{url}	% \url{}コマンド用．URLを表示する際に便利
%\usepackage{graphicx}  % ←graphicx.styを用いてEPSを取り込む場合有効にする
			% 他のパッケージ・スタイルを使う場合には適宜追加
\usepackage{latexsym}
\usepackage[dvipdfmx]{graphicx,xcolor}
\usepackage{tabularx}
\usepackage{stfloats}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}


\newcommand{\todo}[1]{\colorbox{yellow}{{\bf TODO}:}{\color{red} {\textbf{[#1]}}}}
\newcommand{\memo}[1]{\colorbox{magenta}{\textbf{MEMO}}{\color{red}\textbf{[#1]}}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 主に表紙を作成するための情報
%%

%%  タイトル(修論の場合は英語表記も指定)
% \title{連続的な貢献を予測する\\
% 逆強化学習を用いたLTC予測モデルの提案}

\title{逆強化学習を用いた or コードレビューにおける\\
継続的な貢献を行うLTC予測モデルの提案}
%\etitle{Test\\Test\\Test}

%%  著者名(修論の場合は英語表記も指定)
\author{橋本 一輝}
%\eauthor{Akinori Ihara}

%% 卒業論文・修士論文(以下のどちらかを選択)
\bachelar	% 卒業論文(4年生用)
%\master  	% 修士論文(M2用)

%%  学科・クラスタ
\department{システム工}
%\department{デザイン情報}
%\department{デザイン科学}

%%  学生番号
\studentid{60276189}

%%  卒業年度
\gyear{2025}		% 提出年が2022年なら，2021年度

%%  論文提出日
\date{2026年2月10日}	% 修士の場合は月(2021年2月)までとし，英語表記も指定
%\edate{February 2021}	% 修士の場合，こちら(英語表記)も有効化

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\maketitle

%%
%%  概要
%%
\begin{abstract}
オープンソースソフトウェア(OSS)開発において，長期貢献者(LTC)はプロジェクトの継続的な発展と持続可能性を高める上で重要な役割を担っている．しかし，多くの開発者はプロジェクトに参加後して早期に離脱してしまうことが多いため，LTCとなり得る開発者を早期に特定することが必要である．また，従来のLTC予測は，予測時点から1年や3年以降に貢献があるかどうかを予測する．そのため，1年後以降や3年後以降のように単一時点以降に貢献があるかどうかを予測するため，開発者がどの期間で活動するかが明確でなく，特定の期間において開発者が貢献するか否かといった詳細な活動状況を予測することができない．そこで，本研究では，開発者の活動をより詳細に捉えるために，予測期間を区切り，将来の特定の期間内において，継続した貢献がある否かを連続的に予測する．具体的には，コードレビューにおける貢献に着目し，開発者がコードレビュー依頼を継続的に引き受けるか否かを開発者の意思決定の軌跡と捉え，逆強化学習(IRL)を用いて報酬関数を推定することにより予測モデルを構築する．

ケーススタディとして，OpenStack/Novaプロジェクトを対象とし，報酬関数に基づき予測を行なった結果，比較手法であるランダムフォレスト(RF)と同等の精度を示し，最大値では劣るものの，予測を行う期間を変更した多くのパターンでRFの精度を上回った．また，提案手法において予測期間が近いものに関しては活動量などの量的な特徴量が重視され，予測期間が遠いものに関しては質的な特徴量が重視されることが明らかになった．



% \todo{最後に修正}
% オープンソースソフトウェアプロジェクトでは，長期的に貢献する開発者（長期貢献者）が継続的な開発・保守において重要な役割を担っている．プロジェクトに参加してから1年後まで活動する長期貢献者を早期に特定するためには，少なくともプロジェクト参加後1ヶ月間の貢献の観察を要する．しかし，その期間の貢献が2ヶ月以降1年以内の活動に寄与しているかは明らかでない．
% 本研究は，開発者がコードレビュー依頼を継続的に引き受けるか否かを予測するする手法を提案する．具体的には，逆強化学習を用いてレビュアーの貢献を学習し，継続的なコードレビュー依頼の受け入れ可否を決定する報酬関数を推定するモデルを構築する過程で，高い精度で予測可能な学習期間を検討する．ケーススタディとして，OpenStack/Novaプロジェクトを対象とし，報酬関数に基づき予測を行なった結果，提案モデルはAUC-ROCが0.820で予測可能であり，予測に寄与する特徴量が，短期では活動量が重視され，長期では質的な要因が重視されるようになることが明らかになった．また，学習期間に応じて高い適合率を得られる予測期間は異なることが明らかになった．
\end{abstract}

%%  目次
\tableofcontents

%%  図目次 (図目次をいれたければ以下のコメントをはずす)
%\listoffigures

%%  表目次 (表目次をいれたければ以下のコメントをはずす)
%\listoftables

\newpage
\pagenumbering{arabic}	% 以降のページ番号を算用数字に

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%%  本文はここから
%%

%1
\chapter{はじめに}
オープンソースソフトウェア (OSS) は，地理的に分散した開発者によって実装，保守されているが，多くの開発者は，OSS開発組織から金銭的な利益を得ることなく，個人の知的好奇心や開発技術の学習などを動機として貢献している\cite{motivation}.

Raymondらは，OSS開発の成功例としてLinuxを対象として調査し, 不特定多数の開発者が自発的に参加する開発形態をバザール方式と名付けた\cite{bazaar}．この開発モデルは，多様な専門知識を有する開発者の参加を促進する一方で，開発者の流動性が極めて高い\cite{movement}．
また，開発者の動機が多様なため，時間的制約や興味の変化によって数回の貢献後に活動を停止する開発者が多い\cite{OTC}．こうした開発者の離脱は，OSSプロジェクトの失敗の一要因になっている\cite{failed}.

OSSを長期的に運用，保守しているプロジェクトでは，長期的にOSSプロジェクトに貢献する長期的貢献者 (LTC) が重要な役割を担っている\cite{related1}\cite{successful}．

LTCが，ソフトウェアの大部分を実装しているプロジェクトも存在する\cite{LTC}．
また，LTCは実装に限らず，コードレビュー，新規参加した開発者のメンターの役割を担っている．一部のLTCには，リポジトリの主要なブランチにコミットする権限が与えられプロジェクトの継続的な運用を支えている\cite{LTC}．このような経験が豊富なLTCは，流動性の高い組織構造を有するOSSプロジェクトでは，離脱してしまう開発者が多いため少数しか存在しないことが多い．そのため，LTCに作業負担がかかることが少なくない\cite{related2}．OSSプロジェクトの長期的な発展や，LTCへの負担軽減のためにも，継続的に貢献する開発者の参加，特定が喫緊の課題である．

従来研究では，将来的にLTCとして長期間にわたってプロジェクトに参加する開発者を早期に特定する手法が提案されている\cite{related1}\cite{LTC}．しかし，従来研究では，開発者がプロジェクトに参加してから一定期間が経過した時点を基準とし，基準点以降もその開発者がプロジェクトへの貢献を継続しているか否かを予測する研究が多い．そのため，開発者が実際にどの期間でタスクに取り組み，どの期間でタスクに取り組まないのかといった詳細な貢献を予測することができず，適切なタスクを分配することが難しい．結果として，LTCへの負担軽減につながりにくい．

本研究は，開発者がタスクの作業依頼を受諾するか，あるいは依頼に反応しないかを明確に判断することができるコードレビューを題材に，将来のコードレビュー依頼に対するレビュアーの貢献行動可否を，開発者の継続的な貢献と捉えたLTC候補者予測モデルを提案する．
% ここで開発者が依頼を受け入れるかどうかは開発者の興味や状況により左右される．受け入れることは興味や貢献する意思があることを示唆し，明示的に拒否する，または反応がない場合には継続的に貢献する意思がないことを示唆する．

本研究では，LTC候補者予測モデルの構築に向けて，次の3つのResearch Questions(RQs)に回答する．
\begin{itemize}
    \item RQ1:逆強化学習に基づく提案モデルは，コードレビューに関する継続的なタスク依頼の受諾をどの程度予測できるか？
    \item RQ2:学習方法や予測期間の長さに応じて，予測モデルの精度に差が出るか?
    \item RQ3:推定された報酬関数において，長期貢献者の継続的なレビュー承諾に寄与する特徴量は何か?
\end{itemize}
RQ1は，LTC候補者予測モデルの構築に逆強化学習を用いた際に，どの程度の妥当な予測を行うことができるのかを調査する．RQ2は，学習方法やレビュー依頼を承諾するか否かを予測する期間を変更することによるモデルの精度差があるのか，開発者単位で予測期間ごとにレビュー承諾/承諾なしを予測することができるのかを調査する．RQ3においては，予測モデルにおける特徴量の重要度を分析することにより，各特徴量がレビュアーの継続的なレビュー承諾に与える影響の重要度を調査する．

続く\ref{sec:related}章では，本研究の関連研究を紹介し，本研究の位置付けを明確にし，\ref{sec:ml}章では，本研究で用いる逆強化学習とそれに関連する強化学習を紹介する．\ref{sec:model}章では，本研究の提案手法を述べ，\ref{sec:casestudy}章でケーススタディの結果を述べRQに回答する．\ref{sec:discussion}章で本研究の結果を考察し，\ref{sec:validity threat}章で妥当性の脅威について議論し，\ref{sec:conclusion}章で本研究をまとめる．




%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%3
\chapter{持続可能なOSS開発に向けて}\label{sec:related}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{OSS開発の現状・課題}
OSSでは，ソースコードが公開され，ライセンスの条件下において，誰でも自由に閲覧・修正・再配布が可能なソフトウェアであり，世界中に地理的に分散した多くのボランティア開発者の貢献により開発が行われている\cite{motivation}.
一般的な，OSS開発のフローは次のように進行する．
\begin{itemize}
    \item ユーザや開発者が，発見したバグや機能追加などの要求をイシューやチケットとして報告
    \item 要求の中から対応するものを選択し，コードを修正・追加し要求に対応
    \item 変更内容をパッチやプルリクエストとしてプロジェクトに提出
\end{itemize}

提出された変更は，プロジェクトメンバーによるコードレビューを受ける．膨大な変更提案を効率的に処理するために，誰がコードレビューを担当するかを明確にするアサインという仕組みが取られている．具体的には，変更されたファイルの管理者（コードオーナー）がシステムによって自動的に割り当てられたり，変更提案者やプロジェクトの管理者が適切なレビュアーを指名（アサイン）したりすることにより，レビュータスクが振り分けられる．レビュアーとの議論を経て，最終的に変更が承認された場合には，変更がリポジトリに統合される．

このOSS開発プロセスでは，開発者の多くがボランティアであるために，アサインというタスク割り当てが存在しても，強制力がないために，アサインされてもレビュアーにレビュー依頼を拒否されたり，反応がなく放置されたりするケースが存在する．さらに，開発者は個人の事情や興味の変化などによりいつでもプロジェクトから離脱することができ，離脱や貢献といった動機は明確でないため，労働力が安定しないことや労働力を見積もることが困難であるといった課題がある．

こうした環境の中で，OSSの持続可能な開発には，プロジェクトに長期的に貢献する開発者であるLTCが重要である．しかし，OSSプロジェクトにおいてLTCとなり得る開発者は少数であり，特定の貢献者に負担が集中し，LTCの疲弊や離脱を招くリスクがある．そのため，多様な動機を持つ開発者の中から，継続的に貢献するLTCになり得る開発者を特定し，適切なサポートを行うことが重要となる．




\section{持続可能なOSS開発に向けた関連研究}
\subsection{OSSへ継続的に貢献する開発者の特徴}

従来研究では，開発者が特定のプロジェクトで活動を継続する動機を調査している．Wuらによる定性的分析では，OSS開発者は利他的な動機（例：貢献による名誉や実績）や人的資本の向上に関連する経済的動機（例：専門知識を有する開発者との協調）をもってOSSプロジェクトへの参加を継続していることを明らかにしている\cite{motivation2}．
その他，多くの研究者がOSSプロジェクトにおける新規参加者の定着に影響を与える要因を調査している\cite{LTC}\cite{bib1}\cite{bib2}\cite{abadon}．
\todo{従来研究確認}
Birdらは，開発者がOSSプロジェクトに貢献する期間に影響を与える要因を理解するためのハザード率モデル（hazard-rate model）を提案している\cite{bib1}.ケーススタディとして，Apache，Python，PostgreSQLプロジェクトでは，特定のプロジェクトで約1年間活動した開発者をリポジトリの主要なブランチにコミットする権限を与えていることを示している．特に，ApacheおよびPythonプロジェクトでは，継続的にパッチを作成している開発者に権限を与えていることを明らかにしている．このように，OSSプロジェクトは，LTC候補者に継続的な貢献を期待していることが確認できる．


\subsection{長期貢献者候補者の予測}

従来研究では，LTC候補者を予測する手法が提案されている\cite{related1}\cite{LTC}．Zhouらは，OSSプロジェクトに新規に参加した開発者の中からLTC候補者を特定するモデルを開発している\cite{LTC}．当該研究において，LTC候補者に分類される開発者は，技術的能力，意欲，および環境に依存することを明らかにしている．特に，LTC候補者を特定するためには，プロジェクト参加後の1ヶ月間の貢献を観察し，「参加後１ヶ月間の活動量」などのメトリクスを用いることで，0.80程度のAUCスコアでLTCを特定することを実現している．

ELuriらは，OSSプロジェクトに新規に貢献した開発者の特徴量，および貢献したリポジトリの特徴量に基づき，開発者が3年後に貢献しているか否かを予測するモデルを開発している\cite{related1}．提案モデルは，従来手法の予測精度に比べて非常に高く，AUCが0.913という結果を得ている．当該研究の比較対象とされたBaoらの研究も同様に，T年後 (T=1 ,2, 3) に開発者が貢献しているか否かを，プロジェクト参加直後1ヶ月間の貢献から予測モデルを構築している．これらの研究は，OSSの開発記録であるコミットやイシューへの対応など，プロジェクトでの全般的な活動を貢献とみなし，将来の単一時点での予測に留まっている．

\subsection{開発者の離職予測に関する研究}
Baoらは，商用企業を対象とし，プロジェクトに参加直後の6ヶ月間の月次レポートに基づき，1年後に離脱しているか否かを予測するモデルを提案している\cite{turnover}．予測に寄与する特徴量には，月次レポートのタスク報告内容，労働時間の標準偏差，プロジェクト開発者での労働時間の標準偏差であった．この研究も同様に，月次レポートに基づく活動でありプロジェクトでの全般的な活動を貢献とみなしている．また，予測に関しても単一の時点である．

\section{本研究の位置付け}
従来研究では，コミットや月次レポートなどのプロジェクトへの貢献量に基づき，特定の期間以降に開発者の貢献があるか否かを予測する手法を提案している．従来手法は，将来的な継続を大まかに判断することには有効である一方で，貢献がある期間と貢献がないをより詳細に予測することは行われていない．そのため，活動頻度の低下などによる開発者の離脱の兆候を把握したり，特定の期間において開発者の貢献の有無を予測し，タスクの負荷を分散することが難しい．タスクが継続的に発生する開発プロセスにおいて，開発者の離脱の兆候や，特定の開発者に負荷が集中すること避けるためには，特定の期間以降ではなく，期間を分割し連続的に開発者の継続的な貢献を把握する必要がある．

OSS開発では，自主的に貢献する活動に限らず，開発者間の協調作業として作業依頼に基づく活動も少なくない．本研究では，作業依頼に対しての応答が把握しやすく，開発者の貢献を捉えやすいコードレビューに着目し,開発者の継続的なレビュー依頼の受諾を予測し，継続的な貢献と捉えるLTC予測モデルを提案する．継続的な貢献とは単発の行動の繰り返しではなく，レビュアーがそのレビュー依頼を受けた際の状況に応じて依頼を受諾するか否かを選択する一連の意思決定プロセスである．ここで，教師あり学習による予測では，ある時点までの累積値や平均値といった情報を用いて学習/予測を行うために，その値に至るまでの過程や変化を捉えることが困難である．そのため，将来にわたる価値を最大化するように行動を選択する方策を学習する強化学習の枠組みを活用することで，状態と行動の時系列的な遷移としてモデル化する．具体的には開発者の状態と行動の時系列的な遷移から，0から3ヶ月後，3から6ヶ月後，6から9ヶ月後，9から12ヶ月後にそれぞれ開発者がレビュー依頼を承諾するかどうかを予測するモデルを作成する．




% OSS開発者の中には，一時的にプロジェクトでの活動を休止することが少なくない．そのため，従来研究では開発者の継続的な活動期間を計測することが困難である．


%4
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{継続的なLTC予測に向けた強化学習と逆強化学習}\label{sec:ml}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

本研究では，OSS開発者の継続的な貢献を，その時々の状況に応じてレビュー依頼を受諾するか否かを選択する一連の意思決定プロセスとして捉える．この時系列的な行動の軌跡に着目し，開発者の動的な行動パターンをモデル化するために，逆強化学習を用いてLTC予測モデルを構築する.本章では，逆強化学習の基盤となる強化学習について述べ，続いて逆強化学習および本研究での具体的な活用方法について述べる．

\section{強化学習}
強化学習 (RL: Reinforcement Learning) は，ある環境に存在するエージェントが，その時点での状態を観測し，行動を選択することにより，行動や行動によって変化した状態に応じて，環境から得られる報酬の累積値を最大化するような行動の方策を学習する機械学習の1つの手法である．教師あり学習が正解データに基づき，学習を行うのに対し，強化学習は環境から得られる報酬を手がかりに最適な行動指針を自律的に探索するのが特徴である．
数学的には，マルコフ決定過程として定式化され，次の4つの要素S,A,T,Rから成る．
\begin{itemize}
    \item 状態集合(S):エージェントが観測する環境の状態の集合．本研究ではレビュアーの過去の活動履歴や抱えているタスクの量などが該当する．
    \item 行動集合(A):エージェントが選択可能な行動の集合．本研究では，アサインされたレビュー依頼に対して承諾するか拒否するかが該当する．
    \item 遷移確率(T):ある状態sで行動aを選択した際に，次の状態s'へ遷移する確率．
    \item 報酬関数(R):状態sで行動aをとった際に得られる即時報酬．エージェントがその行動に感じる価値を定量化したもの．
\end{itemize}

表\ref{tab:RL}は強化学習の基本的な構成要素を示す．
\begin{table}[h]
\centering
\caption{強化学習の主な構成要素}
\label{tab:RL} 
\begingroup 
\setlength{\tabcolsep}{4pt} 
\footnotesize 
    \begin{tabularx}{\columnwidth}{lXX} 
    \toprule
    用語 & 説明 & OSS適応例 \\
    \midrule
    エージェント & 学習・意思決定を行う主体 & レビュアー \\
    環境 & エージェントと相互作用する対象 & OSSプロジェクト \\
    状態 & 環境の現在状況 & レビュアーの活動履歴など \\
    行動 & エージェントが取る選択肢 & レビュー依頼を引き受けるかどうか \\
    報酬 & 行動に応じて与えられるスカラー量 & 承諾なら高報酬 \\
    方策 & エージェントの行動を決定する & レビューを引き受ける際の判断基準 \\
    \bottomrule
    \end{tabularx}
\endgroup
\end{table}

\begin{figure}[t]
    \centering
    \includegraphics[width=1.0\linewidth]{./Hashimoto_fig/RL.pdf}
    \caption{強化学習のイメージ図.}
    \label{fig:RL}
\end{figure}






\memo{もう少し追記}
強化学習において，報酬関数$R(s, a)$は最も重要な要素の一つであり，エージェントにとって何が望ましい行動であるかを示す指標となる．しかし，多くの実世界の問題では，多様な要因が複雑に作用しているため，事前に報酬関数を設計することは困難であり，不適切な報酬関数はエージェントを意図しない行動に導くことになる．

\section{逆強化学習}

強化学習において事前に報酬関数を設計することが困難である課題を解決するため，専門家の行動から，その行動を説明する報酬関数を逆算するアプローチとして逆強化学習が用いられる\cite{IRL}．既知の報酬関数から最適な方策を学習する強化学習に対して，逆強化学習は既知の専門家の軌跡から，その行動を説明する報酬関数を推定する．専門家の軌跡は，状態と行動を対として時系列に並べたものであり，専門家はある報酬関数（目的）を最大化するよう行動していると仮定し，その報酬関数を推定する．本研究への適応方法については，次に述べる．


\section{本研究における活用方法}
本研究では，OSSプロジェクトにおけるコードレビュー活動を，レビュアーがその時点の状態に応じて最適な行動を選択する，一連の意思決定プロセスとしてモデル化する．具体的には，レビュアーがその時点で置かれているプロジェクト内での立ち位置や，レビュアーの活動パターンおよび作業負荷などを捉えたものを状態として用いる．また，アサインされたレビュー依頼に対してレビュアーが承諾するか拒否するかを行動とする．この際に，単にレビュー承諾するか否かのみでなく，対象となるレビュー依頼のタスク規模や，レビュー依頼に対する応答の早さといった側面も行動を表現する特徴量として用いる．

従来の教師あり学習でのLTC予測では，単一時点の特徴量の累積値や平均値といったスナップショットの情報を用いて学習を行う．この手法では，開発者の全体的な活動量を把握することはできるが，活動頻度の変化などといった活動のパターンの変化を捉えることが難しい．これに対して，逆強化学習は時系列の活動を軌跡として扱い，各状態と行動から報酬関数を推定する．そのため，高負荷なタスクが重なる，レビューの応答時間が伸びるといった，単なる累積値や平均値では捉えることが難しかったことを考慮することができる．結果として，正確に開発者のレビュー承諾と拒否を判断することが可能であると考える．\memo{どのような開発者がスナップショットで当てられへんか書いてもいい}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%5
\chapter{長期貢献者予測モデルの構築}
\label{sec:model}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{データ収集・前処理}
本研究では，Gerrit REST APIを用いて，GerritからOpenStackプロジェクトのデータを取得する．APIから取得する情報には，変更（Change）の基本情報，レビュアー情報，レビューメッセージ，変更ファイル一覧が含まれる．収集したデータから自動化されたbotアカウントと判断されるものをパターンマッチにより除外する．具体的には，レビュアーのメールアドレスに，bot，cl，jenkins，zuulのパターンを含むアカウントの除外を行なった．全てのデータをレビュー依頼時刻でソートし，時系列順に整列する．逆強化学習では，将来の行動を予測する際に，過去の情報のみを用いるため，学習データと予測対象データを時間的に分離する．そのため，本研究では基準点を設定し，各基準点より過去の活動履歴を学習に用い，将来の一定期間におけるレビュー承諾の有無を予測とする．

% 逆強化学習による継続予測では，訓練データと予測対象は時間的に分離されているのため，基準日を設けることで次のように分割する．

% \begin{table}[h]
%     \centering
%     \label{table:dataList}
%     \caption{収集データ}
%     \begin{tabularx}{\columnwidth}{@{}lX@{}} 
%         \toprule
%         \textbf{項目} & \textbf{内容} \\
%         \midrule
%         \texttt{レビュー依頼} & 変更ID,プロジェクト名，作成日時，ステータス，コード変更量（追加/削除）\\ 
%         \texttt{レビュアー情報} & メールアドレス，名前，アサイン日時 \\
%         \texttt{レビュー活動} & レビューコメント内容，投票スコア，応答時刻 \\
%         \texttt{活動統計情報} & レビュー回数，レビュー応答率，プロジェクト在籍期間 \\
%         \bottomrule
%     \end{tabularx}
% \end{table}

．
\begin{itemize}
    \item 学習期間：\\ 
    スナップショットより過去のn(本研究ではn=3)ヶ月間の活動履歴を使用し，各ステップ時から見て過去の活動履歴のみを使用する．
    \item 予測期間：\\
    スナップショットより未来のa-bヶ月後の間で活動があるかどうかを予測する．
\end{itemize}



\section{学習指標の作成}
\begin{figure}[h]
    \centering
    \includegraphics[width = 1.0\textwidth]{./Hashimoto_fig/label.pdf}
    \caption{指定期間が0-3mであった時の例}
    \label{fig:approach}
\end{figure}


本研究では，LTC候補者予測モデルを構築するために，逆強化学習を用いて報酬関数を推定する．逆強化学習は，観測された行動の履歴から，その行動の評価基準を報酬関数として推定するものである．本研究では．ここで用いる逆強化学習では，基準点から指定期間内にレビュー依頼の承諾が生じた場合をTrue，レビュー依頼が承諾されなかった場合をFalseとする二値変数として表現し，これを学習の入力データとする．この際の報酬関数は，開発者が継続してレビュー依頼を承諾しやすい状態にあるかどうかを，過去の行動履歴に基づいて推定したものと解釈することができる．

% \subsubsection{ラベルの基準点の設定}
% レビュアーの活動ごとにラベルを付与すると，ラベルの数が膨大になり，学習時間の増加や，細かすぎるラベル付により類似した状態での学習が増加するため，過学習になる恐れがある．そのため，本研究では各月の末日を基準点とすることで，その月内の全活動に同一のラベルを付与する，これにより月単位での継続パターンを学習し，より一般化された予測モデルの構築を目指す．

\subsection{基準点の設定}
レビュアーの活動ごとに学習指標を割り当てると，その数が膨大になり，学習時間の増加や，類似する状態に基づく学習が増加することで，過学習を引き起こすおそれがある．そこで，本研究では各月の末日を基準点とし，その月内の全活動に同一の学習指標を割り当てる．本手法により，月単位での継続的な活動パターンを学習し，より詳細な粒度で予測モデルの構築を目指す．

\subsection{レビュー承諾の可否を判定}
学習指標の作成は図\ref{fig:approach}に示し，具体的には次の手順で行う．
図中の丸は，開発者の活動日を示しており，T,Fはそれぞれ，各活動から見て指定期間後の活動の有無を示す．

\begin{enumerate}
    \item 各開発者の活動日から活動月を特定し，その月の最終日を基準点とする
    \begin{itemize}
        \item 活動日が2022-04-05, 2022-04-14の場合 $\rightarrow$ 2022-04-31を基準点とする
    \end{itemize}
    \item 基準点から指定期間の間で将来の活動があるか調査（例：指定期間が0-3mの場合）
    \begin{itemize}
        \item 2022-04-31（基準点）から2022-07-31（指定期間）にレビュー依頼があるか調査
    \end{itemize}
    \item 2.の期間でレビュー依頼を受け入れるか否かを判定
    \begin{itemize}
        \item レビュー承諾 $\rightarrow$ True, レビュー拒否 $\rightarrow$ False
    \end{itemize}
        \item その月の全活動に同じラベルを付与する
    \begin{itemize}
        \item 4月の全活動にTまたはFのラベルを付与
    \end{itemize}
\end{enumerate}
上記の1$\sim$4を訓練期間内の各月で行い開発者の各活動にラベル付を行い学習指標を作成する．

% \subsubsection{ラベルの学習時の役割}
% このラベルは，逆強化学習の際に単なる過去の事実としてではなく，その時点での状態が，将来の継続とどう関連しているかを学習するための教師信号として機能する．具体的には，Trueのラベルからは，継続したレビュアーの活動から継続を促進する要因を学習し，Falseのラベルからは離脱したレビュアーのパターンから離脱のシグナルを学習することができる．

\subsection{学習指標の役割}
本研究で用いる学習指標は，逆強化学習において単なる過去の事実を示すものではなく，レビュー依頼時の状態が将来の継続にどのように影響するかを学習するための信号として機能する．具体的には，Trueの学習指標からは，レビュアーが貢献を継続するする要因を学習し，Falseの学習指標からはレビュアーが依頼に反応しない行動パターンを示す特徴を学習する．


\section{状態・行動特徴量の生成}
本研究では，逆強化学習を行うにあたり，10個の状態特徴量(表\ref{table:stateList})と，4個の行動特徴量（表\ref{table:actionList}）を使用する．

\begin{table}[h]
    \centering
    \caption{状態特徴量（10次元）}
    \label{table:stateList}
    \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{特徴量名} & \textbf{定義} \\
        \midrule
        \texttt{経験日数} & 初回活動から現在までの日数 \\
        \texttt{総レビュー依頼数} & これまでのレビュー依頼数 \\
        \texttt{総レビュー数} & これまでのレビュー数 \\
        \texttt{最近の活動頻度} & 1日あたりの平均活動量 \\
        \texttt{平均活動間隔} &  タイムスタンプ間の平均日数 \\
        \texttt{活動トレンド} & 月活動比が増加したかどうか \\
        \texttt{協力スコア} & 共同レビュー回数 \\ 
        \texttt{総承諾率} & これまでのレビュー承諾率 \\
        \texttt{最近の受諾率} & 直近10件のレビュー依頼受諾数 \\
        \texttt{レビュー負荷} & 未完了レビュー数 \\
        \bottomrule
    \end{tabular}
\end{table}

\begin{table}[h]
    \centering
    \caption{行動特徴量（4次元）}
    \label{table:actionList}
        \begin{tabular}{@{}ll@{}}
        \toprule
        \textbf{特徴量名} & \textbf{定義} \\
        \midrule
        \texttt{強度} & レビューするファイル数 \\
        \texttt{協力度} & その人をレビューした回数 \\
        \texttt{応答速度} & レビューリクエストから応答までの日数 \\
        \texttt{レビュー規模} & 1回のレビューで変更された総行数 \\
        \bottomrule
    \end{tabular}
\end{table}





\section{モデル構造と学習フロー}本研究で学習に用いるニューラルネットワークは，エンコーダ，LSTM，デコーダから構成される．
\subsubsection{エンコーダ}
レビュアーの各活動を表す状態 (State) および行動 (Action) の2つの特徴量ベクトルを，それぞれ独立したエンコーダ（多層パーセプトロン)に入力する．これらのエンコーダは元のベクトルよりもより表現力のある高次元のベクトルに変換する役割を持つ．
\subsubsection{LSTM}
各時点でエンコードされた状態ベクトルと行動ベクトルを連結し，時系列プロセッサであるLSTM (Long Short-Term Memory) に入力する．LSTMは活動履歴を記憶し，時間的な依存関係を理解するため，単一の活動だけでなく，活動量が徐々に低下する動的なパターンも把握することができる．

\subsubsection{デコーダ}
LSTMによりレビュアーの全活動が処理された後，LSTMで得られた時系列情報（特徴ベクトル）をもとに多層パーセプトロンで最終的な予測値を算出する．ここではLSTMで処理されたレビュアーの全活動履歴を考慮した情報をデーコードし，シグモイド関数により最終的な継続確率を算出する.

\subsection{報酬関数}
逆強化学習では，専門家の行動を説明する報酬関数を学習する．本研究ではレビュアーがレビュー依頼を承諾するか，あるいは承諾しないといった行動を，継続的な意思決定の結果として捉える．具体的には，レビュアーが状態$s$において，レビュー依頼を受け入れる，または反応しないという行動$a$を選択した際の評価を，報酬関数$R(s, a; \theta)$として表現し，ニューラルネットワークのパラメータ$\theta$を学習する．

学習過程では，レビュー依頼を継続して承諾しているレビュアーの行動軌跡に対しては高い累積報酬が与えられるようにし，レビュー依頼が承諾されない行動軌跡に対しては低い累積報酬が与えられるように報酬関数を最適化する．ここで行動軌跡とは，レビュー依頼時点での状態と，それに対する行動の選択から構成される時系列データを指す．推定される報酬関数は，レビュアーが過去にどの程度継続してコードレビューに関与してきたのかを反映し，将来もレビュー依頼を受け入れやすい状態にあるかどうかを数値として表現したものであると解釈できる．

継続確率$P$は，推定された報酬関数に基づく行動軌跡の累積報酬をシグモイド関数$\sigma$に入力することにより算出される．累積報酬が高いほど継続確率$P$は高くなり，累積報酬が低いほど継続確率$P$は低くなる．

学習時には4.2節で定義した学習指標を用い，予測確率Pと実際の結果との誤差を計算するために損失関数を用いる．この誤差が最小となるように，最適化アルゴリズム (Adam Optimizer) で反復的に学習を行うことにより，継続してレビュー承諾をするレビュアーの行動パターンを高く評価し，離脱者のパターンを低く評価する報酬関数を推定する．

% 逆強化学習では専門家の行動を説明する報酬関数を学習する．具体的には，状態sで行動aを選択することの価値を表す報酬関数R(s,a；θ)を，ニューラルネットワーぅのパラメータθとして学習する．本研究での専門家は継続したレビュアーを指し，非専門家は離脱したレビュアーを指す．学習においては，専門家（継続者）の活動軌跡に対しては高い累積報酬を与え，非専門家（離脱者）の活動軌跡に対しては低い累積報酬を与えるように，報酬関数R(s,a;θ)を調整する．

\subsection{損失関数による報酬関数の最適化}

本研究では，推定された報酬関数に基づいて算出される継続確率と，実際に観測されたレビュー承諾の有無との誤差を最小化するために，損失関数を用いる．本研究では，将来の指定期間内にレビュー承諾があるか否かを予測する二値分類問題として定式化することができる．
従来研究では多くの開発者が長期貢献者と呼ばれるまでに離脱することが知られている\cite{OTC}.そのため，LTC候補者予測のデータセットは正例（LTC候補者）と負例（LTC候補者以外の開発者）の間で学習データ数に不均衡が生じる．
このような不均衡なデータに対しては適切な損失関数を用いることが重要である．

二値クロスエントロピー (BCE: Binary Cross-Entropy)は，二値分類において一般的に用いられる損失関数であり，モデルの予測確率と正解ラベルとの差を評価する．正解に近い予測ほど損失は小さく，誤った予測ほど損失が大きくなる性質を持つ．二値クロスエントロピーは次の式で定義される\cite{BCE-FocalLoss}．

\[\textbf{二値クロスエントロピー：Binary Cross-Entropy (BCE)}\]
\[
L_{\mathrm{BCE}} = - \frac{1}{N} \sum_{i=1}^{N} \left[ y_i\log(p_i) + (1 - y_i)\log(1 - p_i)\right]
\]
ここで，$N$はデータセットのサンプル数，$y_i \in \{0, 1\}$は$y_i$の正解ラベル，モデルが予測したクラスの確率は$p_i \in [0, 1]$を示す．

\[p_i = \sigma(z_i) = \frac{1}{1 + e^{-z_i}}\]はシグモイド関数を表す．

不均衡データに対してBCEを用いた場合，容易に分類できる多数派クラスのサンプルの影響が大きくなり，少数派クラスに対応する重要な行動パターンの学習が不十分となる可能性がある．
そこで，本研究では不均衡データによる問題を対処するため，BCEを拡張した損失関数である，Focal Lossを用いる．FocalLossは予測が容易なデータの影響を抑え，予測が困難なデータをより重視して学習を行うことを目的としており，次の式で定義される．\cite{BCE-FocalLoss}


\[\textbf{Focal Loss (FL)}\]
\begin{align}
L_{\mathrm{FL}} = - \frac{1}{N} \sum_{i=1}^{N} \Bigl( & \alpha (1 - p_i)^{\gamma} y_i \log(p_i) \nonumber \\
& + (1 - \alpha) p_i^{\gamma} (1 - y_i) \log(1 - p_i) \Bigr) \nonumber 
\end{align}


ここで，
\(\alpha \in [0,1]\) はクラス不均衡を補正する重みパラメータを示し，
\(\gamma \ge 0\) は難しいサンプルを強調する焦点パラメータである．  
\(\gamma = 0\) のときは，Focal Loss は BCE に一致し，レビュー承諾の有無を各サンプルの難易度を区別せずに学習する場合に相当する．



% \subsubsection{報酬関数}
\begin{figure*}[t]
    \centering
    \includegraphics[width = 1.0\textwidth]{./Hashimoto_fig/prediction.pdf}
    \caption{学習から予測のフロー}
    \label{fig:flow}
\end{figure*}
















% \subsection{Focal Lossによる報酬関数の最適化}




\section{予測・評価}
% 本研究では，学習時に予測時点以降の情報を使用することを防ぐために，学習期間と予測期間を分離するだけでなく，特徴量計算に使用できるデータの最大時点 (Max-date) を設けている．
% 具体的には，各時点においてはその時点以前のデータから特徴量を計算し，その月末の時点で継続ラベルを付与するが，Max-date以降のデータはラベル生成の際のみ使用する．これにより，モデルが将来の情報を事前に学習することを防ぐことができる．
% ここで，最適化された報酬関数を用いて，特徴量に対する報酬を算出する．この報酬値にシグモイド関数を適応することで0\textasciitilde1の継続確率に変換することで予測を行う．

本研究では，学習時に予測時点以降の情報を使用することを防ぐため，学習期間と予測期間を分離するだけでなく，特徴量計算に使用可能なデータの最大時点（Max-date）を設定している．具体的には，各時点においてはその時点以前のデータから特徴量を算出し，その月末時点で継続の有無を示す教師信号を付与する．一方，Max-date以降のデータは教師信号の生成の際のみに使用する．これにより，モデルが将来の情報を事前に学習してしまうことを防止できる．
さらに，最適化された報酬関数を用いて各特徴量に対する報酬を算出し，この報酬値にシグモイド関数を適用して0から1までの範囲に正規化することで，モデルの予測結果としての継続確率を得る．この確率Pが，次のレビュー依頼にレビュアーが受け入れるか否かの予測値として用いられる．


% \begin{figure*}[t]
%     \centering
% \includegraphics[width=1.0\textwidth]{./Hashimoto_fig/heatmap.pdf}
%     \caption{各モデルのクロス評価）}
%     \label{fig:result}
% \end{figure*}


\todo{結果・考察を増やして評価方法を体系的に書くイメージ RQ1で学習期間も予測評価する必要性がわかるようにも書く}
\subsection{評価指標}
 % 評価指標にはPression,Recall,F1スコア，AUC-ROC(ROC 曲線下面積)，AUC-PR(Precision-Recall 曲線下面積)の５つを用いる
 % AUC-ROCでは，閾値に依存しない全体的な分類性能を評価し，継続者と離脱者を区別するモデルの基本的な能力を図る．さらに，本研究が用いるデータは，継続者（正例）と離脱者（負例）の差が顕著であり不均衡なクラスになっているため，不均衡なデータに対して予測性能をより適切に評価できることが知られているAUC-PRを使用する．本研究ではこのAUC-ROCの値を重視する．

本研究では，4つの評価指標（適合率，再現率，F1値，AUC-ROC）を用いる．
本研究における二値分類の混同行列の構成要素（TP, FP, TN, FN）は以下のように定義される．
\begin{itemize}
    \item \textbf{真陽性 (TP)}: 実際にレビュー依頼を受諾し，予測でも受諾すると正しく判定された件数
    \item \textbf{偽陽性 (FP)}: 実際には受諾していない（拒否または反応なし）が，予測では受諾すると誤って判定された件数
    \item \textbf{真陰性 (TN)}: 実際には受諾しておらず，予測でも受諾しないと正しく判定された件数
    \item \textbf{偽陰性 (FN)}: 実際にレビュー依頼を受諾したが，予測では受諾しないと誤って判定された件数
\end{itemize}

これらの値を用いて，各評価指標は次式で定義される．
\[
\text{適合率(Precision)} = \frac{TP}{TP + FP}, \quad
\text{再現率(Recall)} = \frac{TP}{TP + FN}
\]
\[
\text{F1値(F1 Score)} = \frac{2 \cdot \text{適合率} \cdot \text{再現率}}{\text{適合率} + \text{再現率}}
\]
AUC-ROCは閾値を変化させたときのROC曲線（横軸に偽陽性率，縦軸に真陽性率）下面積であり，閾値に依存せず，レビュー依頼を受け入れるレビュアーと反応しないレビュアーを区別するモデルの基本的な能力を総合的に評価する．ROC曲線に用いる真陽性率（TPR）と偽陽性率（FPR）は次式で定義する．
\[
\text{真陽性率(TPR)} = \frac{TP}{TP + FN}, \quad
\text{偽陽性率(FPR)} = \frac{FP}{FP + TN}
\]

\subsection{クロス評価}
従来研究では，予測時点以降の任意の時点（例えば，開発者がOSSプロジェクトに参加して1年後）以降における活動の有無を予測していた．本研究では，未来の任意の区間での活動の有無を予測し，この区間をスライドさせながら予測することで，どの学習方法が将来の活動の予測に最も寄与するかを明らかにする．また，各学習期間において，報酬を算出するために参照する指定期間も，同様にスライドする．具体的には，レビュー依頼を受けた時点の月の末日を基準に，学習時に参照する指定期間をスライドさせた4パターン（0から3か月，3から6か月，6から9か月，9から12か月）のモデルに対して，訓練期間 $\le$ 予測期間となるように予測期間をスライドさせた計10パターンで評価を行う．
以降の実験結果および考察において，基準点から$X$ヶ月後から$Y$ヶ月後の期間を表す際に「$X$-$Y$m」という表記を用いる（例：3ヶ月後から6ヶ月後の期間は3-6mとする）．また，特定の期間（例：3-6m）の貢献有無を学習したモデルを「3-6mモデル」のように表記する．

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{ケーススタディ}
\label{sec:casestudy}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{データセット}
% \subsubsection{対象プロジェクト}
本研究では，Gerritによる詳細なレビュー履歴が利用可能かつ大規模で十分なデータが蓄積されていることから，仮想マシンインスタンスのライフサイクル管理を行うOpenStackの中核サービスOpenStack/Novaを対象とした．
表\ref{table:dataset}は，分析対象プロジェクトの統計データを示す．

% \subsubsection{実験設計}
\begin{table}[h]
    \centering
    \caption{データセットの概要}
    \begin{tabularx}{\columnwidth}{XX}
        \hline
        項目 & 値 \\
        \hline
        対象期間 & 2021-01-01 ～ 2024-01-01 \\
        学習期間 & 2021-01-01 ～ 2023-01-01 \\
        予測期間 & 2023-01-01 ～ 2024-01-01 \\
        \hline
        総レビュー依頼数 & 約5,473件 \\
        訓練時レビュアー数 & 約70人 \\
        予測時レビュアー数 & 約60人 \\ 
        \hline
    \end{tabularx}
    \label{table:dataset}
\end{table}


\section{RQ1:逆強化学習に基づく提案モデルは，コードレビューにおけるタスク受け入れをどの程度予測できるか？}

%--------------------
% \begin{table}[h]
%     \label{table:train}
%     \centering
%     \caption{3-6mモデルの学習期間内における貢献予測結果}
%     \begin{tabularx}{\columnwidth}{XXXXX}
%         \hline
%        Precision & Recall & F1値 & AUC-ROC \\
%        \hline
%        0.909  & 0.556 & 0.689 & 0.818 \\ 
%        \hline
%     \end{tabularx}
% \end{table}
% %--------------------

% %--------------------
% \begin{table}[h]
%     \label{table:predict}
%     \centering
%     \caption{3-6mモデルで3-6m後の貢献予測結果}
%     \begin{tabularx}{\columnwidth}{XXXXX}
%         \hline
%        Precision & Recall & F1値 & AUC-ROC \\
%        \hline
%        0.769  & 0.556 & 0.645 & 0.820 \\ 
%        \hline
%     \end{tabularx}
% \end{table}
%--------------------
%--------------------
\begin{figure}[ht]
    \centering
\includegraphics[width=0.7\textwidth]{./Hashimoto_fig/RQ1.pdf}
    \caption{訓練・予測期間における予測結果}
    \label{fig:result1}
\end{figure}
%--------------------

RQ1では，構築したIRLのモデルが訓練対象の期間における開発の行動パターンを適切に学習し妥当な予測を行うことができるのか，また過学習になっていないかを検証することを目的としている．
設定した学習期間のうち，最も短期的な予測（0-3m）と最も長期的な予測（9-12m）の中間に位置し，モデルの標準的な挙動を確認するために3-6mモデルを代表例として取り上げ，評価を行う．

図\ref{fig:result1}は，3-6ヶ月後に貢献があるかをラベル付し学習した3-6mのモデルにおける，学習期間での予測の精度と，予測期間での予測の精度を示している．具体的には，Precision, recall, F1 Score, AUC-ROCの4つの評価指標を用いており，各指標のグラフの左側が訓練期間，右側が予測期間の結果を示している.

予測期間と評価期間の両期間における各指標の値を比較すると，大きな乖離が見られず同程度の精度を維持していることが確認できる．この結果は，提案モデルが学習データに過剰に適合しておらず，学習データに含まれない将来の期間（未知の期間）におけるレビュアーの行動予測に対しても，汎化性能を有していることを示唆する．
また，評価期間の予測におけるAUC-ROCが0.820であり，提案モデルがレビュー依頼を承諾する貢献者と離脱者を効果的に区別できることを示している．

\section{RQ2:学習方法や予測期間の長さに応じて，予測モデルの精度はどのように変化するか?}

%--------------------
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.8\textwidth]{./Hashimoto_fig/IRLheatmap.pdf}
    \caption{IRLの各モデルのクロス評価}
    \label{fig:IRLheatmap}
    
    \vspace{1cm}
    
    \includegraphics[width=0.8\textwidth]{./Hashimoto_fig/RFheatmap.pdf}
    \caption{RFの各モデルのクロス評価}
    \label{fig:RFheatmap}
\end{figure}
%--------------------

% 図4のヒートマップは縦軸を予測する区間でスライドさせたものであり，横軸は学習時に付与するラベルの指定期間をスライドさせたものである．それぞれのモデルでスライドさせた予測期間を5つの評価指標で評価し．ヒートマップを作成した結果である．学習時のラベル付の期間と予測期間が一致する際に予測精度が最も高くなることが期待されていたが，結果として，全てのモデルにおいて，予測期間3-6mでAUC-PRが最大になった．他の予測期間においても，全てのモデルで同様の結果が見られ，AUC-PRが高いものから順に並べると3-6m, 0-3m, 9-12m, 6-9mの順になった．

RQ2では，学習期間から予測期間までの時間的距離がモデル性能に与える影響についてクロス評価を分析する．図\ref{fig:IRLheatmap}は，学習期間と予測期間の組み合わせによる精度の違いを，4つの評価指標を用いて評価した結果をヒートマップで示している．横軸は訓練期間，縦軸は評価期間を示す．
本実験では，学習データの期間設定と予測期間設定が同一である場合に，データの性質が最も近くなるため予測精度が高くなると予想される．しかし，結果として学習期間と同一の予測期間よりも，さらに後の期間（例：0-3mモデルにおける3-6mや6-9mなど）でAUC-ROCが高くなる傾向が確認された．3-6mモデルはに評価期間に関わらずAUC-ROCが高く，9-12mモデルではAUC-ROCが低下することがわかった．
さらに，比較対象として多くの既存手法で用いられているRF(Random Forest)を採用し，同様の評価を行なった．
図\ref{fig:RFheatmap}は，RFを用いて4つの評価指標を用いて評価した結果を示す．
RFは評価期間がAUC-ROCに着目すると6-9mにおいて一貫して精度が高くなっていた．
評価期間が6-9m以外のパターンで，IRLがRFのAUC-ROCを上回った．

\section{RQ3:推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か？}
RQ3では，提案モデル（IRL）および比較モデル（RF）を用いて特徴量の重要度分析を行い，継続的なタスク受諾に寄与する要因を明らかにする．
比較手法であるRFモデルの重要度分析に関しては，Gini係数に基づく重要度の分析を行なった．これは，決定木の不純度の減少量を基準としており，その特徴量が分類精度向上にどの程度寄与するのかを示す指標である．

また，提案手法である，IRLの重要度分析に関しては，ニューラルネットワークによって近似された報酬関数 $R(s,a)$ を解析対象とし分析を行なった．ここでは，入力する特徴量の変化が，継続確率である報酬値に与える影響を分析するために，勾配ベースの重要度分析を行なった．具体的には，学習済みモデルにおいて入力特徴量に対する報酬関数の偏微分係数（勾配）を算出する．これは，ある特徴量の値を微小に変化させた際，出力である報酬値がどの程度変動するかを定量化するものである．例えば，ある特徴量の入力値を僅かに増加させた結果，出力される報酬値が増加すればその特徴量は継続を促進する，正の影響を与えていると解釈できる．逆に，入力の増加に伴って報酬値が減少すればそれは継続を阻害する，負の影響を与えていることになる．また，値が0に近い場合はその特徴量が予測（報酬値の増減）に与える影響が小さいことを示す．
図~\ref{fig:IRLimportance}はIRL，図~\ref{fig:RFimportance}はRFにおける各特徴量の重要度をそれぞれモデル別で分析した結果である．IRLでは，レビュアーの貢献を予測する状態特徴量として総レビュー数が，行動特徴量としては協力度が強い正の影響を与えていることが明らかになった．また，平均活動間隔，レビュー規模，総承諾率は強い負の影響を与えていることが明らかになった．さらに，予測する期間が長くなるにつれ，重要とされる特徴量が総レビュー数から協力度に変化した．
次にRFでは，主に総レビュー依頼数が重要であり，9-12mのモデルにおいては，これまでのレビューの承諾率である総承諾率やレビュー負荷が重要であることがわかった．


\begin{figure}[ht]
    \centering
    \hspace*{2cm}
    \includegraphics[width=0.9\textwidth]{./Hashimoto_fig/irl_importance.pdf}
    \caption{各特徴量の予測における重要度(IRL)}
    \label{fig:IRLimportance}
    
    \vspace{1cm}
    
    \hspace*{2cm}
    \includegraphics[width=0.9\textwidth]{./Hashimoto_fig/rf_importance.pdf}
    \caption{各特徴量の予測における重要度(RF)}
    \label{fig:RFimportance}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{考察}
\label{sec:discussion}
\section{時系列データの考慮が予測に与える影響}

~図\ref{fig:IRLheatmap}から，IRLにおいて，訓練期間が3-6mであるモデルと，他の訓練期間のモデルと比較してAUC-ROCが平均的に高くなることがわかる．これは分析対象とするnovaプロジェクトが3ヶ月ごとにリリースしていることが一要因として考えられる．また~図\ref{fig:IRLheatmap}，~図\ref{fig:RFheatmap}から，RFはIRLと比較して，AUC-ROCの最大値が0.928であり，IRLの最大値である0.910より高くなることがわかった．しかし，IRLはRFと比較し，評価期間が6-9mの場合を除いたすべてのパターンでAUC-ROCが高くなることが明らかになった．

これらのことから，RFはIRLに比べ高い精度を出すことができ，学習したパターンにうまく当てはまる評価期間では高い精度が出るが，IRLでは，時系列的な開発者の行動パターンやプロジェクトの特性を上手く学習するため，最大値では劣るものの，多くの評価パターンでRFの精度を上回ることができる考えられる．
%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{短期予測と長期予測における特徴量の違い}
図\ref{fig:IRLimportance}に示す通り，IRLにおいて各モデルにおいて寄与する特徴量の変化が確認され，初期段階では総レビュー依頼数やレビュー数などの活動量が予測に大きく寄与したが，期間が進むにつれてその影響が減少し，協力度や活動間隔やレビュー強度といった活動量以外の要因の寄与が高まった．これは，0-3mのモデルで，予測区間が9-12mにまで離れると，活動量を重視した予測を行うが，活動量以外の特徴量も重要となるために，予測精度が低下する原因になると考えられる．これらの結果から，短期間の予測では活動量による影響が強く，長期間になるにつれて協力度や応答速度といった行動の量ではなく質的な要因の影響が高くなると考える．また，レビュー規模，先月の活動回数と比較した活動回数の上昇率である活動トレンドは，一貫して負の影響を示し，レビュー負荷も負の値であるモデルが多いことから，レビュー規模や回数がかかり過ぎるとレビュアーへの負担が増加し，離脱の要因となることが示唆される．また平均活動間隔においても一貫して負の影響を示し，活動間隔が増加することもレビュアーの離脱の要因となることが示唆される．

一方で，RFに関しては~図\ref{fig:RFimportance}に示す通り，モデルによって最も重要とされる特徴量の変化は見られたものの，どのモデルにおいても，総レビュー依頼数，総承諾率といった活動量を示す特徴量が最も重要な特徴量であった．RFにおいては，時系列的なタスクのつながりを持たないために，活動トレンドの重要度が低くなっていることが考えられる．

また，IRLにおいては，報酬を増加させることが直接タスクの受け入れ確率につながるため，タスクの受け入れ確率を向上させるため，RFと比較してレビュアーに対してどのような行動を起こすと良いのかを判断し易く，プロジェクトのマネジメントに有効であると考える．


\section{IRL/RFにおいて予測不一致の開発者について}
\subsubsection{レビュー回数は比較的多いが，レビュー受諾率が低いレビュアー}
レビュー回数が比較的多いが，レビュー承諾率が低いレビュアーの予測において，RFはは誤ってレビュー承諾を行うと予測し，IRLでは正しくレビュー承諾を行わないといった予測する事例が見られた．これは，RFではレビュー回数が多いというルールに基づき，誤ってレビュー承諾をすると予測し，一方でIRLでは時系列的な承諾率などの活動の変化や協力度といったことを考慮することで，正しくレビュー承諾をしないといった予測ができたと考える．
\subsubsection{レビュー回数が少ないが，レビュー受諾率が高いレビュアー}
レビュー回数が非常に少ないが，レビューの承諾率は高いレビュアーの予測において，RFは誤ってレビュー承諾を行わないと予測し，IRLでは正しくレビュー承諾を行うと予測する事例が見られた．これは，RFでは先ほどと同様にレビュー回数が少ないといった条件によって，誤って離脱すると予測し，一方で，IRLでは継続してレビュー数は少ないがレビュー承諾率は高いといった時系列での活動パターンを考慮したことによ正しくレビュー承諾すると予測ができたと考える．
\subsubsection{}



\chapter{妥当性の脅威}
\label{sec:validity threat}
\section{内的妥当性の脅威}
\todo{}
本研究では，レビュアーの状態と行動を表すために，それぞれ10次元と4次元の特徴量を定義したが，これらの特徴量がレビュアーのタスク受け入れの意思決定の全てを網羅しているわけではない．しかし，本研究では，レビュー活動の代表的な特徴量である，活動の量や質，社会性といった多面的な視点をカバーしているためOSSレビュアーの多様な行動の傾向を理解するための一指標として有効であると考える．また，逆強化学習は観測されたデータからその背後にある行動原理（報酬関数）を推定するものであり，未観測のデータに対してもある程度の頑健性を持つことが考えられる．

\section{外的妥当性の脅威}
\todo{}

本研究では，OpenStack/Novaという単一プロジェクトを対象としている．OpenStack/Novaは独自の開発形態やレビュープロセスを持っており，本研究で得られた結果が，他のドメインや開発プロセスを持つプロジェクト（例：GitHubを使用しているプロジェクトなど）においては，今回の結果とは異なる新たな知見が得られる可能性がある．しかし，OpenStack/Novaは，大規模かつ長期間運用されている代表的なプロジェクトであるため，OSS開発における継続的な貢献パターンの一般的プロジェクトの一例として有効な知見が得られると考える．

また．分析対象とした期間（2021年〜2024年）において，対象プロジェクトであるnovaプロジェクトはプロジェクトを立ち上げてから十分な期間が経過しており，成熟したプロジェクトを対象にしている．そのため，プロジェクトの立ち上げ期や，成長のフェーズが異なる場合には，重要となる特徴量が異なる可能性がある．しかし，成熟したプロジェクトにおいても開発者の入れ替わりが発生しており，新規開発者が多数存在する．そのため，新規開発者がどのようにコミュニティに定着していくのかを学習・予測するといった流れをとっているため，成熟期のプロジェクトを対象にすることで，長期的な貢献者と新規の開発者の両者を学習することができるため妥当であると考える．

%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{おわりに}

\todo{はじめに同様最後にまとめます}
\label{sec:conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%
本研究では，コードレビュープロセスを対象に，長期的貢献者の予測を行うモデルを提案した．従来では将来の単一時点での貢献の有無を予測していたのに対し，本手法は逆強化学習を用いることで活動履歴を時系列で捉え，レビュアーの行動を説明する報酬関数を推定し，将来の単一時点ではなく，連続的な区間での貢献の有無を予測することを試みた．

今後の課題としては，妥当性の脅威に対処していくために，GitHubを利用しているプロジェクトなどを対象とした追加分析を行うこと，本研究では考慮しなかったタスクの専門性の一致度などといった特徴量を追加し予測精度の向上を目指す．さらに本研究では単一プロジェクトを対象にしていたが，複数プロジェクトを対象に分析を行うことで，プロジェクト間でのレビュアーの動きを捉えることを目指す．


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 謝辞
%%
%% \begin{acknowledgements}
%% 感謝します．
%% \end{acknowledgements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 参考文献
%%
\bibliographystyle{junsrt}
\bibliography{@Bachelor2025_Hashimoto/references}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% 付録
%%
% \appendix
% 
% \chapter{サンプルプログラム}
% 
% プログラムリストや実行結果など，本論を補足する上で必要と思われるものが
% あれば付録として付ける．
% 
% {
% \footnotesize
% \begin{verbatim}
% #include <stdio.h>
% int main(void)
% {
%     printf("Hello, World!\n");
%     return 0;
% }
% \end{verbatim}
% }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\end{document}
