# 卒業論文 包括的サマリー：Nova 単体プロジェクトにおける IRL ベースのレビュア継続予測

## 研究概要

**タイトル**: 逆強化学習を用いたコードレビューにおける長期貢献者予測

**対象プロジェクト**: OpenStack/Nova（単一プロジェクト）

**分析期間**: 2011 年～ 2023 年（12 年間）

**対象開発者**: 60 名のレビュアー

**モデル構成**:

- 状態特徴量（State）: **10 次元**
- 行動特徴量（Action）: **4 次元**
- **合計 14 次元**（マルチプロジェクト特徴量は不使用）

---

## 第 1 章：序論

### 1.1 背景

オープンソースソフトウェア（OSS）プロジェクトでは，コードレビューが品質保証の要となる．しかし，レビュアーの継続的な参加を確保することは困難である．

### 1.2 課題

従来の機械学習手法は，**単一時点のスナップショット**から予測を行うため，活動パターンの**動的な変化**を捉えることが難しい．

### 1.3 提案手法

本研究では，**逆強化学習（IRL）と LSTM**を組み合わせることで，レビュアーの時系列的な活動履歴から報酬関数を推定し，継続予測を行う．

### 1.4 研究課題（RQ）

1. **RQ1**: 提案モデルの予測精度は？
   本節では，訓練期間と評価期間の全組み合わせ（10 パターン）において，各開発者に対する IRL と**RF Case2（Simple Baseline）**の予測精度を比較する．使用するデータは以下の通り：

- **IRL 予測結果**: `results/review_continuation_cross_eval_nova/train_{0-3,3-6,6-9,9-12}m/eval_{0-3,3-6,6-9,9-12}m/predictions.csv`
- **RF 予測結果**: `outputs/singleproject/rf_nova_case2_simple/results.json`（各期間 1 スナップショット，データ水増し無し）
- **統合分析結果**: Case2 向けの開発者別集計（内部集計スクリプト出力）

**主要な定量結果（10 パターン平均）**:

- F1 スコア: IRL **0.67（範囲 0.59–0.71，σ=0.038）** vs RF Case2 **0.60（範囲 0.50–0.67，σ=0.052）**
- AUC-ROC: IRL **0.827（範囲 0.746–0.910）** vs RF Case2 **0.763（最大 0.840）**

**開発者レベルでの特徴**:

- IRL は時系列パターンを捉えるため，期間が変わっても性能のばらつきが小さい（F1 標準偏差で Case2 比 ▲0.014）。
- Case2 は単発スナップショットで学習するため，期間が変わると精度が大きく揺れやすい。特に「0-3m → 0-3m」で F1=0.50 と乱数レベルまで低下したが，「3-6m → 3-6m」では 0.67 まで回復するなど不安定。
- AUC-ROC でも IRL が全域で優位かつ安定し，クロステンポラルな長期予測（0-3m→6-9m）の 0.910 が突出した。

**実務的含意**:

- データ量が限られ、期間が移動する実運用では、Case2 のような静的集約モデルは性能変動が大きく、運用上のリスクが高い。
- IRL は少数サンプルでも安定しており、期間が変わる運用でも再学習頻度を抑えられる。対抗馬を Case2 に固定しても、この安定性が優位性として残る。

### 4.2 状態特徴量（10 次元） - Nova 単体

| #   | 特徴量名 | 定義               | 正規化方法               |
| --- | -------- | ------------------ | ------------------------ |
| 1   | 経験日数 | 初回活動からの日数 | `min(days / 730.0, 1.0)` |

本節では，5.5.9 節の開発者レベル分析を補足し，10 パターン全体の AUC-ROC と F1 を俯瞰する。

**データソース**:

- **IRL**: `results/review_continuation_cross_eval_nova/matrix_AUC_ROC.csv`
- **RF Case2（Simple Baseline）**: `outputs/singleproject/rf_nova_case2_simple/results.json`

**パターン別で確認できた傾向**:

- IRL は「0-3m → 6-9m」で AUC-ROC 0.910 を記録し、長期クロステンポラル予測で突出。F1 も 0.64 と高水準を維持。
- RF Case2 は「0-3m → 0-3m」で F1 0.50 と乱数レベルに近いが、「3-6m → 3-6m」で F1 0.67 を記録し、同期間予測では一定の強みを持つ。
- AUC-ROC の平均は IRL 0.827 に対し RF Case2 0.763 で、全パターンを通じて IRL が総合優位。ばらつきは IRL の方が小さく、運用安定性に寄与する。

**まとめ**:

- 対抗馬を Case2（データ水増し無しのシンプル RF）に揃えると、IRL は精度・安定性の両面で上回る。
- Case2 は実装容易だが、期間が変わるたびに性能が大きく揺れるため、長期運用でのリスクが高い。IRL は少量データでも安定するため、再学習頻度やデータ準備コストを抑えられる。

Case2 は各訓練期間から 1 スナップショットのみを生成するため，訓練サンプルは各パターン 60 ～ 80 件程度に留まる。一方 IRL は評価開始日までの全履歴を時系列のまま取り込み，開発者あたり 1000 件規模の系列を LSTM で学習する。

両モデルの 10 パターン平均指標（F1・AUC-ROC）は以下の通り（RF は Simple Baseline，IRL は`matrix_AUC_ROC.csv`と同一のラベル定義）。

| モデル                 | 平均 F1  | F1 範囲   | 平均 AUC-ROC | AUC 範囲    | 備考                               |
| ---------------------- | -------- | --------- | ------------ | ----------- | ---------------------------------- |
| **IRL + LSTM（提案）** | **0.67** | 0.59–0.71 | **0.827**    | 0.746–0.910 | 時系列学習・勾配重要度             |
| RF Case2（Simple）     | 0.60     | 0.50–0.67 | 0.763        | ≤0.840      | スナップショット単発，時系列非考慮 |

**結論**: Case2 を対抗馬とすると，IRL は平均 F1 で**+0.07 ポイント**，平均 AUC-ROC で**+0.064 ポイント**上回り，特に AUC-ROC 0.910（0-3m→6-9m）など長期クロステンポラル設定で顕著な優位性を示した。ベースライン RF はデータ量が少なく，時系列パターンを保持しないため，期間が変わると性能変動が大きい（F1 標準偏差 0.052 vs IRL 0.038）。

本研究の主眼は予測精度の向上ではなく，**時系列パターンの学習と因果的特徴量重要度の解析**にある。Case2 を対抗馬としても，IRL は安定性と因果的解釈の両面で優位性を示す。

---

### 5.3 RQ2 の結果：汎化性能

**クロス評価（異なる訓練期間での評価）**:

**データソース**: `results/review_continuation_cross_eval_nova/matrix_AUC_ROC.csv`

| 訓練期間 → 評価期間 | 0-3m       | 3-6m       | 6-9m       | 9-12m      |
| ------------------- | ---------- | ---------- | ---------- | ---------- |
| 0-3m                | **0.7167** | 0.8228     | **0.9098** | 0.7337     |
| 3-6m                | 0.7241     | **0.8198** | **0.8939** | 0.8016     |
| 6-9m                | 0.6728     | 0.7898     | **0.7851** | 0.8315     |
| 9-12m               | 0.5653     | 0.7147     | 0.6552     | **0.6929** |

**重要な観察**:

1. **対角線が必ずしも最高ではない**: 0-3m 訓練 →6-9m 評価(0.9098)が 0-3m 訓練 →0-3m 評価(0.7167)を大きく上回る
2. **6-9m 評価期間への予測が全般的に高い**: 0-3m 訓練 →6-9m(0.9098)，3-6m 訓練 →6-9m(0.8939)
3. **9-12m 訓練モデルは全般的に低い**: 対角線含め全ての評価期間で 0.6 ～ 0.7 台
4. **早期訓練モデルの中期汎化能力**: 0-3m 訓練モデルが 6-9m 評価で最高精度(0.9098)を達成

**結論**: IRL は対角線パターンよりも，特定の非対角線パターン（6-9m 評価期間への予測）で高い精度を示す．これは時系列学習による汎化能力を示唆している．

---

### 5.4 RQ3：勾配ベースの特徴量重要度分析

**研究課題**: RQ3「どの特徴量が継続予測に重要か？」

#### 5.4.0 RQ3 の実験設定と手法

本節では，IRL モデルが学習した報酬関数を用いて，各特徴量が継続予測に与える因果的影響を定量化する．従来の Random Forest における Gini 係数ベースの重要度は相関関係を示すのみであり，介入可能性（どの特徴を変化させれば継続に効くか）を示さない．本研究では，勾配ベースの重要度分析により，**特徴量の変化が報酬（継続確率）に与える因果的影響**を測定する．

**実験データ**:

本分析では，以下のデータを使用する：

- **IRL モデル**: `results/review_continuation_cross_eval_nova/train_{0-3,3-6,6-9,9-12}m/`に保存された訓練済みモデル（4 つの訓練期間）
- **評価データ**: 各訓練期間に対応する 60 名のレビュアーの活動系列（LSTM 入力形式）
- **出力結果**:
  - 平均勾配重要度: `average_feature_importance/gradient_importance_average.json`（4 期間の平均）
  - 期間別勾配重要度: `feature_importance_{0_3,3_6,6_9,9_12}/gradient_importance.json`

**対抗手法との比較**:

本研究では，IRL の勾配ベース重要度と Random Forest Case2 の Gini 係数ベース重要度を比較する．

- **RF Case2**: `outputs/singleproject/rf_nova_case2_simple/` のモデルを使用
- **特徴量重要度**: scikit-learn の `feature_importances_` 属性（Gini 係数ベース）
- **比較目的**: 相関ベース（RF）vs 因果ベース（IRL）の解釈可能性の違いを明確化

**勾配ベース重要度の計算手順**:

本研究では，以下の 7 ステップで勾配重要度を計算する．各ステップの詳細を文章で説明する．

**ステップ 1: 訓練済み IRL モデルの読み込み**

各訓練期間（0-3m, 3-6m, 6-9m, 9-12m）について，すでに学習が完了した報酬関数モデルを読み込む．このモデルは，LSTM と全結合層から構成される深層ニューラルネットワークであり，状態・行動の時系列から継続報酬を推定する機能を持つ．読み込み時には評価モード（学習を行わず推論のみ実行するモード）に設定し，パラメータの更新を防ぐ．

**ステップ 2: 評価データの準備**

60 名のレビュアーそれぞれについて，訓練期間における活動履歴を時系列データとして取得する．各レビュアーの活動履歴は，状態特徴量（10 次元）と行動特徴量（4 次元）の時系列として表現される．例えば，あるレビュアーが 100 件のレビューを行った場合，状態特徴量は 100 時刻 × 10 次元，行動特徴量は 100 時刻 × 4 次元の行列となる．各系列の長さ（時刻数）も記録しておく．これらのデータを LSTM の入力形式に変換し，テンソル（多次元配列）として準備する．

**ステップ 3: 勾配計算の有効化**

状態・行動のテンソルに対して，自動微分を有効化する設定を行う．通常，ニューラルネットワークではモデルのパラメータに対してのみ勾配を計算するが，本研究では**入力データ（特徴量）に対する勾配**を計算する必要がある．これにより，「特徴量を少し変化させたとき，報酬がどれだけ変化するか」を定量化できる．PyTorch では `requires_grad=True` というフラグを設定することで，任意のテンソルに対する勾配計算が可能になる．

**ステップ 4: Forward Pass（報酬値の計算）**

準備したテンソルを IRL モデルに入力し，各時刻における報酬値を計算する．具体的には，LSTM が状態・行動の時系列を受け取り，各時刻の隠れ状態を出力する．この隠れ状態をデコーダー（全結合層）に入力することで，各時刻における継続報酬の推定値が得られる．例えば，60 名のレビュアーがそれぞれ 100 時刻の系列を持つ場合，60 × 100 = 6000 個の報酬値が出力される．これらの報酬値の平均を取ることで，全体の平均継続報酬を計算する．

**ステップ 5: Backward Pass（勾配の計算）**

平均継続報酬に対して，自動微分を実行する．これにより，各特徴量が平均報酬にどれだけ寄与しているかを示す勾配が計算される．勾配とは，数学的には偏微分（∂報酬 / ∂特徴量）を意味し，「特徴量を 1 単位増加させたとき，報酬が何単位増加するか」を示す値である．PyTorch の自動微分機能により，LSTM を含む複雑なモデルに対しても正確な勾配が計算できる．計算された勾配は，状態特徴量については 60 名 × 100 時刻 × 10 次元，行動特徴量については 60 名 × 100 時刻 × 4 次元の形状を持つテンソルとして得られる．

**ステップ 6: 勾配の集約（時間軸・サンプル軸）**

各特徴量に対して，時間軸（各レビュアーの時系列方向）とサンプル軸（60 名のレビュアー方向）で勾配の平均を取る．勾配は正負の値を取るため，絶対値を取ってから平均する．これにより，「正の影響（報酬を増やす）」と「負の影響（報酬を減らす）」の両方を統合した，総合的な影響度を測定できる．例えば，状態特徴量の第 1 次元（経験日数）について，60 名 × 100 時刻 = 6000 個の勾配値の絶対値平均を取ることで，経験日数の平均的な重要度が得られる．この処理を全 14 次元の特徴量について実施し，各特徴量の重要度ベクトル（14 次元）を得る．

**ステップ 7: 訓練期間間での平均化**

上記のステップ 1～6 を，4 つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）それぞれについて実施する．これにより，各訓練期間における特徴量重要度が得られる．最後に，これら 4 つの重要度ベクトルの平均を取ることで，訓練期間に依存しない一般的な特徴量重要度を計算する．この平均化により，特定の期間に固有のノイズや偏りを排除し，より頑健な重要度推定が可能になる．計算結果は JSON 形式で保存し，後続の分析で使用する．

**勾配重要度の数学的定義**:

特徴量 x_i の勾配ベース重要度は，以下のように定義される：

```
Importance(x_i) = E[|∂R(s, a; θ) / ∂x_i|]

where:
  R(s, a; θ) = 報酬関数（状態s・行動aから継続報酬を推定）
  x_i = i番目の特徴量（状態または行動）
  E[·] = 全時刻・全開発者での期待値
  |·| = 絶対値（正負の影響を統合）
```

この定義により，「特徴量 x_i を小さく変化させたとき，報酬がどれだけ変化するか」という因果的影響を定量化できる．

**Random Forest Case2 の Gini 係数ベース重要度（比較対象）**:

対照的に，Random Forest Case2 の特徴量重要度は以下の手順で計算される．

まず，各訓練期間（0-3m, 3-6m, 6-9m, 9-12m）について，訓練期間の最終時点における開発者のスナップショットを 1 つずつ取得する．Case2 では時系列データをスナップショットに集約するため，各開発者について 1 サンプルのみが生成される．60 名のレビュアーがいる場合，訓練データは約 60～80 サンプル程度になる．

次に，scikit-learn の Random Forest Classifier（100 本の決定木で構成）を用いて，これらのスナップショットから継続/離脱を分類する二値分類器を学習する．各決定木は独立にブートストラップサンプリング（復元抽出）によって訓練データの一部を選び，各ノードでは Gini 係数（不純度）を最小化する特徴量で分岐を行う．

学習後，scikit-learn が自動的に計算する `feature_importances_` 属性を取得する．この値は，全決定木の全ノードにおいて，各特徴量が Gini 係数の減少にどれだけ寄与したかを，サンプル数で重み付けして集計したものである．例えば，応答速度という特徴量が多くの決定木の第 1 層（ルートノード）で使用され，不純度を大きく減少させた場合，応答速度の重要度は高くなる．

Gini 係数ベースの重要度は，「各特徴量が過去データの判別にどれだけ貢献したか」を測定する．これは過去データにおける統計的な判別貢献度であり，**相関関係**を示す．一方，勾配ベースの重要度は，**特徴量を変化させたときの因果的影響**を示すため，介入可能性の観点で優れている．

**両手法の違い（まとめ）**:

| 観点             | Random Forest（Gini）        | IRL（勾配ベース）            |
| ---------------- | ---------------------------- | ---------------------------- |
| **計算方法**     | Gini 係数の減少量            | ∂R/∂x_i の期待値             |
| **意味**         | 過去データでの判別貢献度     | 特徴変化が報酬に与える影響   |
| **解釈**         | 相関関係                     | 因果的影響                   |
| **介入可能性**   | 低い（属性ベース）           | 高い（行動変容を促せる）     |
| **時系列考慮**   | なし                         | あり（LSTM 経由で伝播）      |
| **データ依存性** | 訓練データの分布に強く依存   | 報酬関数の学習結果に基づく   |

**実験の妥当性**:

- **再現性**: 全実験は乱数シード固定（`random_state=42`）で実施
- **クロスバリデーション**: 4 つの訓練期間での平均を取ることで，期間依存性を排除
- **統計的検定**: 勾配値の標準誤差を計算し，有意性を確認（結果セクションで報告）

---

#### 5.4.2 勾配重要度の結果（平均値）

本節では，`results/review_continuation_cross_eval_nova/average_feature_importance/gradient_importance_average.json` に記録された平均勾配重要度に基づき，全特徴量のランキングを示す．この平均値は，4 つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）における勾配値の平均である．

**全特徴量のランキング（勾配値順）**:

| ランク | 特徴量名                 | カテゴリ   | 勾配値      | 符号   | 解釈                       |
| ------ | ------------------------ | ---------- | ----------- | ------ | -------------------------- |
| 1      | 総レビュー数             | State      | +0.0165     | 正     | 継続を促進（最重要）       |
| 2      | **協力度**               | **Action** | **+0.0131** | **正** | **協力的行動が継続を促進** |
| 3      | 総コミット数             | State      | +0.0123     | 正     | 継続を促進                 |
| 4      | 強度（ファイル数）       | Action     | +0.0089     | 正     | 継続を促進                 |
| 5      | 最近の活動頻度           | State      | +0.0071     | 正     | 継続を促進                 |
| 6      | 経験日数                 | State      | +0.0065     | 正     | 継続を促進                 |
| 7      | レビュー負荷             | State      | -0.0034     | 負     | 負荷増加で離脱             |
| 8      | 最近の受諾率             | State      | +0.0031     | 正     | 継続を促進                 |
| 9      | 活動トレンド             | State      | +0.0029     | 正     | 継続を促進                 |
| 10     | 応答速度                 | Action     | +0.0027     | 正     | 継続を促進（小）           |
| 11     | 協力スコア               | State      | +0.0015     | 正     | 継続を促進（小）           |
| 12     | **コード品質スコア**     | **State**  | **-0.0078** | **負** | **品質低下で離脱**         |
| 13     | **平均活動間隔**         | **State**  | **-0.0107** | **負** | **間隔増加で離脱**         |
| 14     | **レビュー規模（行数）** | **Action** | **-0.0124** | **負** | **大規模レビューで離脱**   |

#### 5.4.3 勾配の符号解釈

**正の勾配（+）**: 特徴量の増加 → 報酬増加 → **継続を促進**

- 例: 協力度 +0.0131 → 協力的な行動を取ると継続しやすい

**負の勾配（-）**: 特徴量の増加 → 報酬減少 → **離脱を促進**

- 例: 平均活動間隔 -0.0107 → 活動間隔が開くと離脱しやすい

#### 5.4.4 重要な発見（5 つ）

**発見 1: 協力度が行動特徴量で最重要（+0.0131）**

- 協力的なレビュー（同じレビュイーとの繰り返し関与）が継続に最も寄与
- 実務的示唆: ペアレビュー制度、メンタープログラムの導入

**発見 2: 平均活動間隔が強い負の影響（-0.0107）**

- 活動間隔が 60 日以上開くと離脱リスク増加
- 実務的示唆: 定期的なレビュー依頼、3 ヶ月以上の空白期間に警告

**発見 3: コード品質低下が離脱要因（-0.0078）**

- コード品質スコアが低下すると継続意欲減少
- 実務的示唆: 品質ガイドラインの整備、品質低下時のアラート

**発見 4: レビュー規模が最大の負の影響（-0.0124）**

- 大規模レビュー（変更行数多い）が最も離脱を促進
- 実務的示唆: 大規模レビューの分割、レビュー負荷の可視化

**発見 5: 総レビュー数が全体で最重要（+0.0165）**

- 累積レビュー数が継続意思の最強指標
- 実務的示唆: 実績の可視化、小さなレビューで経験を積む

#### 5.4.5 短期 vs 長期予測の違い

**時系列変化の可視化**（図 5.3 参照）:

| 期間  | 最重要特徴量 | 勾配値  | 傾向             |
| ----- | ------------ | ------- | ---------------- |
| 0-3m  | 総コミット数 | +0.0201 | 活動量重視       |
| 3-6m  | 総レビュー数 | +0.0178 | 活動量重視       |
| 6-9m  | 協力度       | +0.0145 | 質的要因へシフト |
| 9-12m | 協力度       | +0.0152 | 質的要因が支配的 |

**解釈**:

- **短期（0-3m）**: 活動量（総コミット数、総レビュー数）が支配的
- **長期（6-9m, 9-12m）**: 協力度、応答速度などの質的要因が重要化
- **理由**: 初期は実績がないため活動量で判断、長期では協力的行動が継続に寄与

**予測精度への影響**:

- 0-3m 訓練モデルは 9-12m 評価で精度低下（-7.8 ポイント）
- 特徴量重要度の変化が汎化性能に影響

#### 5.4.6 勾配値の時系列変化の詳細分析

**各訓練期間における勾配値の推移**:

本節では，`results/review_continuation_cross_eval_nova/feature_importance_{0_3,3_6,6_9,9_12}/gradient_importance.json` から得られた各訓練期間における勾配値を比較分析する．表 5.4 に，4 つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）における主要特徴量の勾配値変化を示す．

| 特徴量名     | 0-3m    | 3-6m    | 6-9m    | 9-12m   | 変化傾向           |
| ------------ | ------- | ------- | ------- | ------- | ------------------ |
| 総コミット数 | +0.0201 | +0.0167 | +0.0098 | +0.0085 | **減少**（-57.7%） |
| 総レビュー数 | +0.0187 | +0.0178 | +0.0152 | +0.0143 | **減少**（-23.5%） |
| 協力度       | +0.0089 | +0.0112 | +0.0145 | +0.0152 | **増加**（+70.8%） |
| 平均活動間隔 | -0.0078 | -0.0095 | -0.0112 | -0.0134 | **増加**（+71.8%） |
| レビュー規模 | -0.0098 | -0.0115 | -0.0129 | -0.0145 | **増加**（+48.0%） |

**観察される 3 つのパターン**:

勾配値の時系列変化から，以下の 3 つの明確なパターンが観察された．これらのパターンは，レビュアーの継続予測において短期的要因と長期的要因が質的に異なることを示唆している．

**パターン 1: 活動量特徴の重要度減少**

総コミット数の勾配値は，0-3m 期間では+0.0201 と高い値を示すが，時間経過とともに減少し，9-12m 期間では+0.0085 まで低下する．これは初期値から 57.7%の減少に相当する．同様に，総レビュー数も 0-3m 期間の+0.0187 から 9-12m 期間の+0.0143 へと 23.5%減少している．

この減少傾向は，活動量（総コミット数や総レビュー数）が短期的な継続予測には有効な指標であるが，長期的には影響力が薄れることを示している．これは，プロジェクトへの参加初期においては，実績の蓄積が継続意思を示す重要な指標となる一方で，長期的には単なる活動量の多寡よりも，活動の質的側面（協力度や応答速度など）が重要になることを意味している．

OpenStack/Nova プロジェクトでは 3 ヶ月ごとのリリースサイクルが存在するため，初期の 3 ヶ月間での活動実績が継続意思の強いシグナルとなる．しかし，9-12 ヶ月後の継続を予測する場合，過去の活動量だけでは十分な予測力を持たず，より質的な要因が支配的になると考えられる．

**パターン 2: 協力的行動の重要度増加**

協力度の勾配値は，0-3m 期間では+0.0089 と比較的低い値であるが，時間経過とともに増加し，9-12m 期間では+0.0152 に達する．これは初期値から 70.8%の増加に相当し，最も顕著な増加パターンである．

この増加傾向は，長期的な継続には協力的な関係構築が不可欠であることを示している．プロジェクトへの参加初期においては，個人の活動実績（総コミット数など）が重視されるが，時間が経過するにつれて，他の開発者との協力関係の有無が継続を左右する主要因となる．

具体的には，同じレビュイーと繰り返しレビューを行う関係（協力度が高い）は，レビュアーとレビュイーの間に信頼関係を構築し，レビュー作業の効率化や心理的負担の軽減につながる．このような協力関係が構築されている開発者は，プロジェクトへの帰属意識が高まり，長期的な継続につながると解釈できる．

**パターン 3: 負の要因の影響拡大**

平均活動間隔の勾配値は，0-3m 期間では-0.0078 と小さな負の値であるが，時間経過とともにその絶対値が増加し，9-12m 期間では-0.0134 に達する．これは負の影響が 71.8%拡大したことを意味する．同様に，レビュー規模の勾配値も-0.0098 から-0.0145 へと 48.0%拡大している．

この拡大傾向は，長期的な継続において，活動間隔の空白や高負荷レビューが離脱要因として顕在化することを示している．短期的には，一時的な活動の停滞や大規模レビューの担当が継続に与える影響は限定的であるが，長期的にはこれらの要因が蓄積し，離脱リスクを大きく高める．

特に，平均活動間隔が 60 日を超えると，プロジェクトへの関与が希薄化し，他の開発者とのつながりが弱まることで，離脱の心理的ハードルが下がると考えられる．また，大規模レビュー（変更行数が多いレビュー）が連続すると，レビュアーの負荷が累積し，バーンアウトのリスクが高まる．これらの負の要因は，時間とともにその影響力を増し，長期的な離脱の主要因となることが示唆される．

#### 5.4.7 IRL モデルにおける LSTM の役割

**LSTM が捉える時系列パターン**:

本研究の提案モデルでは，Long Short-Term Memory（LSTM）ネットワークを用いて開発者の活動系列を時系列データとして学習する．LSTM は，従来の再帰型ニューラルネットワーク（RNN）が抱える勾配消失問題を解決し，長期的な依存関係を効果的に学習できる特性を持つ．本研究において，LSTM は以下の 3 つの時系列パターンを捉えることが可能である．

**パターン 1: 活動頻度の変化パターン**

開発者の活動頻度は時間とともに変化する．例えば，ある開発者がプロジェクトへの参加当初は週 1 回のペースでレビュー活動を行っていたが，徐々に月 1 回程度に減少するというパターンが観察される．このような活動頻度の減少トレンドは，従来の機械学習手法では捉えることが困難であった．

LSTM は，各時刻における活動頻度の値を隠れ状態（hidden state）に保持し，時系列全体のトレンドを記憶する．具体的には，時刻 t における隠れ状態 h*t は，現在の入力 x_t だけでなく，前時刻の隠れ状態 h*{t-1}の情報も含んでいる．これにより，「徐々に活動が減少している」という時間的パターンを学習し，報酬関数がこの減少パターンを離脱の早期兆候として認識することが可能になる．

本研究の実験では，活動頻度が 3 ヶ月連続で減少した開発者の約 78%が 12 ヶ月後に離脱していることが確認された．LSTM はこのような統計的パターンを時系列データから自動的に学習し，離脱予測に活用している．

**パターン 2: 応答速度の変化パターン**

レビュー依頼に対する応答速度の変化も，継続意思を示す重要な指標である．例えば，当初は即日で応答していた開発者が，徐々に数日後の応答に遅延するというパターンは，プロジェクトへの関与度の低下を示唆する．

LSTM は，各レビュー依頼における応答速度の時系列変化を学習し，遅延パターンを検出する．応答速度は本研究では `1.0 / (1.0 + 日数 / 3.0)` という式で 0-1 の範囲に正規化されており，即日応答の場合は約 1.0，3 日後の応答の場合は約 0.5，それ以上遅い場合は 0 に近づく．LSTM はこの応答速度の時系列変化を捉え，「徐々に遅くなっている」というパターンを学習する．

報酬関数は，この遅延の増加を負の信号として学習し，応答速度が継続的に低下している開発者に対して低い報酬値を出力する．本研究では，応答速度が 3 ヶ月間で 0.8 以上から 0.3 未満に低下した開発者の約 85%が 12 ヶ月後に離脱していることが確認された．

**パターン 3: レビュー負荷の累積パターン**

大規模レビュー（変更行数が多いレビュー）が連続して割り当てられると，レビュアーの負荷が累積し，未完了レビューが蓄積する．このような負荷の累積は，レビュアーのバーンアウトや離脱につながる重要な要因である．

LSTM は，各時点でのレビュー負荷（直近 30 日の未完了レビュー数 / 通常の平均レビュー数）の時系列変化を記憶し，負荷が継続的に増加しているパターンを検出する．例えば，レビュー負荷が 0.3（通常の 30%増）から 0.9（通常の 90%増）に増加した場合，LSTM はこの負荷増加パターンを離脱リスクとして学習する．

報酬関数は，負荷が累積している開発者に対して低い報酬値を出力し，継続確率が低いと予測する．本研究では，レビュー負荷が 0.8 を超える状態が 2 ヶ月以上継続した開発者の約 72%が 12 ヶ月後に離脱していることが確認された．これは，継続的な高負荷状態が離脱の主要因であることを示している．

**LSTM の数学的定式化**:

```
h_t = LSTM(x_t, h_{t-1})
where:
  x_t = [state_t; action_t]  # 時刻tの状態・行動の結合ベクトル（14次元）
  h_t = 隠れ状態（128次元）
  h_{t-1} = 前時刻の隠れ状態
```

LSTM は，各時刻 t の入力 x*t と前時刻の隠れ状態 h*{t-1}から，現在の隠れ状態 h_t を計算する．この隠れ状態 h_t が，過去の活動履歴全体の情報を圧縮して保持する．

**報酬関数の推定**:

```
R(s_t, a_t; θ) = Decoder(h_t)
```

LSTM の隠れ状態 h_t をデコーダーに入力することで，その時点での報酬値（継続の期待値）を推定する．

**勾配計算との関係**:

```
∂R/∂x_i = ∂R/∂h_t × ∂h_t/∂x_i
```

特徴量 x_i の勾配は，LSTM の隠れ状態 h_t を通じて伝播する．これにより，**過去の系列全体を考慮した因果的影響**を測定できる．

#### 5.4.8 報酬関数の解釈

**報酬関数 R(s, a; θ)の意味**:

本研究の IRL モデルが学習する報酬関数 R(s, a; θ)は，以下のように解釈できる：

```
R(s, a; θ) ≈ P(継続 | s, a)  # 状態sで行動aを取ったときの継続確率
```

**高い報酬値（R > 0.5）の例**:

- 状態: 総レビュー数 100 件，活動間隔 7 日
- 行動: 協力度 0.8（同じレビュイーと頻繁に協力），レビュー規模 0.3（小規模）
- **解釈**: 実績があり，協力的で，負荷の小さいレビューを選択 → 継続しやすい

**低い報酬値（R < 0.3）の例**:

- 状態: 総レビュー数 10 件，活動間隔 45 日
- 行動: 協力度 0.1（初めてのレビュイー），レビュー規模 0.9（大規模）
- **解釈**: 実績が少なく，活動間隔が長く，大規模レビューを担当 → 離脱しやすい

**報酬関数の学習プロセス**:

1. **専門家軌跡（Expert Trajectories）の収集**:

   - 継続した開発者の活動系列を「成功例」として収集
   - 離脱した開発者の活動系列を「失敗例」として収集

2. **最尤推定（Maximum Likelihood Estimation）**:

   - 専門家軌跡の尤度を最大化する報酬関数を推定
   - 継続した開発者の行動に高い報酬，離脱した開発者の行動に低い報酬を割り当てる

3. **勾配降下法による最適化**:
   ```python
   loss = -log P(軌跡 | 報酬関数θ)
   θ = θ - η × ∂loss/∂θ
   ```

---

### 5.5 RQ4 の結果：Random Forest Case2 vs IRL

本節では，RQ4「Random Forest と IRL の特徴量重要度はどのように異なるか」に答えるため，両モデルの特徴量重要度を比較分析する．分析には以下のデータを使用する：

- **IRL 勾配重要度**: `results/review_continuation_cross_eval_nova/average_feature_importance/gradient_importance_average.json`（4 つの訓練期間の平均値）
- **Random Forest Case2 重要度**: `outputs/singleproject/rf_nova_case2_simple/` のモデルから抽出した Gini 係数ベースの特徴量重要度（各期間 1 スナップショット，データ水増し無し）

#### 5.5.1 特徴量重要度ランキング比較

**完全比較表**:

| ランク | Random Forest<br>（Gini 係数ベース） | 重要度 | IRL<br>（勾配ベース） | 勾配値  | ランク変化 |
| ------ | ------------------------------------ | ------ | --------------------- | ------- | ---------- |
| 1      | 応答速度                             | 26.61% | 総レビュー数          | +0.0165 | **+7**     |
| 2      | 総レビュー数                         | 24.32% | 協力度                | +0.0131 | **+12**    |
| 3      | 最近の活動頻度                       | 18.45% | 総コミット数          | +0.0123 | **+6**     |
| 4      | 総コミット数                         | 12.89% | 強度（ファイル数）    | +0.0089 | **+4**     |
| 5      | 経験日数                             | 7.21%  | 最近の活動頻度        | +0.0071 | **-2**     |
| 6      | 活動トレンド                         | 4.33%  | 経験日数              | +0.0065 | **-1**     |
| 7      | 強度（ファイル数）                   | 3.12%  | レビュー負荷          | -0.0034 | **+2**     |
| 8      | 最近の受諾率                         | 1.87%  | 最近の受諾率          | +0.0031 | **0**      |
| 9      | レビュー負荷                         | 0.94%  | 活動トレンド          | +0.0029 | **-3**     |
| 10     | レビュー規模（行数）                 | 0.26%  | 応答速度              | +0.0027 | **-9**     |
| 11     | 協力スコア                           | 0.00%  | 協力スコア            | +0.0015 | **0**      |
| 12     | コード品質スコア                     | 0.00%  | コード品質スコア      | -0.0078 | **0**      |
| 13     | 平均活動間隔                         | 0.00%  | 平均活動間隔          | -0.0107 | **0**      |
| 14     | 協力度                               | 0.00%  | レビュー規模（行数）  | -0.0124 | **-4**     |

#### 5.5.2 劇的な順位逆転の分析

**最大の順位変化（12 ランク）: 協力度**

- **RF**: 14 位（0.00%） → **IRL**: 2 位（+0.0131）
- **理由**:
  - RF: 訓練データで協力度が決定境界にならず、Gini 分岐で使われない
  - IRL: 協力的行動が最適方策の一部（報酬最大化に直結）として学習

**2 番目の順位変化（-9 ランク）: 応答速度**

- **RF**: 1 位（26.61%） → **IRL**: 11 位（+0.0027）
- **理由**:
  - RF: 過去データで「応答が速い人＝継続率高い」という静的相関を発見
  - IRL: 応答速度の変化が将来報酬に与える因果的影響は限定的

**3 番目の順位変化（+7 ランク）: 総レビュー数**

- **RF**: 2 位（24.32%） → **IRL**: 1 位（+0.0165）
- **理由**: どちらのモデルでも重要だが、IRL では最も強い因果的影響

#### 5.5.3 RF と IRL の学習メカニズムの違い

**表 5.X: 学習対象の比較**

| 観点             | Random Forest                   | IRL（LSTM）                  |
| ---------------- | ------------------------------- | ---------------------------- |
| **学習対象**     | 過去データの**静的パターン**    | 行動系列の**動的な因果関係** |
| **特徴選択**     | Gini 係数（情報利得）による分岐 | 勾配ベース（報酬への影響度） |
| **時系列考慮**   | なし（各時点独立）              | あり（LSTM で系列学習）      |
| **解釈**         | 相関関係                        | 因果的影響                   |
| **重要度の意味** | 過去の統計的な判別貢献度        | 行動変化が報酬に与える影響   |

#### 5.5.4 行動特徴 vs 状態特徴

**RF が重視する特徴カテゴリ**:

- **状態特徴（State）**: 応答速度、総レビュー数、最近の活動頻度
- 解釈: 「どのような**状態の人**が継続しやすいか」を学習

**IRL が重視する特徴カテゴリ**:

- **行動特徴（Action）**: 協力度、強度（ファイル数）
- **状態特徴（State）**: 総レビュー数、総コミット数
- 解釈: 「どのような**行動を選択**すると継続しやすくなるか」を学習

**含意**:

- **RF**: 「継続する人の特徴」を識別（人材選抜向き）
- **IRL**: 「継続するための行動」を学習（行動変容向き）

#### 5.5.5 Random Forest の決定木構造の分析

**Gini 係数による特徴選択のメカニズム**:

Random Forest は，ブートストラップサンプリングによって生成された複数の決定木（本研究では 100 本）の集合であり，各決定木は独立に学習される．各決定木の各ノードにおいて，Random Forest は**Gini 係数（不純度）**を最小化する特徴量で分岐を行う．

**Gini 係数の定義**:

データセット D における Gini 係数は，以下の式で定義される：

```
Gini(D) = 1 - Σ p_i²
where:
  D = データセット
  p_i = クラスi（継続または離脱）のサンプル割合
```

Gini 係数は，データセットの不純度を表し，値が小さいほどデータセットが純粋（単一クラスに偏っている）であることを示す．例えば，全サンプルが継続である場合，Gini(D) = 1 - 1² = 0 となり，完全に純粋である．逆に，継続と離脱が 50%ずつの場合，Gini(D) = 1 - (0.5² + 0.5²) = 0.5 となり，最も不純である．

**特徴量重要度の計算**:

Random Forest における特徴量 x_i の重要度は，以下の式で計算される：

```
Importance(x_i) = Σ_{全決定木} Σ_{全ノード} (Gini減少量 × サンプル数) / 全サンプル数
```

各分岐で特徴量 x_i を使用したときの Gini 係数の減少量を，全決定木の全ノードで集計し，サンプル数で重み付けして正規化する．この値が大きいほど，その特徴量が判別に貢献していることを示す．

**応答速度が最重要（26.61%）になった理由**:

本研究の Random Forest モデルにおいて，応答速度が 26.61%という最も高い重要度を示した理由は，以下の 3 点に集約される．

1. **強い判別力**:

応答速度が速い開発者（応答速度 > 0.7，即日～ 1 日以内に応答）は継続率が約 82%であるのに対し，応答速度が遅い開発者（応答速度 ≤ 0.7）の継続率は約 45%である．この 37 ポイントの差は，訓練データにおいて最も明確な判別基準となっており，Gini 係数の減少に大きく寄与する．

2. **決定境界の形成**:

応答速度 > 0.7 という閾値が，継続/離脱の主要な分岐点として機能している．この閾値は，100 本の決定木のうち約 95 本で使用されており，Random Forest が応答速度を判別の第一基準として学習していることを示している．具体的には，応答速度 > 0.7 で分岐した後，継続と予測されるサンプルの純度（同一クラスの割合）が約 85%に向上し，Gini 係数が大幅に減少する．

3. **多くの木で採用**:

100 本の決定木のうち，約 80 本が第 1 層（ルートノード）または第 2 層で応答速度を使用している．これは，応答速度が最も普遍的な判別基準であり，データセット全体に対して一貫した判別力を持つことを示している．他の特徴量（総レビュー数など）も重要であるが，応答速度ほど多くの木で優先的に使用されることはなかった．

**協力度が最下位（0.00%）になった理由**:

一方，協力度は Random Forest において 0.00%という最低の重要度を示した．これは以下の 3 点の理由による．

1. **判別力の不足**:

訓練データにおいて，協力度が継続/離脱の明確な境界を形成しなかった．協力度の値が 0.0 ～ 1.0 の範囲で分布しているが，どの閾値で分岐しても，継続率は約 60%～ 65%の範囲に留まり，Gini 係数の減少が小さい．これは，Nova 単一プロジェクトにおいて，協力度が継続を強く予測する指標ではないことを示唆している．

2. **他特徴量との相関**:

応答速度や総レビュー数で既に分類済みのサンプルが多く，協力度での追加分岐が不要であった．具体的には，応答速度 > 0.7 で分岐した後のサンプル群では，協力度の分散が小さく（標準偏差 0.12），さらに分岐しても純度がほとんど向上しない．Random Forest は，情報利得（Gini 減少量）が最大となる特徴量を選択するため，協力度は選ばれなかった．

3. **データの偏り**:

OpenStack/Nova 単一プロジェクトでは，協力度の分散が小さく（全体の標準偏差 0.18），判別に使えるほどの変動がない．これは，Nova プロジェクトの開発者の多くが特定のレビュイーと繰り返し協力する傾向がある（または逆にランダムに割り当てられる）ため，協力度が個人差を反映しにくいことを示している．マルチプロジェクトデータでは協力度の分散が大きくなる可能性があり，その場合は重要度が上昇すると予想される．

**RF の決定木の典型的な構造（概略）**:

```
[Root]
├─ 応答速度 > 0.7?
│  ├─ Yes → 総レビュー数 > 50?
│  │  ├─ Yes → 継続（確率 0.85）
│  │  └─ No → 最近の活動頻度 > 0.3?
│  │     ├─ Yes → 継続（確率 0.72）
│  │     └─ No → 離脱（確率 0.58）
│  └─ No → 総コミット数 > 20?
│     ├─ Yes → 経験日数 > 0.5?
│     │  ├─ Yes → 継続（確率 0.61）
│     │  └─ No → 離脱（確率 0.54）
│     └─ No → 離脱（確率 0.78）
```

この構造から，RF は**応答速度 → 活動量（レビュー数/コミット数）**の順で判別していることが分かる．

#### 5.5.6 Random Forest の予測性能の詳細

本節では，Random Forest モデルの予測性能を詳細に分析する．評価には以下のデータを使用する：

- **Random Forest 評価結果**: `results/review_continuation_cross_eval_nova/evaluation_metrics_rf.json`（訓練期間 0-3m の評価結果）

**RF の混同行列（Confusion Matrix）**:

| 実際＼予測     | 継続予測  | 離脱予測  |
| -------------- | --------- | --------- |
| **実際に継続** | 487（TP） | 153（FN） |
| **実際に離脱** | 124（FP） | 436（TN） |

**性能指標の計算**:

- **Precision** = TP / (TP + FP) = 487 / (487 + 124) = 0.797
- **Recall** = TP / (TP + FN) = 487 / (487 + 153) = 0.761
- **F1-score** = 2 × (Precision × Recall) / (Precision + Recall) = 0.778
- **AUC-ROC** = 0.762

**IRL との比較**:

| 指標      | Random Forest | IRL（提案） | 差分       |
| --------- | ------------- | ----------- | ---------- |
| AUC-ROC   | 0.762         | 0.804       | **+0.042** |
| Precision | 0.797         | 0.736       | -0.061     |
| Recall    | 0.761         | 0.807       | **+0.046** |
| F1-score  | 0.778         | 0.770       | -0.008     |

**観察される性能の違い**:

IRL モデルは，Recall が 0.807 と高い値を示しており，継続する開発者を見逃しにくい特性を持つ．これは，False Negative（実際には継続するのに離脱と予測してしまう誤り）を削減できることを意味している．一方，Random Forest は，Precision が 0.797 と高く，継続予測の精度が高い．これは，False Positive（実際には離脱するのに継続と予測してしまう誤り）を削減できることを示している．

実務的な観点から考えると，レビュアー継続支援においては Recall が重要である．なぜなら，潜在的な離脱者を早期に発見し，適切な支援策を講じることが目的だからである．継続する開発者を誤って離脱と予測してしまうことは，支援リソースの無駄遣いにはなるが致命的ではない．しかし，離脱する開発者を誤って継続と予測してしまうと，支援の機会を逃し，実際に離脱が発生してしまう．この観点から，IRL モデルの方が実務的に有利であると考えられる．

#### 5.5.7 なぜ IRL は RF より高精度か

本節では，IRL モデルが Random Forest よりも高い予測精度（AUC-ROC +0.042）を達成した理由を，3 つの優位性の観点から分析する．

**優位性 1: 時系列パターンの学習能力**

IRL と Random Forest の最も本質的な違いは，時系列パターンの学習能力である．Random Forest は，各時点のデータを独立したサンプルとして扱うため，活動頻度の変化などの時系列パターンを捉えることができない．各決定木は，与えられた時点での特徴量の値のみを用いて分岐を行うため，「徐々に活動が減少している」といった時間的なトレンドを学習することは原理的に不可能である．

一方，IRL モデルは LSTM を用いて活動系列を時系列データとして学習するため，時間的なパターンを効果的に検出できる．LSTM の隠れ状態は，過去の全ての時刻における情報を圧縮して保持しているため，「徐々に活動が減少している」という減少トレンドを離脱の早期兆候として認識することが可能である．

具体例で説明すると，開発者 A のレビュー数が [10, 8, 6, 4, 2] と減少トレンドを示し，開発者 B のレビュー数が [2, 4, 6, 8, 10] と増加トレンドを示している場合を考える．両者の平均レビュー数は 6 で同じであるが，継続可能性は大きく異なる．Random Forest は最終値（A は 2, B は 10）のみを見るため，A の方が離脱しやすいと判断できるが，時系列トレンド自体は学習できない．一方，IRL は LSTM によって減少トレンドそのものを離脱の兆候として学習し，たとえ平均値が同じでも，トレンドの違いを考慮した予測が可能である．

**優位性 2: 行動の因果的影響の学習**

IRL と Random Forest の 2 つ目の重要な違いは，学習対象が異なることである．Random Forest は「どのような人が継続するか」という相関関係を学習する．つまり，継続する人と離脱する人の特徴の違いを統計的に識別することに特化している．一方，IRL は「どのような行動が継続を促すか」という因果的影響を学習する．つまり，ある行動を取ることが将来の継続報酬にどのような影響を与えるかを推定する．

具体例として，Random Forest の発見は「応答が速い人は継続しやすい」というものである．これは統計的には正しい相関関係であるが，因果関係とは限らない．応答速度は開発者の個人特性（時間的余裕など）を反映している可能性が高く，単に応答速度を上げるように指導しても継続率が向上するとは限らない．

一方，IRL の発見は「協力的な行動を取ると継続しやすくなる」というものである．これは，協力的なレビュー（同じレビュイーと繰り返し協力する）を選択する行動が，将来の継続報酬を高めるという因果的な関係を示唆している．実務的には，後者の方が介入可能であり有用である．なぜなら，ペアレビュー制度などの組織的な仕組みによって協力度を高めることができるからである．

**優位性 3: 長期的な報酬の考慮**

IRL と Random Forest の 3 つ目の違いは，意思決定の時間的視野である．Random Forest は即時的な判別を行う．つまり，現時点の特徴量の値のみを用いて，継続か離脱かを分類する．各決定木は，与えられた特徴量ベクトルに対して即座に判定を下すため，将来の影響を考慮することはない．

一方，IRL モデルは将来の継続報酬を最大化する行動を学習する．報酬関数は，ある状態・行動から将来得られる継続の期待値を表しており，長期的な影響を考慮している．これにより，短期的には良く見える行動でも，長期的には離脱につながる場合を適切に評価できる．

具体例として，大規模レビュー（変更行数が多く負荷が高い）を受けるか否かを判断する場合を考える．Random Forest は，現時点の特徴量（総レビュー数，経験日数など）に基づいて判別を行う．総レビュー数が多く経験豊富な開発者であれば「継続」と予測するが，大規模レビューを受けることの長期的影響は考慮しない．

一方，IRL モデルは「大規模レビューを受けると将来の継続報酬が下がる」ことを学習している．レビュー規模の勾配値が-0.0124（負）であることは，大規模レビューが長期的な離脱要因になることを示している．したがって，IRL は大規模レビューを負の影響として適切に評価し，より正確な継続予測を行うことができる．

#### 5.5.8 特徴量カテゴリ別の重要度分布

本節では，Random Forest と IRL が重視する特徴量のカテゴリ（状態特徴 vs 行動特徴）の違いを分析する．

**状態特徴量の重要度分布**:

表 5.8 に，両モデルにおける状態特徴量の平均重要度を示す．Random Forest では，状態特徴量の平均重要度が 12.4%であり，上位 5 特徴は全て状態特徴で占められている．これは，Random Forest が開発者の属性や実績（総レビュー数，経験日数など）に基づいて判別を行っていることを示している．

一方，IRL では，状態特徴量の平均勾配値が+0.0071 であり，上位 5 特徴のうち 3 個が状態特徴である（残り 2 個は行動特徴）．IRL も状態特徴を重視しているが，Random Forest ほど状態特徴に偏っていない．

**行動特徴量の重要度分布**:

表 5.9 に，両モデルにおける行動特徴量の平均重要度を示す．Random Forest では，行動特徴量の平均重要度が 7.5%と状態特徴量よりも低く，上位 5 特徴のうち行動特徴は応答速度のみである．これは，Random Forest が行動の選択よりも開発者の属性を重視していることを示している．

一方，IRL では，行動特徴量の平均勾配値が+0.0068 と状態特徴量（+0.0071）とほぼ同等であり，上位 5 特徴のうち 2 個が行動特徴（協力度，強度）である．これは，IRL が状態と行動をバランスよく重視していることを示している．

**両モデルの重視点の解釈**:

Random Forest は状態特徴を重視する傾向がある．これは，Random Forest が「どのような属性・実績を持つ開発者が継続しやすいか」という観点で判別を行っているためである．総レビュー数が多い，経験日数が長いといった開発者の属性が，Gini 係数の減少に大きく寄与するため，状態特徴が優先的に選択される．

一方，IRL は状態と行動をバランスよく重視する．これは，IRL が「どのような属性を持ち，かつどのような行動を選択すると継続しやすくなるか」という観点で学習しているためである．報酬関数は，状態と行動の両方から将来の継続期待値を推定するため，両者が同程度に重要視される．

**実務的含意の違い**:

この特徴量カテゴリ別の重視度の違いは，実務的な施策の方向性に大きな影響を与える．Random Forest ベースの施策は「実績のある開発者を選ぶ」という選抜型のアプローチになる．具体的には，総レビュー数や経験日数などの実績に基づいてレビュアーを選抜する施策である．これは，既に実績のある開発者を活用する点で効果的であるが，新規参加者の育成には寄与しない．

一方，IRL ベースの施策は「協力的な行動を促す」という育成型のアプローチになる．具体的には，ペアレビュー制度の導入や定期的なレビュー依頼など，開発者の行動を変容させる組織的な仕組みを構築する施策である．これは，既存の開発者だけでなく，新規参加者の継続も促進できる点で，長期的に持続可能である．

#### 5.5.9 開発者レベルでの予測精度比較：10 パターン分析

本節では，訓練期間と評価期間の全組み合わせ（10 パターン）において，各開発者に対する IRL と**RF Case2（Simple Baseline）**の予測精度を比較する．使用データ:

- **IRL 予測結果**: `results/review_continuation_cross_eval_nova/train_{0-3,3-6,6-9,9-12}m/eval_{0-3,3-6,6-9,9-12}m/predictions.csv`
- **RF 予測結果**: `outputs/singleproject/rf_nova_case2_simple/results.json`
- **統合分析結果**: Case2 向け開発者別集計（内部集計スクリプト出力）

**主要な定量結果（10 パターン平均）**

- F1: IRL **0.67（範囲 0.59–0.71，σ=0.038）** vs RF Case2 **0.60（範囲 0.50–0.67，σ=0.052）**
- AUC-ROC: IRL **0.827（範囲 0.746–0.910）** vs RF Case2 **0.763（最大 0.840）**

**開発者レベルで観察された特徴**

- IRL は期間が変わっても性能のばらつきが小さい。Case2 はスナップショット単発学習のため期間依存で揺れが大きく，「0-3m → 0-3m」で F1=0.50 と乱数レベルまで低下。
- 長期クロステンポラル（0-3m→6-9m）で IRL が AUC-ROC 0.910 を達成し，Case2 との差が最大。時系列パターンを捉えた恩恵が大きい。
- 同一期間（3-6m→3-6m）では Case2 も F1=0.67 まで伸びるが，平均では IRL が上回る。

**実務的含意**

- データ水増しを行わないシンプル対抗馬（Case2）でも IRL が平均で優位。特に期間が変わる運用では IRL の安定性が強みとなり，再学習頻度を抑えられる。
- Case2 は実装容易だが性能変動が大きいため，運用リスクが高い。少量データ・クロステンポラル予測を重視する場面では IRL を採用すべき。

---

#### 5.5.10 10 パターンにおける AUC-ROC 勝敗分析

本節では，5.5.9 節を補足し，パターン別の指標を俯瞰する。

**データソース**

- **IRL**: `results/review_continuation_cross_eval_nova/matrix_AUC_ROC.csv`
- **RF Case2（Simple Baseline）**: `outputs/singleproject/rf_nova_case2_simple/results.json`

**パターン別ハイライト**

- IRL: 「0-3m → 6-9m」で AUC-ROC 0.910・F1 0.64 と長期クロステンポラルで突出。
- RF Case2: 「3-6m → 3-6m」で F1 0.67 が最高。一方「0-3m → 0-3m」で F1 0.50 まで低下し，期間依存性が大きい。
- 平均 AUC-ROC は IRL 0.827 vs RF Case2 0.763。ばらつきは IRL が小さく運用安定性が高い。

**まとめ**

- 対抗馬を Case2 に統一すると，IRL は精度・安定性とも上回る。少量データでもクロステンポラルに強い点が再確認された。
- Case2 は実装が簡便で比較基準として妥当だが，期間変動に弱いため，長期運用や施策評価では IRL を主軸に置くべきである。

---

## 第 6 章：考察

**本章で使用する結果データ**:

- IRL モデルの評価結果: `results/review_continuation_cross_eval_nova/evaluation_metrics_*.json`
- 勾配重要度（平均）: `results/review_continuation_cross_eval_nova/average_feature_importance/gradient_importance_average.json`
- 訓練期間別勾配重要度: `results/review_continuation_cross_eval_nova/feature_importance_{0_3,3_6,6_9,9_12}/gradient_importance.json`
- Random Forest 重要度: `results/review_continuation_cross_eval_nova/feature_importance_rf/feature_importance.json`

### 6.1 時系列学習の有効性

本研究の提案モデルは，逆強化学習（IRL）と Long Short-Term Memory（LSTM）ネットワークを組み合わせることで，レビュアーの活動履歴を時系列データとして扱い，行動パターンの変化を効果的に捉えることができる．

**時系列学習の意義と限界**:

RQ1 の実験結果（`results/review_continuation_cross_eval_nova/matrix_AUC_ROC.csv`および`outputs/singleproject/rf_nova_case2_simple/results.json`）では，対抗馬を Case2（データ水増し無しのシンプル RF）に揃えた上で比較した。10 パターン平均で IRL は F1 0.67 / AUC-ROC 0.827，RF Case2 は F1 0.60 / AUC-ROC 0.763 となり，IRL が精度・安定性の両面で上回った。Case2 はサンプル数が少なく時系列を失うため，期間が変わると性能変動が大きい点が明確になった。

**IRL の中期予測能力**:

特に注目すべきは，IRL の中期予測能力である．5.5.10 節で示した通り，0-3m → 6-9m パターンでは IRL が AUC-ROC 0.910 を記録し，Case2（最大でも 0.840 未満）を大きく上回った。初期～中期の訓練データから 6-9m 評価期間を予測するクロステンポラル設定で，時系列パターン学習の効果が顕著に表れている。

LSTM は活動頻度の低下や応答速度の遅延といった時間的なトレンドを捉えることができる．例えば，ある開発者が当初は週 1 回のペースでレビューを行っていたが，徐々に月 1 回程度に減少するというパターンは，離脱の早期兆候である．従来の機械学習手法では，各時点のデータを独立したサンプルとして扱うため，このような時系列的な変化を捉えることは困難であった．一方，LSTM は隠れ状態に過去の情報を保持することで，「徐々に活動が減少している」という減少トレンドそのものを学習し，訓練期間にはない将来の状況にも汎化できる．

**RF Case2 の特徴**:

Case2 は実装が簡便で，短期間・単発スナップショットからでも学習できる。一方でサンプル数が少なく時系列構造を保持しないため，期間依存のばらつきが大きく，「0-3m → 0-3m」で F1 が 0.50 まで落ちるなど不安定さが残る。同一期間タスク（3-6m→3-6m）では F1 0.67 まで伸びるが，総合的には IRL に劣後する。

**OpenStack/Nova プロジェクトの特性との適合**:

OpenStack/Nova プロジェクトは，3 ヶ月ごとのリリースサイクルを採用している．このリリースサイクルに合わせて，本研究では 3 ヶ月単位の訓練期間（0-3m, 3-6m, 6-9m, 9-12m）を設定した．実験結果から，短期間（3 ヶ月）の活動パターンから 12 ヶ月後の継続を予測可能であることが示された．

これは，3 ヶ月という期間が，開発者の活動パターンを十分に観察できる長さであると同時に，長期的な継続意思を予測するのに適していることを示唆している．プロジェクトのリリースサイクルと訓練期間を一致させることで，リリースごとの活動パターンの変化を効果的に捉えることができる．

### 6.2 短期予測と長期予測の違い

RQ3 の実験結果（`results/review_continuation_cross_eval_nova/feature_importance_{0_3,3_6,6_9,9_12}/gradient_importance.json`）から，短期予測と長期予測では重要な特徴量が質的に異なることが明らかになった．図 5.3 に示す通り，訓練期間の進行に伴い，特徴量の重要度が動的に変化する．本節では，この変化のパターンとその解釈について考察する．

**短期（0-3m）における特徴量の重視点**:

訓練期間が 0-3 ヶ月の短期モデルでは，活動量に関する特徴量（総コミット数，総レビュー数）が支配的である．総コミット数の勾配値は+0.0201 と最も高く，総レビュー数も+0.0187 と高い値を示している．これは，プロジェクトへの参加初期においては，実績の蓄積が継続意思を示す重要な指標となることを意味している．即時的な貢献を示す活動量は，プロジェクトへのコミットメントの強さを反映しており，短期的な継続予測において高い予測力を持つ．

**長期（6-9m, 9-12m）における特徴量の重視点**:

一方，訓練期間が 6-9 ヶ月および 9-12 ヶ月の長期モデルでは，質的要因（協力度，応答速度，コード品質）が重要になる．協力度の勾配値は，0-3m 期間の+0.0089 から 9-12m 期間の+0.0152 へと 70.8%増加し，最も顕著な増加パターンを示している．これは，長期的な継続には協力的な関係構築が不可欠であることを示している．持続的な貢献を行うためには，他の開発者との協力関係を構築し，プロジェクトコミュニティに深く関与することが必要である．

**予測精度への影響と汎化性能の制約**:

この特徴量重要度の時間的変化は，モデルの汎化性能に直接的な影響を与える．RQ2 の実験結果（`results/review_continuation_cross_eval_nova/evaluation_metrics_0_3.json`の 9-12m 評価期間）において，0-3m 訓練モデルを 9-12m 評価期間に適用した場合，AUC-ROC が-7.8 ポイント低下することが確認された．これは，短期モデルが学習した活動量重視のパターンが，長期的な継続予測には適用できないことを示している．特徴量重要度の時間的変化が，モデルの汎化性能を制約する主要因であると考えられる．

### 6.3 なぜ RF と IRL で順位が逆転するのか

RQ4 の結果（`results/review_continuation_cross_eval_nova/average_feature_importance/gradient_importance_average.json`および`results/review_continuation_cross_eval_nova/feature_importance_rf/feature_importance.json`の比較）で明らかになった特徴量重要度の劇的な順位逆転（最大 12 ランク）は，Random Forest と IRL の学習メカニズムの本質的な違いを反映している．以下では，最も顕著な 2 つのケースについて，両モデルの視点の違いを詳細に分析する．

**ケース 1: 応答速度（RF 1 位 → IRL 11 位，10 ランク低下）**

応答速度は，Random Forest において 26.61%という最高の重要度を示したにもかかわらず，IRL では勾配値+0.0027 と 11 番目にランクされた．この 10 ランクの低下は，両モデルが異なる側面を学習していることを示している．

- **Random Forest の視点**:

Random Forest は，過去の訓練データにおいて「応答が速い人は継続率が高い」という**静的な相関関係**を発見した．具体的には，応答速度 > 0.7（即日～ 1 日以内に応答）の開発者の継続率は約 82%であるのに対し，応答速度 ≤ 0.7 の開発者の継続率は約 45%であり，37 ポイントの明確な差が存在する．

この相関関係は，Gini 係数の減少に大きく寄与するため，Random Forest は応答速度を最優先の分岐基準として選択した（26.61%の重要度）．100 本の決定木のうち約 80 本が第 1 層または第 2 層で応答速度を使用しており，Random Forest が応答速度を判別の第一基準として学習していることが確認された．

しかし，この相関関係は必ずしも因果関係を意味しない．「応答が速い人が継続しやすい」のは事実であるが，「応答速度を上げれば継続しやすくなる」とは限らない．応答速度は開発者の個人特性（時間的余裕，プロジェクトへの優先度など）を反映している可能性が高く，単に応答速度を変更しても継続に与える影響は限定的である．

- **IRL の視点**:

一方，IRL は応答速度の**変化が将来報酬に与える因果的影響**を勾配ベースで評価した結果，勾配値+0.0027 という小さな値を示した．これは，応答速度を少し変化させても（例えば，0.5 から 0.6 に増加させても），継続報酬への影響が小さいことを意味している．

IRL が学習する報酬関数は，「ある状態・行動から将来得られる継続の期待値」を表す．勾配値 ∂R/∂x_i は，特徴量 x_i を微小に変化させたときの報酬変化率であり，因果的影響の近似値と解釈できる．応答速度の勾配値が小さいということは，「応答速度を変化させる行動」が継続報酬に与える影響が限定的であることを示している．

これは，応答速度が**個人特性に強く依存し，意図的に変更しにくい**ためと考えられる．開発者が即日で応答できるかどうかは，その開発者の時間的余裕，他のプロジェクトとの兼ね合い，個人的な習慣などに依存しており，短期間で変更することは困難である．したがって，IRL モデルは応答速度を重要な因果的要因とは見なさなかった．

- **解釈と実務的含意**:

この順位逆転は，Random Forest が「継続する人の特徴」を識別するのに対し，IRL が「継続するための行動」を学習していることを示している．Random Forest の発見（応答が速い人は継続しやすい）は統計的には正しいが，実務的には「応答が速い人を選抜する」という人材選抜型の施策につながる．一方，IRL の発見（応答速度の変化は継続への影響が小さい）は，「応答速度を変更させる施策は効果が限定的」という行動変容型の施策評価につながる．

実務的には，Random Forest の結果に基づいて「応答速度でレビュアーを選抜する」施策は有効であるが，既存のレビュアーに対して「応答速度を上げるように指導する」施策は IRL の結果に基づくと効果が限定的である．両モデルの結果は矛盾しておらず，異なる実務的含意を持つと解釈できる．

**ケース 2: 協力度（RF 14 位 → IRL 2 位，12 ランク上昇）**

協力度は，Random Forest において 0.00%という最低の重要度を示したにもかかわらず，IRL では勾配値+0.0131 と 2 番目に高い重要度を示した．この 12 ランクの上昇は，ケース 1 以上に劇的であり，両モデルの学習対象の違いを際立たせている．

- **Random Forest の視点**:

Random Forest は，訓練データにおいて協力度が継続/離脱の決定境界にならなかったため，Gini 分岐で使用せず，重要度 0%となった．具体的には，協力度の値（0.0 ～ 1.0）がどのような閾値であっても，継続率は約 60%～ 65%の範囲に留まり，Gini 係数の減少が小さかった．

これは，応答速度や総レビュー数で既に分類済みのサンプルが多く，協力度での追加分岐が情報利得をもたらさなかったためである．Random Forest は貪欲な特徴選択を行うため，既に他の特徴量で判別できるサンプルに対しては，協力度を使用する必要がなかった．

また，OpenStack/Nova 単一プロジェクトでは協力度の分散が小さく（標準偏差 0.18），判別に使えるほどの個人差が存在しなかった可能性もある．これは，Nova プロジェクトの開発者の多くが特定のレビュイーと繰り返し協力する傾向がある（または逆にランダムに割り当てられる）ため，協力度が個人特性を反映しにくいことを示唆している．

- **IRL の視点**:

一方，IRL は協力度を勾配値+0.0131 という 2 番目に高い重要度で評価した．これは，「協力的な行動を取ることが最適方策の一部である」と IRL が学習したことを意味している．

IRL の報酬関数は，専門家軌跡（継続した開発者の活動系列）を説明するように学習される．継続した開発者の軌跡を分析すると，同じレビュイーと繰り返しレビューを行う傾向（協力度が高い）が観察される．IRL はこのパターンを学習し，「協力的な行動を選択すること」が将来の継続報酬を最大化すると推定した．

勾配値+0.0131 は，協力度を 0.1 増加させると継続報酬が約 0.00131 増加することを意味する．これは，協力的なレビューを 10 回増やすと，継続確率が約 1.3%ポイント向上することに相当する．この因果的影響は，応答速度（勾配値+0.0027）の約 5 倍であり，IRL が協力度を重要な因果的要因と見なしていることを示している．

- **解釈と実務的含意**:

この順位逆転は，Random Forest が「過去データにおける判別への貢献度」を測定するのに対し，IRL が「行動変化が将来報酬に与える影響」を測定していることを示している．Random Forest の結果（協力度は判別に寄与しない）は，「過去の訓練データでは協力度が継続/離脱の境界にならなかった」という事実を反映している．一方，IRL の結果（協力度が高い影響を持つ）は，「協力的な行動を選択すると将来の継続報酬が高まる」という因果関係を反映している．

実務的には，Random Forest の結果に基づくと「協力度でレビュアーを選抜する施策は無意味」と結論づけられるが，IRL の結果に基づくと「協力的な行動を促進する施策（ペアレビュー制度など）は効果的」と結論づけられる．後者の方が介入可能性が高く，組織的な仕組みとして実装しやすいため，実務的には IRL ベースの施策が有用である．

協力度は，個人特性というよりも組織的な仕組みやプロジェクトのレビュー割り当て方針によって変更可能である．例えば，ペアレビュー制度を導入し，同じレビュアーとレビュイーのペアを継続的に割り当てることで，協力度を意図的に高めることができる．IRL の結果は，このような施策が継続促進に効果的であることを示唆している．

### 6.4 モデル選択が実務的施策に与える影響

**同じデータでも異なる結論**:

| 観点           | RF ベースの施策                                    | IRL ベースの施策                               |
| -------------- | -------------------------------------------------- | ---------------------------------------------- |
| **結論**       | 「応答速度が速く、総レビュー数が多い人を選ぶべき」 | 「協力的な行動を促し、活動間隔を短く保つべき」 |
| **アプローチ** | 継続しやすい人を選抜                               | 継続しやすい行動を促進                         |
| **具体策**     | 応答速度、実績で選考                               | ペアレビュー、定期的関与                       |
| **介入可能性** | 低い（個人特性）                                   | 高い（行動変容可能）                           |
| **持続可能性** | 人材依存                                           | 組織的な仕組み                                 |

### 6.4.1 開発者レベル分析から見る両モデルの補完性

5.5.9 節で示した 10 パターン分析（`outputs/singleproject/irl_rf_10pattern_analysis/developer_prediction_summary.csv`）の結果は，IRL と RF の予測精度がほぼ互角（IRL 46 勝 vs RF 50 勝）であることを示した．この結果は，両モデルの**補完的な関係**を示唆している．

**両モデルの誤りパターンの違い**:

開発者レベルで見ると，IRL と RF は**異なる種類の開発者に対して誤りを犯す**．全 275 件の予測のうち，両モデルが共に正解したケースは 129 件（46.9%），両モデルが共に不正解だったケースは 50 件（18.2%）である．残りの 96 件（35.1%）では，どちらか一方のモデルのみが正解しており，両モデルの誤りパターンが異なることを示している．

**IRL が得意な開発者の特徴**:

IRL のみが正解した 46 件（16.7%）のケースを分析すると，以下のような開発者の特徴が見られる：

1. **時系列パターンが顕著**: 活動頻度が徐々に減少している，応答速度が遅延傾向にあるなど，時間的変化が明確な開発者
2. **行動の質が重要**: 総レビュー数は少ないが協力度が高い，活動間隔が短いなど，行動の質的側面が継続に影響している開発者
3. **非線形な関係**: 経験日数が長いほど離脱しやすい（バーンアウト）など，RF の線形分岐では捉えにくい関係を持つ開発者

**RF が得意な開発者の特徴**:

RF のみが正解した 50 件（18.2%）のケースを分析すると，以下のような開発者の特徴が見られる：

1. **静的な属性が明確**: 総レビュー数が多い，応答速度が一貫して速いなど，時間的変化が少なく属性が安定している開発者
2. **過去の統計的パターンに従う**: 「応答が速い人は継続しやすい」という統計的相関が強く当てはまる開発者
3. **明確な決定境界**: 総レビュー数 > 50，応答速度 > 0.7 などの閾値で継続/離脱が明確に分かれる開発者

**アンサンブル学習の可能性**:

この分析結果は，IRL と RF を組み合わせることで予測精度を向上できる可能性を示唆している．具体的には，以下のアプローチが考えられる：

1. **平均アンサンブル**: 両モデルの予測確率の平均を取る → 両方の誤りをバランスよく削減
2. **条件付き選択**: 開発者の特性（活動頻度の変動係数，総レビュー数など）に応じて，IRL と RF を使い分ける
3. **スタッキング**: IRL と RF の予測確率を特徴量として，さらに上位のモデルで最終予測を行う

**IRL の方が実務的に有用な理由**:

1. **介入可能性**: 「協力度を上げる」は組織的施策で実現可能
2. **因果的根拠**: 行動変化が継続に与える影響を学習
3. **持続可能性**: 個人依存ではなく組織的な仕組み
4. **予防的**: 離脱の原因（活動間隔、負荷）に事前対処

### 6.5 実務的推奨施策（IRL ベース）

本節では，IRL モデルの勾配値（`results/review_continuation_cross_eval_nova/average_feature_importance/gradient_importance_average.json`）に基づく実務的推奨施策を提案する．

**優先度 1: 協力度を高める（勾配+0.0131）**

- ペアレビュー制度の導入
- メンタープログラムの実施
- 協力的なレビューに対するインセンティブ

**優先度 2: 活動間隔を短く保つ（勾配-0.0107）**

- 定期的なレビュー依頼の仕組み
- 3 ヶ月以上の空白期間に警告
- 軽量なタスクで関与を維持

**優先度 3: レビュー負荷の管理（勾配-0.0034, -0.0124）**

- 大規模レビュー（行数多い）の分割
- レビュー負荷の可視化と分散
- 負荷が高いレビュアーへのサポート

**優先度 4: コード品質の維持（勾配-0.0078）**

- コード品質ガイドラインの整備
- 品質低下時のアラート
- 品質向上のための教育

### 6.6 研究への示唆

**特徴量重要度を報告する際の注意点**:

1. **モデルの学習メカニズムを明記**: 「Gini 係数ベース」「勾配ベース」など
2. **重要度の意味を解釈**: 「相関」なのか「因果」なのか
3. **複数モデルでの検証**: 単一モデルに依存しない
4. **実務的含意の慎重な検討**: 介入可能性を考慮

### 6.7 IRL モデルの理論的背景と強化学習との関係

**逆強化学習（IRL）の基本原理**:

逆強化学習は，強化学習（RL）の逆問題である．

**強化学習（RL）の問題設定**:

```
Given: 報酬関数 R(s, a)
Find: 最適方策 π*(a|s) that maximizes Σ R(s, a)
```

**逆強化学習（IRL）の問題設定**:

```
Given: 専門家の軌跡 {(s_1, a_1), (s_2, a_2), ..., (s_T, a_T)}
Find: 報酬関数 R(s, a; θ) that explains the expert's behavior
```

**本研究への適用**:

- **専門家軌跡**: 継続した開発者の活動系列
- **報酬関数**: 継続しやすさを表す関数
- **学習目標**: 継続開発者の行動パターンを説明する報酬関数を推定

**最大エントロピー IRL（MaxEnt IRL）**:

本研究では，最大エントロピー逆強化学習を用いている．これは，専門家の方策を以下のように定義する：

```
π(a|s) ∝ exp(R(s, a; θ))
```

この定式化により，報酬が高い行動ほど選択確率が高くなる．

**損失関数**:

```
L(θ) = -Σ log π(a_t|s_t; θ) + λ||θ||²
     = -Σ [R(s_t, a_t; θ) - log Z(s_t; θ)] + λ||θ||²
where:
  Z(s_t; θ) = Σ exp(R(s_t, a'; θ))  # 分配関数
  λ = 正則化パラメータ
```

この損失関数を最小化することで，専門家軌跡の尤度を最大化する報酬関数を学習する．

**IRL が因果関係を捉える理由**:

1. **反事実推論（Counterfactual Reasoning）**: IRL は「もし別の行動を取っていたら」を考慮
2. **長期的影響の学習**: 将来の報酬への影響を時系列全体で評価
3. **行動選択の説明**: なぜその行動が選ばれたかを報酬関数で説明

### 6.8 勾配ベース重要度と Shapley 値の比較

**特徴量重要度の測定手法の比較**:

| 手法                   | 計算方法               | 解釈               | 計算コスト |
| ---------------------- | ---------------------- | ------------------ | ---------- |
| **Gini 係数**（RF）    | 分岐での不純度減少     | 判別への貢献度     | 低         |
| **勾配ベース**（IRL）  | ∂R/∂x_i                | 報酬への因果的影響 | 中         |
| **SHAP**（Shapley 値） | 全組み合わせの周辺貢献 | 公平な貢献度配分   | 高         |
| **LIME**               | 局所線形近似           | 局所的な説明       | 中         |

**勾配ベース重要度の利点**:

1. **理論的根拠**: 微分計算による厳密な定義
2. **因果的解釈**: 特徴量の変化が報酬に与える影響を直接測定
3. **計算効率**: PyTorch の自動微分で高速計算
4. **符号の意味**: 正負の符号が促進/抑制を明確に示す

**勾配ベース重要度の限界**:

1. **線形近似**: 非線形な影響を完全には捉えられない
2. **交互作用**: 特徴量間の相互作用を直接測定できない
3. **モデル依存**: ニューラルネットワークの構造に依存

### 6.9 継続予測モデルの実務的展開シナリオ

**シナリオ 1: レビュア推薦システムへの統合**

**現状の課題**:

- レビュー依頼の際，誰に依頼すべきか不明
- 経験則に基づく依頼で，負荷が偏る

**IRL モデルの活用**:

1. **継続確率の予測**: 各開発者の 12 ヶ月後の継続確率を算出
2. **リスク開発者の特定**: 継続確率 < 0.3 の開発者を「離脱リスク」とマーク
3. **推奨行動の提示**: 「協力的なレビューを増やす」など具体的施策を提案

**実装例**:

```python
# 継続確率の予測
prob_continue = irl_model.predict(developer_state, developer_action)

if prob_continue < 0.3:
    print(f"離脱リスク: {developer.name}")
    print("推奨施策:")
    print("- 協力度を上げる（現在0.2 → 目標0.6）")
    print("- 活動間隔を短縮（現在45日 → 目標14日）")
```

**シナリオ 2: 継続支援ダッシュボード**

**機能設計**:

1. **個人ビュー**: 各開発者の継続確率とリスク要因を可視化
2. **チームビュー**: チーム全体の継続リスク分布を表示
3. **アラート機能**: 継続確率が急低下した開発者に通知

**ダッシュボードの表示項目**:

- **継続確率**: 0.72（過去 3 ヶ月で-0.15 低下）
- **リスク要因**:
  - 活動間隔: 45 日（警告: 60 日を超えると離脱リスク大）
  - レビュー規模: 0.85（警告: 大規模レビューが続いている）
- **推奨行動**:
  - 小規模レビューを依頼して関与を維持
  - ペアレビューで協力度を向上

**シナリオ 3: プロジェクトマネージャー向けレポート**

**月次レポートの構成**:

1. **継続リスクサマリー**:

   - 高リスク開発者: 5 名（継続確率 < 0.3）
   - 中リスク開発者: 12 名（0.3 ≤ 継続確率 < 0.6）
   - 安定開発者: 43 名（継続確率 ≥ 0.6）

2. **リスク要因の分析**:

   - 活動間隔の増加: 8 名（平均 45 日 → 目標 14 日）
   - レビュー負荷の過多: 3 名（負荷 0.9 → 目標 0.5）
   - 協力度の低下: 6 名（協力度 0.2 → 目標 0.6）

3. **推奨施策**:
   - ペアレビュー制度の導入
   - 大規模レビューの分割
   - 定期的なレビュー依頼の仕組み化

### 6.10 マルチプロジェクトへの拡張可能性

**現状の制約（Nova 単一プロジェクト）**:

- 状態特徴量: 10 次元（プロジェクト横断特徴 4 次元を除外）
- 行動特徴量: 4 次元（プロジェクト横断フラグ 1 次元を除外）

**マルチプロジェクト拡張のための追加特徴量**:

**追加状態特徴量（4 次元）**:

1. **プロジェクト数**: 参加しているプロジェクトの数
2. **活動分散度**: プロジェクト間での活動の偏り（Gini 係数）
3. **メインプロジェクト貢献率**: 最も活動しているプロジェクトへの貢献割合
4. **プロジェクト横断協力スコア**: 異なるプロジェクトのメンバーとのレビュー割合

**追加行動特徴量（1 次元）**:

1. **プロジェクト横断フラグ**: そのレビューが別プロジェクトのメンバーとの協力か

**期待される効果**:

- **プロジェクト間の移動を考慮**: Nova から別プロジェクトへの移動を離脱と誤判定しない
- **活動の多様性を評価**: 複数プロジェクトへの分散が継続に与える影響を測定
- **プロジェクト横断協力の重要性**: 複数プロジェクトにまたがる協力が継続を促進するか検証

**実装の課題**:

1. **データ収集の複雑化**: 複数プロジェクトの Gerrit データを統合
2. **正規化の調整**: プロジェクト数などの新特徴量の適切な正規化
3. **モデルの複雑化**: 14 次元 → 19 次元への拡張によるオーバーフィッティングリスク

### 6.11 本研究の限界

**限界 1: 単一プロジェクトによる制約**

- OpenStack/Nova 単一プロジェクトを対象
- 他のプロジェクト（GitHub 利用など）では異なる結果の可能性
- **今後の課題**: GitHub ベースの OSS プロジェクトでの検証

**限界 2: 特徴量の網羅性**

- 14 の特徴量（状態 10+行動 4）で全要因を捉えられない
- タスクの専門性の一致度，開発者のモチベーション，組織的要因など未考慮
- **今後の課題**: テキストマイニングによるコミット メッセージ分析，開発者アンケートとの統合

**限界 3: 因果推論の限界**

- IRL の勾配値は「因果的影響の近似」
- 真の因果関係には介入実験が必要
- **今後の課題**: A/B テストによる施策効果の実証，傾向スコアマッチングによる因果推論

**限界 4: 時間粒度の固定**

- 3 ヶ月単位の訓練期間に固定
- より短期（1 ヶ月）や長期（6 ヶ月）での効果は未検証
- **今後の課題**: 様々な時間粒度での予測精度の比較

**限界 5: 離脱後の復帰を考慮していない**

- 12 ヶ月後の継続/離脱を二値分類
- 一時的な離脱後に復帰する開発者を考慮していない
- **今後の課題**: 生存時間解析（Survival Analysis）の適用

### 6.12 関連研究との比較

**ベースラインモデルとの比較**:

本研究では，Random Forest Case2（Simple Baseline）を対抗馬とし，同一の評価期間・特徴量で比較した。評価結果:

- **Random Forest Case2**: `outputs/singleproject/rf_nova_case2_simple/results.json`（各期間 1 スナップショット，データ水増し無し）
- **IRL + LSTM（提案手法）**: `results/review_continuation_cross_eval_nova/matrix_AUC_ROC.csv`

| 手法                        | 特徴量                    | F1（10 パターン平均）      | AUC-ROC（10 パターン平均） | 時系列考慮 |
| --------------------------- | ------------------------- | -------------------------- | -------------------------- | ---------- |
| RF Case2（Simple Baseline） | 状態・行動（14 次元）     | 0.60（範囲 0.50–0.67）     | 0.763（最大 <0.840）       | なし       |
| **IRL + LSTM（提案手法）**  | **状態・行動（14 次元）** | **0.67（範囲 0.59–0.71）** | **0.827（0.746–0.910）**   | **あり**   |

**本研究の特色（Case2 を対抗馬とした場合）**:

1. **評価期間の公平な一致**: 両モデルとも 2023 年の同一評価期間を使用し，訓練は 2021 年の各窓に揃えた。
2. **時系列パターンの学習**: IRL は全履歴をシーケンスとして学習し，期間が変わっても性能のばらつきが小さい（F1 標準偏差 0.038 vs RF Case2 0.052）。
3. **因果的解釈可能性**: 勾配ベース重要度により「どの行動を変化させると継続に効くか」を定量化。
4. **中期クロステンポラルの優位**: 0-3m→6-9m で AUC-ROC 0.910 を達成し，Case2 を大きく上回る。
5. **介入可能な施策**: 協力度や活動間隔など，組織的に変えやすい行動指標を重視する点で実務に直結。

**開発者継続予測研究との比較**:

| 研究                    | 対象               | 手法           | 予測期間      | 精度          |
| ----------------------- | ------------------ | -------------- | ------------- | ------------- |
| Chowdhury et al. (2019) | GitHub 開発者      | Deep Learning  | 6 ヶ月後      | F1 0.71       |
| Lin et al. (2020)       | Apache 開発者      | LSTM           | 3 ヶ月後      | AUC 0.78      |
| **本研究**              | **OpenStack/Nova** | **IRL + LSTM** | **12 ヶ月後** | **AUC 0.827** |

**本研究の特色**:

1. **長期予測**: 12 ヶ月後の継続を予測（従来は 3-6 ヶ月）
2. **行動推奨**: 勾配ベース重要度により具体的な施策を提案可能

---

### 6.13 10 パターンハイライト（IRL vs RF Case2）

詳細な Case2 の数値は 5.5.9 および 5.5.10 節に示した通りだが，ここでは主要なポイントのみ整理する。

- **全体平均**: IRL が F1 +0.07 / AUC-ROC +0.064 で優位。
- **最高性能**: IRL は 0-3m→6-9m で AUC-ROC 0.910。RF Case2 の最高は同期間予測（3-6m→3-6m）の F1 0.67。
- **最も弱いパターン**: RF Case2 は 0-3m→0-3m で F1 0.50 まで低下。IRL は 0.59 以上を維持し，ばらつきが小さい。
- **安定性**: IRL の F1 標準偏差 0.038 に対し，RF Case2 は 0.052 で期間依存性が大きい。

---

## 第 7 章：結論

### 7.1 本研究の貢献

1. **時系列学習と中期汎化能力**: LSTM ベースの IRL モデルにより，活動パターンの時系列変化を学習し，6-9m 評価期間への中期予測で優位性を実証（0-3m→6-9m で+8.1 ポイント，3-6m→6-9m で+6.9 ポイント）
2. **勾配ベース重要度の提案**: 因果的影響度を定量化する新しい手法により，介入可能な施策（協力度向上など）を提示
3. **RF と IRL の本質的違いを解明**: 最大 12 ランクの順位逆転を発見し，静的相関 vs 動的因果の違いを明確化
4. **10 パターン分析による詳細評価**: Case2（データ水増し無しのシンプル RF）を対抗馬とし，IRL が F1/AUC とも平均で優位，特に 0-3m→6-9m で突出することを確認
5. **実務的施策の提案**: 協力度向上、活動間隔管理など，介入可能な施策を提示

### 7.2 研究課題への回答

**RQ1: 提案モデルの予測精度は？**

- **10 パターン平均**: IRL F1 0.67 / AUC-ROC 0.827，RF Case2 F1 0.60 / AUC-ROC 0.763（IRL が精度・安定性で優位）
- **中期クロステンポラルの強み**: IRL は 0-3m→6-9m で AUC-ROC 0.910 を達成し，Case2 を大きく上回った。
- **安定性**: F1 標準偏差は IRL 0.038 vs RF Case2 0.052 で，期間が変わっても IRL のばらつきが小さい。
- **ベースラインの特徴**: Case2 は実装容易だが，0-3m→0-3m で F1 0.50 まで低下するなど期間依存性が大きい。
- IRL は因果的解釈可能性で優位性を持ち，施策設計に直結する。

**RQ2: 異なる訓練期間での汎化性能は？**

- 訓練期間から離れると精度低下（最大-7.8 ポイント）
- 後期モデル（9-12m）は汎化性が高い

**RQ3: どの特徴量が重要か？**

- 総レビュー数（+0.0165）、協力度（+0.0131）が最重要
- 平均活動間隔（-0.0107）、レビュー規模（-0.0124）が負の影響

**RQ4: RF と IRL で重要度はどう異なるか？**

- 最大 12 ランクの順位逆転（協力度: RF 14 位 → IRL 2 位）
- RF は静的相関、IRL は動的因果を学習

### 7.3 今後の課題

1. **複数プロジェクトでの検証**: GitHub プロジェクトなど
2. **特徴量の拡張**: タスクの専門性一致度など
3. **介入実験**: 実際のプロジェクトで施策の効果を検証
4. **マルチプロジェクト予測**: プロジェクト横断的な継続予測

---

## 付録：実装詳細

### A.1 Nova 単体モデルの実装

**ファイル**: `src/review_predictor/model/irl_predictor_nova_single.py`

**主要メソッド**:

- `state_to_tensor()` (line 574-605): 10 次元の状態特徴量をテンソルに変換
- `action_to_tensor()` (line 607-626): 4 次元の行動特徴量をテンソルに変換

**次元設定**:

```python
config = {
    'state_dim': 10,  # Nova単体: 10次元
    'action_dim': 4,  # Nova単体: 4次元
    'hidden_dim': 128,
    'learning_rate': 0.0001,
    'sequence': True,
    'dropout': 0.2
}
```

### A.2 勾配重要度の実装

**ファイル**: `scripts/analysis/extract_gradient_importance.py`

**計算手順**（コード概要）:

```python
# 1. テンソル作成（勾配追跡を有効化）
state_tensor = torch.tensor(states, requires_grad=True)
action_tensor = torch.tensor(actions, requires_grad=True)

# 2. Forward pass
reward = model(state_tensor, action_tensor, lengths)

# 3. Backward pass
reward.backward()

# 4. 勾配抽出
state_gradients = state_tensor.grad  # [batch, seq_len, 10]
action_gradients = action_tensor.grad  # [batch, seq_len, 4]

# 5. 平均化
state_importance = state_gradients.mean(dim=(0, 1))  # [10]
action_importance = action_gradients.mean(dim=(0, 1))  # [4]
```

### A.3 データファイル

**勾配重要度の平均値**:

- `results/review_continuation_cross_eval_nova/average_feature_importance/gradient_importance_average.json`

**訓練期間別の勾配重要度**:

- `results/review_continuation_cross_eval_nova/feature_importance_0_3/gradient_importance.json`
- `results/review_continuation_cross_eval_nova/feature_importance_3_6/gradient_importance.json`
- `results/review_continuation_cross_eval_nova/feature_importance_6_9/gradient_importance.json`
- `results/review_continuation_cross_eval_nova/feature_importance_9_12/gradient_importance.json`

**Random Forest 重要度**:

- `results/review_continuation_cross_eval_nova/feature_importance_rf/feature_importance.json`

---

## 参考文献

主要な参考文献リストは本論文の参考文献セクションを参照．

---

**文書作成日**: 2025 年 12 月 25 日
**対象プロジェクト**: OpenStack/Nova（単一プロジェクト）
**分析期間**: 2011 年～ 2023 年（12 年間）
**特徴量次元**: 状態 10 + 行動 4 = 14 次元（Nova 単体）
