# 第5章 ケーススタディ（拡張版）

## 5.1 データセット

本研究では，仮想マシンインスタンスのプロビジョニングとライフサイクル管理を行う，OpenStack の中核的なコンピュートサービス OpenStack/Nova を対象とした．表 5.1 は，分析対象プロジェクトの統計データを示す．

**表 5.1: データセットの概要**

| 項目 | 値 |
|------|-----|
| 対象期間 | 2021-01-01 ~ 2024-01-01 |
| 学習期間 | 2021-01-01 ~ 2023-01-01 |
| 予測期間 | 2023-01-01 ~ 2024-01-01 |
| 総レビュー依頼数 | 約 5,473 件 |
| 訓練時レビュアー数 | 約 70 人 |
| 予測時レビュアー数 | 約 60 人 |

---

## 5.2 RQ1: 逆強化学習に基づく提案モデルは，コードレビューにおけるタスク受け入れをどの程度予測できるか？

RQ1 では，構築した IRL のモデルが訓練対象の期間における開発の行動パターンを適切に学習し妥当な予測を行うことができるのか，また過学習になっていないかを検証することを目的としている．

図 5.1 は，3-6ヶ月後に貢献があるかをラベル付し学習した 3-6m のモデルにおける，学習期間での予測の精度と，予測期間での予測の精度を示している．具体的には，Precision, recall, F1 Score, AUC-ROC の 4 つの評価指標を用いており，各指標のグラフの左側が訓練期間，右側が予測期間の結果を示している．

この結果から学習期間内で評価を行なった値と，評価期間で評価を行なった値が同様の結果を示す．これは，提案モデルが学習データに過剰に適合しておらず，未知の期間に対しても一定の汎化性を持つことを示唆している．

また，評価期間の予測における **AUC-ROC が 0.820** であり，0.500 よりも高いため，提案モデルがレビュー依頼を承諾する貢献者と離脱者を効果的に区別できることを示している．

**図 5.1: 訓練・予測期間における予測結果**

```
         1.0
         0.9
         0.8
         0.7
         0.6
         0.5
         0.4
         0.3
         0.2
         0.1
           0
              Precision   recall    F1 Score   AUC-ROC
              ■訓練期間   ■予測期間
```

**RQ1の回答**: 提案モデルは AUC-ROC 0.820 で予測可能であり，過学習せず未知データに対しても汎化性を持つことが確認された．

---

## 5.3 RQ2: 学習方法や予測期間の長さに応じて，予測モデルの精度はどのように変化するか?

図 5.2 は，学習期間と予測期間の組み合わせによる精度の違いをヒートマップで示す．横軸は訓練期間，縦軸は評価期間を示す．4 つの評価指標で評価を行なった結果である．

ラベル付の期間と予測期間が一致する際に予測精度が最も高くなることが期待されたが，9-12m 以外のモデルで訓練期間より後の区間（0-3m の場合は 3-6m,6-9m など）で AUC-ROC が高くなる傾向が見られ，期待された結果は得られなかった．

3-6m のモデルで全体的に AUC-ROC が高く，9-12m のモデルでは AUC-ROC が低下することがわかった．

**図 5.2: 各モデルのクロス評価**

```
訓練期間→  0-3m    3-6m    6-9m    9-12m
評価↓
期間  [Precision/Recall/F1値/AUC-ROCのヒートマップ]
```

**RQ2の回答**: 学習期間3-6mのモデルが最も高い予測精度を示し，学習期間に応じて高い適合率を得られる予測期間は異なることが明らかになった．

---

## 5.4 RQ3: 推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か？

### 5.4.1 時間経過による特徴量重要度の変化

図 5.3 は報酬関数における，状態特徴量と行動特徴量の各特徴量の重要度をそれぞれモデル別で分析したものである．

図 5.3 からは，レビュアーの貢献を予測する状態特徴量として**総レビュー数**が，行動特徴量としては**協力度**が強い正の影響を与えていることが明らかになった．また，**平均活動間隔**や**レビュー規模**は強い負の影響を与えていることが明らかになった．

**図 5.3: 各特徴量の予測における重要度（時系列変化）**

```
重要度
 0.03
      ●総レビュー数（最も重要・正の影響）
 0.02  ●総コミット数
      ●最近の活動頻度
 0.01  ▲協力度（行動特徴で最重要）
      ▲強度（ファイル数）
  0.00 ................................................
      ▼平均活動間隔（負の影響）
-0.01  ▼レビュー規模

      0-3m    3-6m    6-9m    9-12m
                訓練期間
```

**重要な発見**:
- 初期段階（0-3m）では**総コミット数**などの活動量がモデルに寄与
- 期間が進むにつれて活動量の影響が減少
- **協力度**や**応答速度**などの質的要因の寄与が高まる

**RQ3の回答**: 短期では活動量が重視され，長期では協力度や応答速度などの質的な要因が重視されるようになることが明らかになった．

---

## 5.5 RQ4（新規追加）: Random ForestとIRLでは、特徴量重要度にどのような違いがあるか？

### 5.5.1 比較手法の概要

本研究では，提案するIRLモデルの特徴量重要度をRandom Forest（RF）と比較することで，両手法の違いを明らかにする．RFは10パターンのSliding Window方式で学習し，IRLは4つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）の平均を用いた．

### 5.5.2 特徴量重要度の定量的比較

**表 5.2: RF vs IRL 特徴量重要度 Top 5**

| 順位 | Random Forest（Gini） | 重要度 | IRL（勾配ベース） | 勾配値 |
|------|---------------------|--------|----------------|--------|
| 1位 | 応答速度 | 26.61% | 総レビュー数 | +0.0165 |
| 2位 | 総レビュー数 | 25.45% | 協力度 | +0.0131 |
| 3位 | 最近の活動頻度 | 16.21% | 平均活動間隔 | -0.0107 |
| 4位 | 経験日数 | 9.28% | 強度（ファイル数） | +0.0083 |
| 5位 | 平均活動間隔 | 6.60% | 総コミット数 | +0.0080 |

### 5.5.3 劇的な順位逆転

両モデル間で**最大12ランクの順位差**が観測された：

**RF最重要 → IRL低重要**:
- **応答速度**: RF 1位（26.61%） → IRL 11位（勾配値+0.0027）【差: +10】
- **経験日数**: RF 4位（9.28%） → IRL 12位（勾配値-0.0026）【差: +8】
- **最近の受諾率**: RF 6位（4.20%） → IRL 14位（勾配値-0.0001）【差: +8】

**IRL最重要 → RF低重要**:
- **協力度**: IRL 2位（勾配値+0.0131） → RF 14位（0.00%）【差: -12】
- **コード品質スコア**: IRL 6位（勾配値-0.0078） → RF 13位（0.00%）【差: -7】
- **総コミット数**: IRL 5位（勾配値+0.0080） → RF 11位（0.00%）【差: -6】

### 5.5.4 重要度の集中度

**表 5.3: モデル別重要度集中度**

| モデル | 上位5特徴の合計 | 上位10特徴の合計 | 特徴 |
|--------|----------------|------------------|------|
| RF | 84.14% | 100.00% | 極端に集中 |
| IRL | 0.0565（絶対値） | 0.0860（絶対値） | 分散的 |

RFは少数の特徴（応答速度、総レビュー数）に**極端に依存**する一方，IRLは多様な特徴を**バランスよく考慮**している．

### 5.5.5 正負の勾配による影響方向（IRLのみ）

IRLの勾配ベース重要度は，特徴量の影響方向を示す：

**正の勾配（継続を促進）**:
- 総レビュー数（+0.0165）
- 協力度（+0.0131）
- 総コミット数（+0.0080）
- 強度/ファイル数（+0.0083）

**負の勾配（継続を抑制）**:
- 平均活動間隔（-0.0107）: 活動間隔が開くと離脱しやすい
- コード品質スコア（-0.0078）: 品質低下は継続を阻害
- 経験日数（-0.0026）: 経験が長いほど離脱傾向（バーンアウト）

**RQ4の回答**: RFは「応答速度」「総レビュー数」などの静的な統計パターンを重視する一方，IRLは「協力度」「総コミット数」などの行動の因果的影響を重視する．両モデルの特徴量重要度には最大12ランクの劇的な逆転が存在し，学習メカニズムの本質的な違いを反映している．

---

## 5.6 各RQのまとめ

| RQ | 主要な発見 | 意義 |
|----|-----------|------|
| RQ1 | AUC-ROC 0.820で予測可能 | IRLモデルの有効性を実証 |
| RQ2 | 3-6mモデルが最高精度 | 学習期間の最適化が重要 |
| RQ3 | 短期は活動量、長期は質的要因 | 時間経過による重要特徴の変化 |
| RQ4 | RFとIRLで最大12ランクの逆転 | モデル選択が解釈に大きく影響 |
