# 第5章 ケーススタディ（特徴量定義完全版）

## 5.1 データセット

本研究では，仮想マシンインスタンスのプロビジョニングとライフサイクル管理を行う，OpenStack の中核的なコンピュートサービス **OpenStack/Nova** を対象とした．表 5.1 は，分析対象プロジェクトの統計データを示す．

**表 5.1: データセットの概要**

| 項目 | 値 |
|------|-----|
| 対象期間 | 2021-01-01 ~ 2024-01-01 |
| 学習期間 | 2021-01-01 ~ 2023-01-01 |
| 予測期間 | 2023-01-01 ~ 2024-01-01 |
| 総レビュー依頼数 | 約 5,473 件 |
| 訓練時レビュアー数 | 約 70 人 |
| 予測時レビュアー数 | 約 60 人 |

**注**: Nova単一プロジェクトを対象としているため、マルチプロジェクト特徴量（プロジェクト数、プロジェクト横断活動など）は使用していない。

---

## 5.2 特徴量の詳細定義

本研究で使用する特徴量は、**状態特徴量10次元**と**行動特徴量4次元**の計14次元である。すべての特徴量は0-1の範囲に正規化されている。

### 5.2.1 状態特徴量（State Features）: 10次元

レビュアーの現在の状態を表す特徴量。各時点での開発者の累積的な活動実績や傾向を捉える。

**表5.2: 状態特徴量の定義（10次元）**

| # | 特徴量名 | 定義 | 正規化方法 | 日本語名（図表用） |
|---|---------|------|-----------|-------------------|
| 1 | experience_days | 初回活動から現在までの経過日数 | 730日(2年)でキャップ、0-1正規化 | 経験日数 |
| 2 | total_changes | これまでの累積コミット数 | 500件でキャップ、0-1正規化 | 総コミット数 |
| 3 | total_reviews | これまでの累積レビュー数 | 500件でキャップ、0-1正規化 | 総レビュー数 |
| 4 | recent_activity_frequency | 直近30日の1日あたり平均活動量<br>(= 活動数 / 30) | 0-1範囲（事前正規化済み） | 最近の活動頻度 |
| 5 | avg_activity_gap | 連続する活動間の平均日数<br>(= Σ活動間隔 / (活動数-1)) | 60日でキャップ、0-1正規化 | 平均活動間隔 |
| 6 | activity_trend | 直近30日 vs 30-60日前の活動比較<br>比率>1.2:増加(1.0)<br>比率<0.8:減少(0.0)<br>その他:安定(0.5) | カテゴリ変数エンコード | 活動トレンド |
| 7 | collaboration_score | 協力活動割合<br>(review, merge等 / 総活動) | 0-1範囲（事前正規化済み） | 協力スコア |
| 8 | code_quality_score | 品質活動割合<br>(test, doc, refactor, fix / 総活動) | 0-1範囲（事前正規化済み） | コード品質スコア |
| 9 | recent_acceptance_rate | 直近30日のレビュー受諾率<br>(= 受諾数 / (受諾数+拒否数)) | 0-1範囲（事前正規化済み） | 最近の受諾率 |
| 10 | review_load | 現在の負荷相対値<br>(= 未完了数 / 平均数) | 0-1に正規化、1.0超はキャップ | レビュー負荷 |

---

### 5.2.2 行動特徴量（Action Features）: 4次元

レビュアーがレビュー依頼に対して取る具体的な行動を表す特徴量。

**表5.3: 行動特徴量の定義（4次元）**

| # | 特徴量名 | 定義 | 正規化方法 | 日本語名（図表用） |
|---|---------|------|-----------|-------------------|
| 1 | intensity | 1回のレビューで変更されたファイル数 | 0-1範囲（事前正規化済み） | 強度（ファイル数） |
| 2 | collaboration | そのレビュイーを過去にレビューした回数<br>協力関係の強さを示す | 0-1範囲（事前正規化済み） | 協力度 |
| 3 | response_speed | レビューリクエストから応答までの素早さ<br>`速度 = 1.0 / (1.0 + 日数 / 3.0)`<br>即日→~1.0, 3日→~0.5, 遅い→~0.0 | 日数を速度に変換し0-1正規化 | 応答速度 |
| 4 | review_size | 1回のレビューで変更された総行数 | 0-1範囲（事前正規化済み） | レビュー規模（行数） |

**重要**: 応答速度は、元の`response_time`（日数、値が大きいほど遅い）を素早さに変換している。これにより、「速い応答」が正の値として扱われる。

---

## 5.3 RQ1: 逆強化学習に基づく提案モデルは，コードレビューにおけるタスク受け入れをどの程度予測できるか？

RQ1 では，構築した IRL のモデルが訓練対象の期間における開発の行動パターンを適切に学習し妥当な予測を行うことができるのか，また過学習になっていないかを検証することを目的としている．

図 5.1 は，3-6ヶ月後に貢献があるかをラベル付し学習した 3-6m のモデルにおける，学習期間での予測の精度と，予測期間での予測の精度を示している．具体的には，Precision, recall, F1 Score, AUC-ROC の 4 つの評価指標を用いており，各指標のグラフの左側が訓練期間，右側が予測期間の結果を示している．

この結果から学習期間内で評価を行なった値と，評価期間で評価を行なった値が同様の結果を示す．これは，提案モデルが学習データに過剰に適合しておらず，未知の期間に対しても一定の汎化性を持つことを示唆している．

また，評価期間の予測における **AUC-ROC が 0.820** であり，0.500 よりも高いため，提案モデルがレビュー依頼を承諾する貢献者と離脱者を効果的に区別できることを示している．

**図 5.1: 訓練・予測期間における予測結果**

```
         1.0
         0.9
         0.8
         0.7
         0.6
         0.5
         0.4
         0.3
         0.2
         0.1
           0
              Precision   recall    F1 Score   AUC-ROC
              ■訓練期間   ■予測期間
```

**RQ1の回答**: 提案モデルは AUC-ROC 0.820 で予測可能であり，過学習せず未知データに対しても汎化性を持つことが確認された．

---

## 5.4 RQ2: 学習方法や予測期間の長さに応じて，予測モデルの精度はどのように変化するか?

図 5.2 は，学習期間と予測期間の組み合わせによる精度の違いをヒートマップで示す．横軸は訓練期間，縦軸は評価期間を示す．4 つの評価指標で評価を行なった結果である．

ラベル付の期間と予測期間が一致する際に予測精度が最も高くなることが期待されたが，9-12m 以外のモデルで訓練期間より後の区間（0-3m の場合は 3-6m,6-9m など）で AUC-ROC が高くなる傾向が見られ，期待された結果は得られなかった．

3-6m のモデルで全体的に AUC-ROC が高く，9-12m のモデルでは AUC-ROC が低下することがわかった．

**図 5.2: 各モデルのクロス評価**

```
訓練期間→  0-3m    3-6m    6-9m    9-12m
評価↓
期間  [Precision/Recall/F1値/AUC-ROCのヒートマップ]
```

**RQ2の回答**: 学習期間3-6mのモデルが最も高い予測精度を示し，学習期間に応じて高い適合率を得られる予測期間は異なることが明らかになった．

---

## 5.5 RQ3: 推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か？

### 5.5.1 時間経過による特徴量重要度の変化

図 5.3 は報酬関数における，状態特徴量と行動特徴量の各特徴量の重要度をそれぞれモデル別で分析したものである．

図 5.3 からは，レビュアーの貢献を予測する状態特徴量として**総レビュー数**が，行動特徴量としては**協力度**が強い正の影響を与えていることが明らかになった．また，**平均活動間隔**や**レビュー規模（行数）**は強い負の影響を与えていることが明らかになった．

**図 5.3: 各特徴量の予測における重要度（時系列変化）**

```
重要度
 0.03
      ●総レビュー数（最も重要・正の影響）
 0.02  ●総コミット数
      ●最近の活動頻度
 0.01  ▲協力度（行動特徴で最重要）
      ▲強度（ファイル数）
  0.00 ................................................
      ▼平均活動間隔（負の影響）
-0.01  ▼レビュー規模（行数）

      0-3m    3-6m    6-9m    9-12m
                訓練期間
```

**重要な発見**:
- 初期段階（0-3m）では**総コミット数**などの活動量がモデルに寄与
- 期間が進むにつれて活動量の影響が減少
- **協力度**や**応答速度**などの質的要因の寄与が高まる

### 5.5.2 特徴量の符号と意味

IRLの勾配ベース重要度は、特徴量の影響方向を示す：

**正の勾配（継続を促進）**:
- **総レビュー数** (+0.0165): 過去の実績が多いほど継続しやすい
- **協力度** (+0.0131): 協力的な行動が継続に強く寄与
- **総コミット数** (+0.0080): 活動量が多いほど継続しやすい
- **強度（ファイル数）** (+0.0083): 適度な負荷のレビューが継続に寄与

**負の勾配（継続を抑制）**:
- **平均活動間隔** (-0.0107): 活動間隔が開くと離脱しやすい
- **コード品質スコア** (-0.0078): 品質低下が継続を阻害
- **レビュー規模（行数）** (-0.0034): 大規模レビューは負担増加
- **経験日数** (-0.0026): 経験が長いほど離脱傾向（バーンアウト）

**RQ3の回答**: 短期では活動量（総コミット数、総レビュー数）が重視され，長期では協力度や応答速度などの質的な要因が重視されるようになることが明らかになった．また、平均活動間隔やレビュー規模は継続を阻害する負の影響を持つ。

---

## 5.6 RQ4: Random ForestとIRLでは、特徴量重要度にどのような違いがあるか？

### 5.6.1 比較手法の概要

本研究では，提案するIRLモデルの特徴量重要度をRandom Forest（RF）と比較することで，両手法の違いを明らかにする．RFは10パターンのSliding Window方式で学習し，IRLは4つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）の平均を用いた．

**重要**: RFとIRLは**同じ14の特徴量**（状態10+行動4）を使用しているが、重要度の算出方法が異なる：
- **RF**: Gini係数ベース（情報利得による分岐貢献度）
- **IRL**: 勾配ベース（報酬関数への影響度）

### 5.6.2 特徴量重要度の定量的比較

**表 5.4: RF vs IRL 特徴量重要度 Top 5（同じ特徴量セットを使用）**

| 順位 | Random Forest（Gini） | 重要度 | IRL（勾配ベース） | 勾配値 | 解釈 |
|------|---------------------|--------|----------------|--------|------|
| 1位 | 応答速度 | 26.61% | 総レビュー数 | +0.0165 | RFは速い応答を重視、IRLは実績を重視 |
| 2位 | 総レビュー数 | 25.45% | 協力度 | +0.0131 | 両者とも総レビュー数を重視、IRLは協力度も重要視 |
| 3位 | 最近の活動頻度 | 16.21% | 平均活動間隔 | -0.0107 | RFは活動頻度、IRLは活動間隔（負の影響）を重視 |
| 4位 | 経験日数 | 9.28% | 強度（ファイル数） | +0.0083 | RFは経験、IRLは行動の強度を重視 |
| 5位 | 平均活動間隔 | 6.60% | 総コミット数 | +0.0080 | 両者とも活動間隔/コミット数を考慮 |

### 5.6.3 劇的な順位逆転

両モデル間で**最大12ランクの順位差**が観測された：

**RF最重要 → IRL低重要**:
- **応答速度**: RF 1位（26.61%） → IRL 11位（勾配値+0.0027）【差: +10】
  - RFは「速い人」を識別、IRLは速さの変化の影響は限定的と判断
- **経験日数**: RF 4位（9.28%） → IRL 12位（勾配値-0.0026）【差: +8】
  - RFは経験を重視、IRLは経験が長いほど離脱傾向（負の勾配）を学習
- **最近の受諾率**: RF 6位（4.20%） → IRL 14位（勾配値-0.0001）【差: +8】
  - RFは過去の受諾パターンを重視、IRLでは因果的影響がほぼゼロ

**IRL最重要 → RF低重要**:
- **協力度**: IRL 2位（勾配値+0.0131） → RF 14位（0.00%）【差: -12】
  - IRLは協力的行動が継続に直結すると学習、RFはGini分岐で無視
- **コード品質スコア**: IRL 6位（勾配値-0.0078） → RF 13位（0.00%）【差: -7】
  - IRLは品質低下が継続を阻害すると学習、RFは分岐に使わず
- **総コミット数**: IRL 5位（勾配値+0.0080） → RF 11位（0.00%）【差: -6】
  - IRLは活動量の因果的影響を重視、RFは無視

### 5.6.4 重要度の集中度

**表 5.5: モデル別重要度集中度**

| モデル | 上位5特徴の合計 | 上位10特徴の合計 | 特徴 |
|--------|----------------|------------------|------|
| RF | 84.14% | 100.00% | 極端に集中（応答速度、総レビュー数で52%） |
| IRL | 0.0565（絶対値） | 0.0860（絶対値） | 分散的（多様な特徴を考慮） |

RFは少数の特徴（応答速度、総レビュー数）に**極端に依存**する一方，IRLは多様な特徴を**バランスよく考慮**している．

### 5.6.5 行動特徴 vs 状態特徴

**RFが重視する特徴**:
- 状態特徴が上位を占める（応答速度、総レビュー数、最近の活動頻度）
- 「どのような状態の人が継続しやすいか」を学習

**IRLが重視する特徴**:
- 行動特徴が上位に含まれる（協力度2位、強度4位）
- 「どのような行動を選択すると継続しやすくなるか」を学習

**RQ4の回答**: RFは「応答速度」「総レビュー数」などの静的な統計パターンを重視する一方，IRLは「協力度」「総コミット数」などの行動の因果的影響を重視する．両モデルの特徴量重要度には最大12ランクの劇的な逆転が存在し、学習メカニズムの本質的な違いを反映している．

---

## 5.7 各RQのまとめ

| RQ | 主要な発見 | 意義 |
|----|-----------|------|
| RQ1 | AUC-ROC 0.820で予測可能 | IRLモデルの有効性を実証 |
| RQ2 | 3-6mモデルが最高精度 | 学習期間の最適化が重要 |
| RQ3 | 短期は活動量、長期は質的要因<br>負の勾配（活動間隔、品質低下）も重要 | 時間経過による重要特徴の変化と<br>継続を阻害する要因の発見 |
| RQ4 | RFとIRLで最大12ランクの逆転<br>RFは状態重視、IRLは行動重視 | モデル選択が解釈に大きく影響<br>因果的影響 vs 統計的相関の違い |

**注**: 本研究はNova単一プロジェクトを対象としているため、すべての分析は状態10次元・行動4次元の特徴量セットに基づく。マルチプロジェクト特徴量（4+1次元）は使用していない。
