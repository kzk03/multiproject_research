# RQ3: 勾配ベース特徴量重要度の算出方法（詳細）

## 1. 概要

RQ3では、**推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か**を明らかにするため、**勾配ベースの特徴量重要度**を算出した。

本手法では、訓練済みIRLモデルの報酬関数に対して各特徴量が与える影響度を、**勾配（gradient）を用いて定量化**する。これにより、各特徴量の変化が報酬（継続確率）に与える因果的な影響を測定できる。

---

## 2. 勾配ベース特徴量重要度の理論的背景

### 2.1 報酬関数の役割

逆強化学習（IRL）では、専門家（継続的に貢献するレビュアー）の行動軌跡から、その行動を説明する**報酬関数 R(s, a; θ)** を学習する。ここで：

- **s**: 状態（State）= レビュアーの現在の状態（経験日数、総レビュー数など10次元）
- **a**: 行動（Action）= レビュー依頼に対する行動（協力度、応答速度など4次元）
- **θ**: ニューラルネットワークのパラメータ

報酬関数は、「この状態でこの行動を取ると、どれだけ将来の継続に寄与するか」を表すスカラー値を出力する。

### 2.2 勾配の意味

特徴量 **x_i**（例: 総レビュー数）に対する報酬関数の勾配は、以下のように定義される：

```
∂R(s, a; θ) / ∂x_i
```

この勾配は、**特徴量 x_i を微小量 Δx だけ変化させたときに、報酬がどれだけ変化するか**を示す。

- **正の勾配（+）**: 特徴量が増加すると報酬が増加 → 継続を促進
- **負の勾配（-）**: 特徴量が増加すると報酬が減少 → 継続を抑制
- **ゼロに近い勾配**: 特徴量の変化が報酬に与える影響は小さい

---

## 3. 本研究における算出手順

本研究では、訓練済みIRLモデルを用いて、以下の手順で勾配ベース特徴量重要度を算出した。

### ステップ1: 訓練済みモデルの準備

4つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）で学習したIRLモデルを用意する。各モデルは、以下のアーキテクチャを持つ：

```
状態エンコーダー: [10次元] → [128次元] → [64次元]
行動エンコーダー: [4次元]  → [128次元] → [64次元]
LSTM:             [64次元]  → [128次元] (時系列処理)
報酬予測器:        [128次元] → [64次元] → [1次元] (報酬スコア)
```

### ステップ2: 評価データの準備

評価期間のレビュアーの活動軌跡を準備する。各軌跡は以下の形式：

```
軌跡 τ = [(s_1, a_1), (s_2, a_2), ..., (s_T, a_T)]
```

ここで、T はその開発者の活動回数（可変長）。

### ステップ3: 勾配の計算（PyTorchによる自動微分）

各軌跡に対して、以下の計算を行う：

#### 3.1 順伝播（Forward Pass）

```python
# 状態と行動をテンソルに変換
state_tensor = torch.tensor(states, requires_grad=True)  # [T, 10]
action_tensor = torch.tensor(actions, requires_grad=True)  # [T, 4]

# モデルで報酬を予測
reward, continuation_prob = model(state_tensor, action_tensor, lengths)
```

#### 3.2 逆伝播（Backward Pass）

報酬に対する各特徴量の勾配を計算：

```python
# 報酬に対する状態特徴量の勾配
reward.backward(retain_graph=True)
state_gradients = state_tensor.grad  # [T, 10]

# 報酬に対する行動特徴量の勾配
action_gradients = action_tensor.grad  # [T, 4]
```

#### 3.3 勾配の平均化

各特徴量の重要度は、**全時点での勾配の平均絶対値**または**平均値**として算出：

```python
# 状態特徴量の重要度（各次元ごと）
state_importance = state_gradients.mean(dim=0)  # [10]

# 行動特徴量の重要度（各次元ごと）
action_importance = action_gradients.mean(dim=0)  # [4]
```

### ステップ4: 複数軌跡での平均化

評価データ中の**全レビュアーの軌跡**に対して上記の計算を行い、最終的な重要度を算出：

```python
# 全軌跡での平均
final_state_importance = mean([state_importance_τ for τ in trajectories])
final_action_importance = mean([action_importance_τ for τ in trajectories])
```

### ステップ5: 4訓練期間での平均化

最終的な特徴量重要度は、4つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）で算出された重要度の**平均値**として定義：

```python
# 4期間の平均
state_importance_avg = mean([
    state_importance_0_3m,
    state_importance_3_6m,
    state_importance_6_9m,
    state_importance_9_12m
])
```

---

## 4. 算出結果の解釈

### 4.1 出力形式

算出結果は、以下のJSON形式で保存される（`gradient_importance_average.json`）：

```json
{
  "state_importance": {
    "経験日数": -0.002631,
    "総コミット数": 0.007953,
    "総レビュー数": 0.016513,
    "最近の活動頻度": 0.007642,
    "平均活動間隔": -0.010681,
    "活動トレンド": -0.006419,
    "協力スコア": 0.004115,
    "コード品質スコア": -0.007848,
    "最近の受諾率": -0.000113,
    "レビュー負荷": 0.001054
  },
  "action_importance": {
    "強度（ファイル数）": 0.008296,
    "協力度": 0.013102,
    "応答速度": 0.002690,
    "レビュー規模（行数）": -0.003400
  },
  "method": "gradient_based"
}
```

### 4.2 正負の符号の意味

#### 正の勾配（例: 総レビュー数 +0.0165）

```
解釈: 総レビュー数が増加すると、報酬（継続確率）が増加する
意味: 過去のレビュー実績が多いほど、将来も継続しやすい
```

#### 負の勾配（例: 平均活動間隔 -0.0107）

```
解釈: 平均活動間隔が増加すると、報酬（継続確率）が減少する
意味: 活動間隔が開く（間が空く）ほど、離脱しやすくなる
```

### 4.3 絶対値の意味

勾配の**絶対値**は、その特徴量の重要度を示す：

- |勾配| が大きい → その特徴量の変化が報酬に強く影響
- |勾配| が小さい → その特徴量の変化が報酬に与える影響は限定的

---

## 5. Random Forestとの比較

### 5.1 算出方法の違い

| 観点 | Random Forest（Gini係数） | IRL（勾配ベース） |
|------|-------------------------|------------------|
| **何を測るか** | 決定木の分岐での情報利得 | 報酬関数への微分的影響 |
| **解釈** | 過去データでの判別貢献度（相関） | 特徴量変化が報酬に与える影響（因果） |
| **符号** | 常に正（0-1に正規化） | 正負両方（-∞〜+∞、実際は-0.02〜+0.02程度） |
| **時系列考慮** | なし（各時点独立） | あり（LSTMで系列学習） |
| **合計** | 1.0（正規化済み） | 任意（絶対値で比較） |

### 5.2 具体例: 応答速度

**Random Forest（Gini係数ベース）**:
```
重要度: 26.61% (1位)
解釈: 「応答が速い人」と「遅い人」を判別する際に、
      この特徴量が最も情報利得をもたらした
意味: 過去データで「応答速度」が継続/離脱の判別に最も有用だった（相関）
```

**IRL（勾配ベース）**:
```
重要度: +0.0027 (11位)
解釈: 応答速度を変化させても、報酬（継続確率）はあまり変化しない
意味: 応答速度の変化が継続に与える因果的影響は限定的
```

**なぜ逆転するのか**:
- **RFの視点**: 「応答が速い人は継続率が高い」という統計的パターンを学習
- **IRLの視点**: 「応答速度を速くする行動」が報酬を増加させる因果的影響は小さい
  - 理由: 応答速度は個人特性で変更しにくいため、行動選択の最適化には寄与しない

---

## 6. 勾配ベース手法の利点

### 6.1 因果的解釈が可能

勾配は「特徴量を変化させたときの報酬の変化率」を表すため、**因果的な影響**を推定できる。

例: 「協力度を0.1増やすと、報酬が0.0131×0.1 = 0.00131増加する」

### 6.2 介入可能な特徴量を重視

IRLの勾配ベース重要度は、**行動を変化させることで変更可能な特徴量**（協力度、活動間隔など）を重視する傾向がある。

これは、実務的な施策設計（「どのような行動を促せば継続しやすくなるか」）に直接役立つ。

### 6.3 正負の方向性がわかる

Gini係数では「重要か否か」のみだが、勾配ベースでは：
- 正の勾配 → 増やすべき特徴量
- 負の勾配 → 減らすべき特徴量

が明確になる。

---

## 7. 本研究での適用結果サマリー

### 7.1 最も重要な正の特徴量（継続を促進）

1. **総レビュー数** (+0.0165): 過去の実績が最重要
2. **協力度** (+0.0131): 協力的な行動が継続に直結
3. **強度（ファイル数）** (+0.0083): 適度な負荷のレビューが有益
4. **総コミット数** (+0.0080): 活動量の多さが寄与

### 7.2 最も重要な負の特徴量（継続を抑制）

1. **平均活動間隔** (-0.0107): 活動間隔が開くと離脱
2. **コード品質スコア** (-0.0078): 品質低下が継続を阻害
3. **活動トレンド** (-0.0064): 活動減少傾向が離脱を示唆
4. **レビュー規模（行数）** (-0.0034): 大規模レビューは負担増加

### 7.3 時間経過による変化

図5.3に示すように、訓練期間が長くなるにつれて：

- **活動量特徴（総コミット数）の重要度が低下**
- **質的要因（協力度、応答速度）の重要度が上昇**

この変化は、短期的な継続予測では「活動量」が重要だが、長期的には「行動の質」が重要になることを示唆している。

---

## 8. 実装上の詳細

### 8.1 使用ツール

- **フレームワーク**: PyTorch
- **自動微分**: `torch.autograd`を使用
- **勾配計算**: `tensor.backward()`で自動計算

### 8.2 計算コスト

- 各軌跡ごとに順伝播・逆伝播を実行
- 評価データ中の全レビュアー（約60人）× 全時点（平均10-20時点）で計算
- 計算時間: 4訓練期間で約5-10分（CPU）

### 8.3 数値の安定性

- 勾配は-0.02〜+0.02程度の小さい値
- 4訓練期間の平均を取ることで、ノイズを低減
- 絶対値での順位付けにより、正負を問わず重要度を比較

---

## 9. 卒論での記述例

### 簡潔版（本文用）

> 本研究では、訓練済みIRLモデルの報酬関数に対する勾配を計算することで、各特徴量の重要度を定量化した。具体的には、各特徴量に対する報酬の偏微分 ∂R(s,a;θ)/∂x_i を算出し、その平均値を特徴量重要度とした。勾配が正の場合、その特徴量の増加が継続確率を高めることを示し、負の場合は継続確率を低下させることを示す。この手法により、各特徴量の変化が報酬に与える因果的な影響を測定できる。

### 詳細版（付録用）

> **勾配ベース特徴量重要度の算出手順**
>
> 1. 訓練済みIRLモデル（状態10次元、行動4次元）を準備
> 2. 評価データの各レビュアー軌跡 τ = [(s_1, a_1), ..., (s_T, a_T)] に対して：
>    a. 状態・行動テンソルを作成（requires_grad=True）
>    b. モデルで報酬を予測: r = R(s, a; θ)
>    c. 報酬に対する勾配を計算: ∂r/∂s, ∂r/∂a
>    d. 各特徴量の勾配を時間方向で平均化
> 3. 全軌跡での勾配を平均化
> 4. 4訓練期間（0-3m, 3-6m, 6-9m, 9-12m）での勾配を平均化
> 5. 最終的な特徴量重要度として報告

---

## 10. 参考文献

勾配ベース特徴量重要度は、以下の研究分野で広く使用されている手法である：

- **深層学習の解釈性**: Saliency Maps, Gradient-based Attribution
- **強化学習**: Policy Gradient, Actor-Critic
- **逆強化学習**: Feature Expectation Matching, Maximum Entropy IRL

本研究では、訓練済みIRLモデルの報酬関数に対して勾配を計算することで、各特徴量が「継続報酬」に与える因果的な影響を定量化した。
