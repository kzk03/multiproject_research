# 5.5 RQ3: 推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か？

## 5.5.1 勾配ベース特徴量重要度の算出方法

本研究では、訓練済みIRLモデルから特徴量重要度を抽出するため、**勾配ベースの手法**を用いた。この手法は、各特徴量の変化が報酬関数に与える影響を定量化することで、継続に寄与する要因を明らかにする。

### 手法の概要

逆強化学習で学習した報酬関数 R(s, a; θ) に対して、各特徴量 x_i の勾配を計算する：

```
重要度(x_i) = ∂R(s, a; θ) / ∂x_i
```

ここで：
- **s**: 状態特徴量（10次元）= 経験日数、総レビュー数など
- **a**: 行動特徴量（4次元）= 協力度、応答速度など
- **θ**: ニューラルネットワークのパラメータ
- **∂/∂x_i**: 特徴量 x_i に関する偏微分

### 算出手順

**ステップ1: 訓練済みモデルの準備**

4つの訓練期間（0-3m, 3-6m, 6-9m, 9-12m）で学習したIRLモデルを用意する。各モデルは以下のアーキテクチャを持つ：

```
状態エンコーダー: [10次元] → [128次元] → [64次元]
行動エンコーダー: [4次元]  → [128次元] → [64次元]
LSTM (時系列):    [64次元]  → [128次元]
報酬予測器:        [128次元] → [64次元] → [1次元]
```

**ステップ2: 勾配の計算**

評価データの各レビュアー軌跡 τ = [(s_1, a_1), (s_2, a_2), ..., (s_T, a_T)] に対して、PyTorchの自動微分機能を用いて勾配を計算する：

```python
# 順伝播: モデルで報酬を予測
reward = model(state_tensor, action_tensor, lengths)

# 逆伝播: 報酬に対する各特徴量の勾配を計算
reward.backward()
state_gradients = state_tensor.grad  # 状態特徴量の勾配 [T, 10]
action_gradients = action_tensor.grad  # 行動特徴量の勾配 [T, 4]
```

**ステップ3: 勾配の平均化**

各特徴量の重要度は、以下の3段階の平均化により算出される：

1. **時間方向の平均**: 各軌跡内の全時点での勾配を平均
2. **軌跡間の平均**: 評価データ中の全レビュアー（約60人）の勾配を平均
3. **訓練期間間の平均**: 4つの訓練期間での勾配を平均

```
最終重要度(x_i) = mean([勾配_0-3m, 勾配_3-6m, 勾配_6-9m, 勾配_9-12m])
```

### 勾配の符号の解釈

算出された勾配値の符号は、特徴量が報酬（継続確率）に与える影響の方向を示す：

- **正の勾配 (+)**: 特徴量が増加すると報酬が増加 → **継続を促進**
  - 例: 総レビュー数 (+0.0165) → レビュー実績が多いほど継続しやすい

- **負の勾配 (-)**: 特徴量が増加すると報酬が減少 → **継続を抑制**
  - 例: 平均活動間隔 (-0.0107) → 活動間隔が開くほど離脱しやすい

- **ゼロに近い勾配**: 特徴量の変化が報酬に与える影響は限定的
  - 例: 最近の受諾率 (-0.0001) → 受諾率の影響はほぼない

### Random Forestとの違い

本手法とRandom ForestのGini係数ベース重要度の主な違いを表5.6に示す。

**表5.6: 特徴量重要度算出方法の比較**

| 観点 | Random Forest（Gini係数） | IRL（勾配ベース） |
|------|-------------------------|------------------|
| **測定対象** | 決定木の分岐での情報利得 | 報酬関数への微分的影響 |
| **解釈** | 過去データでの判別貢献度（相関） | 特徴量変化が報酬に与える影響（因果） |
| **符号** | 常に正（0-1に正規化） | 正負両方（影響の方向を表す） |
| **時系列考慮** | なし（各時点独立） | あり（LSTMで系列学習） |
| **値の範囲** | 0-1（合計=1.0） | 任意（-0.02〜+0.02程度） |
| **実務的含意** | 「誰が継続しやすいか」を識別 | 「何をすれば継続しやすくなるか」を示唆 |

---

## 5.5.2 時間経過による特徴量重要度の変化

図 5.3 は報酬関数における，状態特徴量と行動特徴量の各特徴量の重要度を訓練期間別に示したものである．

**図 5.3: 各特徴量の予測における重要度（時系列変化）**

```
勾配重要度
 +0.03
       ●総レビュー数（最も重要・継続を促進）
 +0.02  ●総コミット数（活動量・正の影響）
       ●最近の活動頻度
 +0.01  ▲協力度（行動特徴で最重要・継続を促進）
       ▲強度（ファイル数）
  0.00 .......................................................
       ▼平均活動間隔（継続を抑制）
 -0.01  ▼コード品質スコア（品質低下が離脱を促進）
       ▼レビュー規模（行数）（負担増加）

       0-3m    3-6m    6-9m    9-12m
                 訓練期間
```

図5.3から、以下の重要な発見が得られた：

### 短期予測（0-3m）の特徴

- **活動量特徴が重視される**:
  - 総コミット数（+0.0125）が高い重要度
  - 総レビュー数（+0.0316）が最重要
  - 最近の活動頻度（+0.0174）も重要

- **解釈**: プロジェクト参加初期では、**活動量の多さ**が継続意思を示す強いシグナルとなる

### 長期予測（6-9m, 9-12m）の特徴

- **質的要因が重視される**:
  - 協力度（+0.0146 at 9-12m）が最重要級に上昇
  - 応答速度の重要度が維持される
  - 総コミット数の重要度が低下（+0.0038 at 9-12m）

- **解釈**: 時間経過とともに、**単なる活動量ではなく、協力的な行動や応答の質**が継続に寄与するようになる

---

## 5.5.3 特徴量の符号と継続への影響

IRLの勾配ベース重要度から、各特徴量が継続に与える影響の方向性が明らかになった。表5.7に4訓練期間の平均値を示す。

**表5.7: 特徴量の勾配重要度と継続への影響（4訓練期間平均）**

### 状態特徴量（State Features）

| 順位 | 特徴量 | 勾配値 | 絶対値 | 影響 | 解釈 |
|------|--------|--------|--------|------|------|
| 1 | 総レビュー数 | +0.0165 | 0.0165 | 促進 | 過去の実績が多いほど継続しやすい |
| 5 | 総コミット数 | +0.0080 | 0.0080 | 促進 | 活動量が多いほど継続しやすい |
| 7 | 最近の活動頻度 | +0.0076 | 0.0076 | 促進 | 直近の活動が活発なほど継続しやすい |
| 9 | 協力スコア | +0.0041 | 0.0041 | 促進 | 協力的な活動が継続に寄与 |
| 13 | レビュー負荷 | +0.0011 | 0.0011 | 促進（弱） | 適度な負荷は継続を促進 |
| 3 | 平均活動間隔 | -0.0107 | 0.0107 | **抑制** | **活動間隔が開くと離脱しやすい** |
| 6 | コード品質スコア | -0.0078 | 0.0078 | **抑制** | **品質低下が継続を阻害** |
| 8 | 活動トレンド | -0.0064 | 0.0064 | **抑制** | **活動減少傾向が離脱を示唆** |
| 12 | 経験日数 | -0.0026 | 0.0026 | 抑制（弱） | 経験が長いほど離脱傾向（バーンアウト） |
| 14 | 最近の受諾率 | -0.0001 | 0.0001 | ほぼ無影響 | 受諾率の因果的影響はほぼゼロ |

### 行動特徴量（Action Features）

| 順位 | 特徴量 | 勾配値 | 絶対値 | 影響 | 解釈 |
|------|--------|--------|--------|------|------|
| 2 | 協力度 | +0.0131 | 0.0131 | 促進 | **協力的な行動が継続に強く寄与** |
| 4 | 強度（ファイル数） | +0.0083 | 0.0083 | 促進 | 適度な負荷のレビューが有益 |
| 11 | 応答速度 | +0.0027 | 0.0027 | 促進（弱） | 速い応答が継続に寄与 |
| 10 | レビュー規模（行数） | -0.0034 | 0.0034 | **抑制** | **大規模レビューは負担増加** |

---

## 5.5.4 重要な発見の詳細分析

### 発見1: 協力度の重要性（行動特徴で最重要）

**協力度**: +0.0131（全体2位、行動特徴で1位）

- **意味**: レビュイーを過去にレビューした回数が多いほど、継続しやすい
- **因果的解釈**: 協力的な関係構築が継続報酬を直接増加させる
- **実務的含意**: ペアレビュー制度やメンタープログラムが有効

### 発見2: 平均活動間隔の負の影響（負の勾配で最大）

**平均活動間隔**: -0.0107（全体3位、負の勾配で最大）

- **意味**: 活動と活動の間隔が長くなるほど、継続確率が低下
- **因果的解釈**: 活動間隔が開くこと自体が離脱リスクを高める
- **実務的含意**: 定期的な関与を促す仕組み（例: 3ヶ月以上空白で警告）が必要

### 発見3: コード品質スコアの負の影響

**コード品質スコア**: -0.0078（全体6位、負の勾配2位）

- **意味**: 品質向上活動（test, doc, refactor, fix）の割合が低下すると継続しやすい
- **因果的解釈**: 品質活動への負担が継続を阻害している可能性
  - または、品質低下が離脱の前兆となっている
- **実務的含意**: 品質維持のサポートやガイドラインの整備が重要

### 発見4: レビュー規模（行数）の負の影響

**レビュー規模（行数）**: -0.0034（全体10位、行動特徴で負の勾配）

- **意味**: 大規模な変更（行数が多い）のレビューは継続を阻害
- **因果的解釈**: レビュー負荷が高すぎると離脱リスクが増加
- **実務的含意**: 大規模レビューの分割や、負荷の可視化・分散が必要

### 発見5: 総レビュー数の圧倒的重要性

**総レビュー数**: +0.0165（全体1位、正の勾配で最大）

- **意味**: 過去のレビュー実績が多いほど、将来も継続しやすい
- **因果的解釈**: 実績が継続のモチベーションや習慣形成に寄与
- **Random Forestとの一致**: RFでも2位（25.45%）と高重要度

---

## 5.5.5 短期 vs 長期予測の違い

図5.3に示すように、訓練期間が長くなるにつれて特徴量重要度が変化する：

**表5.8: 訓練期間別の主要特徴量重要度の変化**

| 特徴量 | 0-3m | 3-6m | 6-9m | 9-12m | 傾向 |
|--------|------|------|------|-------|------|
| 総コミット数 | +0.0125 | +0.0084 | +0.0071 | +0.0038 | ↓ 減少 |
| 最近の活動頻度 | +0.0174 | +0.0121 | +0.0019 | -0.0009 | ↓ 減少（負に転じる） |
| 協力度 | +0.0156 | +0.0091 | +0.0131 | +0.0146 | → 維持・増加 |
| 応答速度 | -0.0027 | +0.0035 | +0.0048 | +0.0052 | ↑ 増加 |

**解釈**:
- **短期（0-3m）**: 活動量（総コミット数、活動頻度）が重視される
- **長期（6-9m, 9-12m）**: 質的要因（協力度、応答速度）が重視される
- **遷移**: 「どれだけ活動しているか」から「どのように活動しているか」へ

---

## 5.5.6 RQ3の回答

**RQ3: 推定された報酬関数において、長期貢献者の継続的なタスク受け入れに寄与する特徴量は何か？**

**回答**:

1. **継続を促進する特徴量（正の勾配）**:
   - **総レビュー数**（+0.0165）が最重要
   - **協力度**（+0.0131）が行動特徴で最重要
   - **総コミット数**、**強度（ファイル数）**も寄与

2. **継続を抑制する特徴量（負の勾配）**:
   - **平均活動間隔**（-0.0107）が最も強い負の影響
   - **コード品質スコア**（-0.0078）の低下が継続を阻害
   - **レビュー規模（行数）**の増加が負担増加

3. **時間経過による変化**:
   - 短期（0-3m）では**活動量**が重視される
   - 長期（6-9m, 9-12m）では**協力度や応答速度などの質的要因**が重視される

4. **勾配ベース手法の利点**:
   - 正負の符号により、**継続を促進/抑制する要因**が明確
   - 因果的影響を測定するため、**介入可能な施策**の設計に直接役立つ

本結果は、レビュア継続を促進するには、単に活動量を増やすだけでなく、**協力的な行動を促し、活動間隔を短く保ち、過度な負荷を避ける**施策が効果的であることを示唆している。
